<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Demystifying inner-workings of Spark SQL"><link href=https://jaceklaskowski.github.io/mastering-spark-sql-book/configuration-properties/ rel=canonical><meta name=author content="Jacek Laskowski"><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.1.4"><title>Configuration Properties - The Internals of Spark SQL</title><link rel=stylesheet href=../assets/stylesheets/main.358818c7.min.css><link rel=stylesheet href=../assets/stylesheets/palette.f0267088.min.css><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-151208281-4","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=none data-md-color-accent=none> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#configuration-properties class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://jaceklaskowski.github.io/mastering-spark-sql-book/ title="The Internals of Spark SQL" class="md-header-nav__button md-logo" aria-label="The Internals of Spark SQL"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> The Internals of Spark SQL </span> <span class="md-header-nav__topic md-ellipsis"> Configuration Properties </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> mastering-spark-sql-book </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class="md-tabs__link md-tabs__link--active"> Home </a> </li> <li class=md-tabs__item> <a href=../new-and-noteworthy/adaptive-query-execution/ class=md-tabs__link> New &amp; Noteworthy </a> </li> <li class=md-tabs__item> <a href=../FileScanBuilder/ class=md-tabs__link> New in Spark 3.0.0 </a> </li> <li class=md-tabs__item> <a href=../rdds/FileScanRDD/ class=md-tabs__link> RDDs </a> </li> <li class=md-tabs__item> <a href=../pyspark/Setup/ class=md-tabs__link> PySpark </a> </li> <li class=md-tabs__item> <a href=../demo/connecting-spark-sql-to-hive-metastore/ class=md-tabs__link> Demos </a> </li> <li class=md-tabs__item> <a href=../spark-sql/ class=md-tabs__link> Extras </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://jaceklaskowski.github.io/mastering-spark-sql-book/ title="The Internals of Spark SQL" class="md-nav__button md-logo" aria-label="The Internals of Spark SQL"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> The Internals of Spark SQL </label> <div class=md-nav__source> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> mastering-spark-sql-book </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1 checked> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Welcome </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-2 type=checkbox id=nav-1-2> <label class=md-nav__link for=nav-1-2> Connector API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Connector API" data-md-level=2> <label class=md-nav__title for=nav-1-2> <span class="md-nav__icon md-icon"></span> Connector API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/Table/ class=md-nav__link> Table </a> </li> <li class=md-nav__item> <a href=../connector/TableCapability/ class=md-nav__link> TableCapability </a> </li> <li class=md-nav__item> <a href=../connector/TableProvider/ class=md-nav__link> TableProvider </a> </li> <li class=md-nav__item> <a href=../connector/WriteBuilder/ class=md-nav__link> WriteBuilder </a> </li> <li class=md-nav__item> <a href=../connector/SupportsTruncate/ class=md-nav__link> SupportsTruncate </a> </li> <li class=md-nav__item> <a href=../connector/Scan/ class=md-nav__link> Scan </a> </li> <li class=md-nav__item> <a href=../connector/Batch/ class=md-nav__link> Batch </a> </li> <li class=md-nav__item> <a href=../connector/SupportsReportStatistics/ class=md-nav__link> SupportsReportStatistics </a> </li> <li class=md-nav__item> <a href=../connector/PartitionReaderFactory/ class=md-nav__link> PartitionReaderFactory </a> </li> <li class=md-nav__item> <a href=../connector/PartitionReader/ class=md-nav__link> PartitionReader </a> </li> <li class=md-nav__item> <a href=../connector/InputPartition/ class=md-nav__link> InputPartition </a> </li> <li class=md-nav__item> <a href=../connector/FileTable/ class=md-nav__link> FileTable </a> </li> <li class=md-nav__item> <a href=../connector/NoopTable/ class=md-nav__link> NoopTable </a> </li> <li class=md-nav__item> <a href=../connector/StagedTable/ class=md-nav__link> StagedTable </a> </li> <li class=md-nav__item> <a href=../connector/SupportsRead/ class=md-nav__link> SupportsRead </a> </li> <li class=md-nav__item> <a href=../connector/ScanBuilder/ class=md-nav__link> ScanBuilder </a> </li> <li class=md-nav__item> <a href=../connector/SupportsWrite/ class=md-nav__link> SupportsWrite </a> </li> <li class=md-nav__item> <a href=../connector/V1Table/ class=md-nav__link> V1Table </a> </li> <li class=md-nav__item> <a href=../connector/TableHelper/ class=md-nav__link> TableHelper </a> </li> <li class=md-nav__item> <a href=../connector/SessionConfigSupport/ class=md-nav__link> SessionConfigSupport </a> </li> <li class=md-nav__item> <a href=../connector/SimpleTableProvider/ class=md-nav__link> SimpleTableProvider </a> </li> <li class=md-nav__item> <a href=../connector/Transform/ class=md-nav__link> Transform </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-3 type=checkbox id=nav-1-3> <label class=md-nav__link for=nav-1-3> Catalog Plugin API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Catalog Plugin API" data-md-level=2> <label class=md-nav__title for=nav-1-3> <span class="md-nav__icon md-icon"></span> Catalog Plugin API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/catalog/CatalogManager/ class=md-nav__link> CatalogManager </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogPlugin/ class=md-nav__link> CatalogPlugin </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogExtension/ class=md-nav__link> CatalogExtension </a> </li> <li class=md-nav__item> <a href=../connector/catalog/DelegatingCatalogExtension/ class=md-nav__link> DelegatingCatalogExtension </a> </li> <li class=md-nav__item> <a href=../connector/catalog/StagingTableCatalog/ class=md-nav__link> StagingTableCatalog </a> </li> <li class=md-nav__item> <a href=../connector/catalog/SupportsNamespaces/ class=md-nav__link> SupportsNamespaces </a> </li> <li class=md-nav__item> <a href=../connector/catalog/SupportsCatalogOptions/ class=md-nav__link> SupportsCatalogOptions </a> </li> <li class=md-nav__item> <a href=../connector/catalog/TableCatalog/ class=md-nav__link> TableCatalog </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-3-9 type=checkbox id=nav-1-3-9> <label class=md-nav__link for=nav-1-3-9> Utilities <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Utilities data-md-level=3> <label class=md-nav__title for=nav-1-3-9> <span class="md-nav__icon md-icon"></span> Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/catalog/Catalogs/ class=md-nav__link> Catalogs </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogV2Util/ class=md-nav__link> CatalogV2Util </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-4 type=checkbox id=nav-1-4> <label class=md-nav__link for=nav-1-4> Structured Query Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Structured Query Execution" data-md-level=2> <label class=md-nav__title for=nav-1-4> <span class="md-nav__icon md-icon"></span> Structured Query Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../QueryExecution/ class=md-nav__link> QueryExecution </a> </li> <li class=md-nav__item> <a href=../Analyzer/ class=md-nav__link> Logical Analyzer </a> </li> <li class=md-nav__item> <a href=../SparkPlanner/ class=md-nav__link> SparkPlanner </a> </li> <li class=md-nav__item> <a href=../SparkOptimizer/ class=md-nav__link> SparkOptimizer </a> </li> <li class=md-nav__item> <a href=../QueryPlanningTracker/ class=md-nav__link> QueryPlanningTracker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-5 type=checkbox id=nav-1-5> <label class=md-nav__link for=nav-1-5> SparkSession <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SparkSession data-md-level=2> <label class=md-nav__title for=nav-1-5> <span class="md-nav__icon md-icon"></span> SparkSession </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SparkSession/ class=md-nav__link> SparkSession </a> </li> <li class=md-nav__item> <a href=../SparkSession-Builder/ class=md-nav__link> SparkSession.Builder </a> </li> <li class=md-nav__item> <a href=../SparkSessionExtensions/ class=md-nav__link> SparkSessionExtensions </a> </li> <li class=md-nav__item> <a href=../spark-sql-SparkSession-implicits/ class=md-nav__link> implicits Object </a> </li> <li class=md-nav__item> <a href=../SharedState/ class=md-nav__link> SharedState </a> </li> <li class=md-nav__item> <a href=../ColumnarRule/ class=md-nav__link> ColumnarRule </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Configuration Properties <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Configuration Properties </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#sparksqladaptiveforceapply class=md-nav__link> spark.sql.adaptive.forceApply </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveloglevel class=md-nav__link> spark.sql.adaptive.logLevel </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsenabled class=md-nav__link> spark.sql.adaptive.coalescePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveadvisorypartitionsizeinbytes class=md-nav__link> spark.sql.adaptive.advisoryPartitionSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsinitialpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.initialPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveenabled class=md-nav__link> spark.sql.adaptive.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivefetchshuffleblocksinbatch class=md-nav__link> spark.sql.adaptive.fetchShuffleBlocksInBatch </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivelocalshufflereaderenabled class=md-nav__link> spark.sql.adaptive.localShuffleReader.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinenabled class=md-nav__link> spark.sql.adaptive.skewJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionfactor class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin class=md-nav__link> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzermaxiterations class=md-nav__link> spark.sql.analyzer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzerfailambiguousselfjoin class=md-nav__link> spark.sql.analyzer.failAmbiguousSelfJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlansienabled class=md-nav__link> spark.sql.ansi.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenwholestage class=md-nav__link> spark.sql.codegen.wholeStage </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmethodsplitthreshold class=md-nav__link> spark.sql.codegen.methodSplitThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqldebugmaxtostringfields class=md-nav__link> spark.sql.debug.maxToStringFields </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultcatalog class=md-nav__link> spark.sql.defaultCatalog </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionreusesubquery class=md-nav__link> spark.sql.execution.reuseSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionsortbeforerepartition class=md-nav__link> spark.sql.execution.sortBeforeRepartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionrangeexchangesamplesizeperpartition class=md-nav__link> spark.sql.execution.rangeExchange.sampleSizePerPartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkfallbackenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.fallback.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowsparkrenabled class=md-nav__link> spark.sql.execution.arrow.sparkr.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasudfbuffersize class=md-nav__link> spark.sql.execution.pandas.udf.buffer.size </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasconverttoarrowarraysafely class=md-nav__link> spark.sql.execution.pandas.convertToArrowArraySafely </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramenabled class=md-nav__link> spark.sql.statistics.histogram.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsessiontimezone class=md-nav__link> spark.sql.session.timeZone </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcescommitprotocolclass class=md-nav__link> spark.sql.sources.commitProtocolClass </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesignoredatalocality class=md-nav__link> spark.sql.sources.ignoreDataLocality </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesvalidatepartitioncolumns class=md-nav__link> spark.sql.sources.validatePartitionColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesusev1sourcelist class=md-nav__link> spark.sql.sources.useV1SourceList </a> </li> <li class=md-nav__item> <a href=#sparksqlstoreassignmentpolicy class=md-nav__link> spark.sql.storeAssignmentPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetswitchthreshold class=md-nav__link> spark.sql.optimizer.inSetSwitchThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangeloglevel class=md-nav__link> spark.sql.optimizer.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogrules class=md-nav__link> spark.sql.optimizer.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogbatches class=md-nav__link> spark.sql.optimizer.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningenabled class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningusestats class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.useStats </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources class=md-nav__link> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerserializernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexpressionnestedpruningenabled class=md-nav__link> spark.sql.optimizer.expression.nestedPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcmergeschema class=md-nav__link> spark.sql.orc.mergeSchema </a> </li> <li class=md-nav__item> <a href=#sparksqldatetimejava8apienabled class=md-nav__link> spark.sql.datetime.java8API.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbinaryfilemaxlength class=md-nav__link> spark.sql.sources.binaryFile.maxLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmapkeydeduppolicy class=md-nav__link> spark.sql.mapKeyDedupPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlmavenadditionalremoterepositories class=md-nav__link> spark.sql.maven.additionalRemoteRepositories </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxplanstringlength class=md-nav__link> spark.sql.maxPlanStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqladdpartitioninbatchsize class=md-nav__link> spark.sql.addPartitionInBatch.size </a> </li> <li class=md-nav__item> <a href=#sparksqlscripttransformationexittimeoutinseconds class=md-nav__link> spark.sql.scriptTransformation.exitTimeoutInSeconds </a> </li> <li class=md-nav__item> <a href=#sparksqlautobroadcastjointhreshold class=md-nav__link> spark.sql.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqlavrocompressioncodec class=md-nav__link> spark.sql.avro.compression.codec </a> </li> <li class=md-nav__item> <a href=#sparksqlbroadcasttimeout class=md-nav__link> spark.sql.broadcastTimeout </a> </li> <li class=md-nav__item> <a href=#sparksqlcasesensitive class=md-nav__link> spark.sql.caseSensitive </a> </li> <li class=md-nav__item> <a href=#sparksqlcatalogspark_catalog class=md-nav__link> spark.sql.catalog.spark_catalog </a> </li> <li class=md-nav__item> <a href=#sparksqlcboenabled class=md-nav__link> spark.sql.cbo.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbojoinreorderenabled class=md-nav__link> spark.sql.cbo.joinReorder.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcboplanstatsenabled class=md-nav__link> spark.sql.cbo.planStats.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbostarschemadetection class=md-nav__link> spark.sql.cbo.starSchemaDetection </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatemapvectorizedenable class=md-nav__link> spark.sql.codegen.aggregate.map.vectorized.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatesplitaggregatefuncenabled class=md-nav__link> spark.sql.codegen.aggregate.splitAggregateFunc.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegencomments class=md-nav__link> spark.sql.codegen.comments </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfactorymode class=md-nav__link> spark.sql.codegen.factoryMode </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfallback class=md-nav__link> spark.sql.codegen.fallback </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenhugemethodlimit class=md-nav__link> spark.sql.codegen.hugeMethodLimit </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenuseidinclassname class=md-nav__link> spark.sql.codegen.useIdInClassName </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmaxfields class=md-nav__link> spark.sql.codegen.maxFields </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegensplitconsumefuncbyoperator class=md-nav__link> spark.sql.codegen.splitConsumeFuncByOperator </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnvectoroffheapenabled class=md-nav__link> spark.sql.columnVector.offheap.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnnameofcorruptrecord class=md-nav__link> spark.sql.columnNameOfCorruptRecord </a> </li> <li class=md-nav__item> <a href=#sparksqlconstraintpropagationenabled class=md-nav__link> spark.sql.constraintPropagation.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcsvfilterpushdownenabled class=md-nav__link> spark.sql.csv.filterPushdown.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultsizeinbytes class=md-nav__link> spark.sql.defaultSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqldialect class=md-nav__link> spark.sql.dialect </a> </li> <li class=md-nav__item> <a href=#sparksqlexchangereuse class=md-nav__link> spark.sql.exchange.reuse </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionuseobjecthashaggregateexec class=md-nav__link> spark.sql.execution.useObjectHashAggregateExec </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignorecorruptfiles class=md-nav__link> spark.sql.files.ignoreCorruptFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignoremissingfiles class=md-nav__link> spark.sql.files.ignoreMissingFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxrecordsperfile class=md-nav__link> spark.sql.files.maxRecordsPerFile </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxpartitionbytes class=md-nav__link> spark.sql.files.maxPartitionBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesopencostinbytes class=md-nav__link> spark.sql.files.openCostInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragecompressed class=md-nav__link> spark.sql.inMemoryColumnarStorage.compressed </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragebatchsize class=md-nav__link> spark.sql.inMemoryColumnarStorage.batchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorytablescanstatisticsenable class=md-nav__link> spark.sql.inMemoryTableScanStatistics.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstorageenablevectorizedreader class=md-nav__link> spark.sql.inMemoryColumnarStorage.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragepartitionpruning class=md-nav__link> spark.sql.inMemoryColumnarStorage.partitionPruning </a> </li> <li class=md-nav__item> <a href=#sparksqljoinprefersortmergejoin class=md-nav__link> spark.sql.join.preferSortMergeJoin </a> </li> <li class=md-nav__item> <a href=#sparksqljsongeneratorignorenullfields class=md-nav__link> spark.sql.jsonGenerator.ignoreNullFields </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydolooseupcast class=md-nav__link> spark.sql.legacy.doLooseUpcast </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycteprecedencepolicy class=md-nav__link> spark.sql.legacy.ctePrecedencePolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytimeparserpolicy class=md-nav__link> spark.sql.legacy.timeParserPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfollowthreevaluedlogicinarrayexists class=md-nav__link> spark.sql.legacy.followThreeValuedLogicInArrayExists </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfromdaytimestringenabled class=md-nav__link> spark.sql.legacy.fromDayTimeString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacynotreserveproperties class=md-nav__link> spark.sql.legacy.notReserveProperties </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyaddsinglefileinaddfile class=md-nav__link> spark.sql.legacy.addSingleFileInAddFile </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyexponentliteralasdecimalenabled class=md-nav__link> spark.sql.legacy.exponentLiteralAsDecimal.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownegativescaleofdecimal class=md-nav__link> spark.sql.legacy.allowNegativeScaleOfDecimal </a> </li> <li class=md-nav__item> <a href=#sparksqllegacybucketedtablescanoutputordering class=md-nav__link> spark.sql.legacy.bucketedTableScan.outputOrdering </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyjsonallowemptystringenabled class=md-nav__link> spark.sql.legacy.json.allowEmptyString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycreateemptycollectionusingstringtype class=md-nav__link> spark.sql.legacy.createEmptyCollectionUsingStringType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowuntypedscalaudf class=md-nav__link> spark.sql.legacy.allowUntypedScalaUDF </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue class=md-nav__link> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue </a> </li> <li class=md-nav__item> <a href=#sparksqllegacysetcommandrejectssparkcoreconfs class=md-nav__link> spark.sql.legacy.setCommandRejectsSparkCoreConfs </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytypecoerciondatetimetostringenabled class=md-nav__link> spark.sql.legacy.typeCoercion.datetimeToString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowhashonmaptype class=md-nav__link> spark.sql.legacy.allowHashOnMapType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyrddapplyconf class=md-nav__link> spark.sql.legacy.rdd.applyConf </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyreplacedatabrickssparkavroenabled class=md-nav__link> spark.sql.legacy.replaceDatabricksSparkAvro.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllimitscaleupfactor class=md-nav__link> spark.sql.limit.scaleUpFactor </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexcludedrules class=md-nav__link> spark.sql.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetconversionthreshold class=md-nav__link> spark.sql.optimizer.inSetConversionThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizermaxiterations class=md-nav__link> spark.sql.optimizer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerreplaceexceptwithfilter class=md-nav__link> spark.sql.optimizer.replaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcimpl class=md-nav__link> spark.sql.orc.impl </a> </li> <li class=md-nav__item> <a href=#sparksqlpysparkjvmstacktraceenabled class=md-nav__link> spark.sql.pyspark.jvmStacktrace.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetbinaryasstring class=md-nav__link> spark.sql.parquet.binaryAsString </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetcolumnarreaderbatchsize class=md-nav__link> spark.sql.parquet.columnarReaderBatchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96astimestamp class=md-nav__link> spark.sql.parquet.int96AsTimestamp </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetenablevectorizedreader class=md-nav__link> spark.sql.parquet.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdown class=md-nav__link> spark.sql.parquet.filterPushdown </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdowndate class=md-nav__link> spark.sql.parquet.filterPushdown.date </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96timestampconversion class=md-nav__link> spark.sql.parquet.int96TimestampConversion </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetrecordlevelfilterenabled class=md-nav__link> spark.sql.parquet.recordLevelFilter.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparserquotedregexcolumnnames class=md-nav__link> spark.sql.parser.quotedRegexColumnNames </a> </li> <li class=md-nav__item> <a href=#sparksqlpivotmaxvalues class=md-nav__link> spark.sql.pivotMaxValues </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionoptionsregex class=md-nav__link> spark.sql.redaction.options.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionstringregex class=md-nav__link> spark.sql.redaction.string.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlretaingroupcolumns class=md-nav__link> spark.sql.retainGroupColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlrunsqlonfiles class=md-nav__link> spark.sql.runSQLOnFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlselfjoinautoresolveambiguity class=md-nav__link> spark.sql.selfJoinAutoResolveAmbiguity </a> </li> <li class=md-nav__item> <a href=#sparksqlsortenableradixsort class=md-nav__link> spark.sql.sort.enableRadixSort </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingenabled class=md-nav__link> spark.sql.sources.bucketing.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesdefault class=md-nav__link> spark.sql.sources.default </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsfallbacktohdfs class=md-nav__link> spark.sql.statistics.fallBackToHdfs </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramnumbins class=md-nav__link> spark.sql.statistics.histogram.numBins </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled class=md-nav__link> spark.sql.statisticsparallelFileListingInStatsComputation.enabled* </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsndvmaxerror class=md-nav__link> spark.sql.statistics.ndv.maxError </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticspercentileaccuracy class=md-nav__link> spark.sql.statistics.percentile.accuracy </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticssizeautoupdateenabled class=md-nav__link> spark.sql.statistics.size.autoUpdate.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsubexpressioneliminationenabled class=md-nav__link> spark.sql.subexpressionElimination.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlshufflepartitions class=md-nav__link> spark.sql.shuffle.partitions </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesfilecompressionfactor class=md-nav__link> spark.sql.sources.fileCompressionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcespartitionoverwritemode class=md-nav__link> spark.sql.sources.partitionOverwriteMode </a> </li> <li class=md-nav__item> <a href=#sparksqltruncatetableignorepermissionaclenabled class=md-nav__link> spark.sql.truncateTable.ignorePermissionAcl.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqluiretainedexecutions class=md-nav__link> spark.sql.ui.retainedExecutions </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferinmemorythreshold class=md-nav__link> spark.sql.windowExec.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferspillthreshold class=md-nav__link> spark.sql.windowExec.buffer.spill.threshold </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-7 type=checkbox id=nav-1-7> <label class=md-nav__link for=nav-1-7> SQL Support <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SQL Support" data-md-level=2> <label class=md-nav__title for=nav-1-7> <span class="md-nav__icon md-icon"></span> SQL Support </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../sql/ class=md-nav__link> SQL Parsing Framework </a> </li> <li class=md-nav__item> <a href=../sql/ParserInterface/ class=md-nav__link> ParserInterface </a> </li> <li class=md-nav__item> <a href=../sql/AbstractSqlParser/ class=md-nav__link> AbstractSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/AstBuilder/ class=md-nav__link> AstBuilder </a> </li> <li class=md-nav__item> <a href=../sql/CatalystSqlParser/ class=md-nav__link> CatalystSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/SparkSqlParser/ class=md-nav__link> SparkSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/SparkSqlAstBuilder/ class=md-nav__link> SparkSqlAstBuilder </a> </li> <li class=md-nav__item> <a href=../sql/CatalogV2Implicits/ class=md-nav__link> CatalogV2Implicits </a> </li> <li class=md-nav__item> <a href=../sql/LogicalExpressions/ class=md-nav__link> LogicalExpressions </a> </li> <li class=md-nav__item> <a href=../sql/Expressions/ class=md-nav__link> Expressions </a> </li> <li class=md-nav__item> <a href=../sql/FieldReference/ class=md-nav__link> FieldReference </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8 type=checkbox id=nav-1-8> <label class=md-nav__link for=nav-1-8> Logical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Operators" data-md-level=2> <label class=md-nav__title for=nav-1-8> <span class="md-nav__icon md-icon"></span> Logical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/LogicalPlan/ class=md-nav__link> LogicalPlan </a> </li> <li class=md-nav__item> <a href=../logical-operators/Command/ class=md-nav__link> Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataWritingCommand/ class=md-nav__link> DataWritingCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/LeafNode/ class=md-nav__link> LeafNode </a> </li> <li class=md-nav__item> <a href=../logical-operators/OrderPreservingUnaryNode/ class=md-nav__link> OrderPreservingUnaryNode </a> </li> <li class=md-nav__item> <a href=../logical-operators/ParsedStatement/ class=md-nav__link> ParsedStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/ class=md-nav__link> RepartitionOperation </a> </li> <li class=md-nav__item> <a href=../logical-operators/RunnableCommand/ class=md-nav__link> RunnableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/SupportsSubquery/ class=md-nav__link> SupportsSubquery </a> </li> <li class=md-nav__item> <a href=../logical-operators/V2CreateTablePlan/ class=md-nav__link> V2CreateTablePlan </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-11 type=checkbox id=nav-1-8-11> <label class=md-nav__link for=nav-1-8-11> Statistics and Hints <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Statistics and Hints" data-md-level=3> <label class=md-nav__title for=nav-1-8-11> <span class="md-nav__icon md-icon"></span> Statistics and Hints </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/Statistics/ class=md-nav__link> Statistics </a> </li> <li class=md-nav__item> <a href=../logical-operators/HintInfo/ class=md-nav__link> HintInfo </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalPlanStats/ class=md-nav__link> LogicalPlanStats </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalPlanVisitor/ class=md-nav__link> LogicalPlanVisitor </a> </li> <li class=md-nav__item> <a href=../logical-operators/SizeInBytesOnlyStatsPlanVisitor/ class=md-nav__link> SizeInBytesOnlyStatsPlanVisitor </a> </li> <li class=md-nav__item> <a href=../logical-operators/BasicStatsPlanVisitor/ class=md-nav__link> BasicStatsPlanVisitor </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-11-7 type=checkbox id=nav-1-8-11-7> <label class=md-nav__link for=nav-1-8-11-7> Estimation Utilities <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Estimation Utilities" data-md-level=4> <label class=md-nav__title for=nav-1-8-11-7> <span class="md-nav__icon md-icon"></span> Estimation Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/AggregateEstimation/ class=md-nav__link> AggregateEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/EstimationUtils/ class=md-nav__link> EstimationUtils </a> </li> <li class=md-nav__item> <a href=../logical-operators/FilterEstimation/ class=md-nav__link> FilterEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/JoinEstimation/ class=md-nav__link> JoinEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/ProjectEstimation/ class=md-nav__link> ProjectEstimation </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-8-12 type=checkbox id=nav-1-8-12> <label class=md-nav__link for=nav-1-8-12> Concrete Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Concrete Operators" data-md-level=3> <label class=md-nav__title for=nav-1-8-12> <span class="md-nav__icon md-icon"></span> Concrete Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/Aggregate/ class=md-nav__link> Aggregate </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterTableRecoverPartitionsCommand/ class=md-nav__link> AlterTableRecoverPartitionsCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterTableRecoverPartitionsStatement/ class=md-nav__link> AlterTableRecoverPartitionsStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterViewAsCommand/ class=md-nav__link> AlterViewAsCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalysisBarrier/ class=md-nav__link> AnalysisBarrier Leaf Logical Operator </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeColumnCommand/ class=md-nav__link> AnalyzeColumnCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeColumnStatement/ class=md-nav__link> AnalyzeColumnStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzePartitionCommand/ class=md-nav__link> AnalyzePartitionCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeTableCommand/ class=md-nav__link> AnalyzeTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AppendData/ class=md-nav__link> AppendData </a> </li> <li class=md-nav__item> <a href=../logical-operators/ClearCacheCommand/ class=md-nav__link> ClearCacheCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CollectMetrics/ class=md-nav__link> CollectMetrics </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateDataSourceTableAsSelectCommand/ class=md-nav__link> CreateDataSourceTableAsSelectCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateDataSourceTableCommand/ class=md-nav__link> CreateDataSourceTableCommand Logical Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTable/ class=md-nav__link> CreateTable Logical Operator </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTableCommand/ class=md-nav__link> CreateTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTempViewUsing/ class=md-nav__link> CreateTempViewUsing Logical Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateV2Table/ class=md-nav__link> CreateV2Table </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateViewCommand/ class=md-nav__link> CreateViewCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataSourceV2Relation/ class=md-nav__link> DataSourceV2Relation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataSourceV2ScanRelation/ class=md-nav__link> DataSourceV2ScanRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DeleteFromTable/ class=md-nav__link> DeleteFromTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeColumnCommand/ class=md-nav__link> DescribeColumnCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeRelation/ class=md-nav__link> DescribeRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeTableCommand/ class=md-nav__link> DescribeTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/DeserializeToObject/ class=md-nav__link> DeserializeToObject </a> </li> <li class=md-nav__item> <a href=../logical-operators/DropTableCommand/ class=md-nav__link> DropTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/Except/ class=md-nav__link> Except </a> </li> <li class=md-nav__item> <a href=../logical-operators/Expand/ class=md-nav__link> Expand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ExplainCommand/ class=md-nav__link> ExplainCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ExternalRDD/ class=md-nav__link> ExternalRDD Leaf Logical Operator </a> </li> <li class=md-nav__item> <a href=../logical-operators/Filter/ class=md-nav__link> Filter </a> </li> <li class=md-nav__item> <a href=../logical-operators/Generate/ class=md-nav__link> Generate </a> </li> <li class=md-nav__item> <a href=../logical-operators/GlobalLimit/ class=md-nav__link> GlobalLimit </a> </li> <li class=md-nav__item> <a href=../logical-operators/GroupingSets/ class=md-nav__link> GroupingSets </a> </li> <li class=md-nav__item> <a href=../logical-operators/Hint/ class=md-nav__link> Hint </a> </li> <li class=md-nav__item> <a href=../logical-operators/InMemoryRelation/ class=md-nav__link> InMemoryRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoDataSourceCommand/ class=md-nav__link> InsertIntoDataSourceCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoDataSourceDirCommand/ class=md-nav__link> InsertIntoDataSourceDirCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoDir/ class=md-nav__link> InsertIntoDir </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoHadoopFsRelationCommand/ class=md-nav__link> InsertIntoHadoopFsRelationCommand Logical Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoTable/ class=md-nav__link> InsertIntoTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/Intersect/ class=md-nav__link> Intersect </a> </li> <li class=md-nav__item> <a href=../logical-operators/Join/ class=md-nav__link> Join </a> </li> <li class=md-nav__item> <a href=../logical-operators/LocalRelation/ class=md-nav__link> LocalRelation Leaf Logical Operator </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalQueryStage/ class=md-nav__link> LogicalQueryStage </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalRDD/ class=md-nav__link> LogicalRDD </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalRelation/ class=md-nav__link> LogicalRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/MergeIntoTable/ class=md-nav__link> MergeIntoTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/OneRowRelation/ class=md-nav__link> OneRowRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/OverwriteByExpression/ class=md-nav__link> OverwriteByExpression </a> </li> <li class=md-nav__item> <a href=../logical-operators/Pivot/ class=md-nav__link> Pivot </a> </li> <li class=md-nav__item> <a href=../logical-operators/Project/ class=md-nav__link> Project </a> </li> <li class=md-nav__item> <a href=../logical-operators/Range/ class=md-nav__link> Range </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepairTableStatement/ class=md-nav__link> RepairTableStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/#Repartition class=md-nav__link> Repartition </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/#RepartitionByExpression class=md-nav__link> RepartitionByExpression </a> </li> <li class=md-nav__item> <a href=../logical-operators/ResolvedHint/ class=md-nav__link> ResolvedHint </a> </li> <li class=md-nav__item> <a href=../logical-operators/SaveIntoDataSourceCommand/ class=md-nav__link> SaveIntoDataSourceCommand Logical Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/SetCatalogAndNamespace/ class=md-nav__link> SetCatalogAndNamespace </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCreateTableCommand/ class=md-nav__link> ShowCreateTableCommand Logical Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCurrentNamespace/ class=md-nav__link> ShowCurrentNamespace </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCurrentNamespaceStatement/ class=md-nav__link> ShowCurrentNamespaceStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowTables/ class=md-nav__link> ShowTables </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowTablesCommand/ class=md-nav__link> ShowTablesCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/Sort/ class=md-nav__link> Sort </a> </li> <li class=md-nav__item> <a href=../logical-operators/SubqueryAlias/ class=md-nav__link> SubqueryAlias </a> </li> <li class=md-nav__item> <a href=../logical-operators/TruncateTableCommand/ class=md-nav__link> TruncateTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/TypedFilter/ class=md-nav__link> TypedFilter </a> </li> <li class=md-nav__item> <a href=../logical-operators/Union/ class=md-nav__link> Union </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedCatalogRelation/ class=md-nav__link> UnresolvedCatalogRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedHint/ class=md-nav__link> UnresolvedHint </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedInlineTable/ class=md-nav__link> UnresolvedInlineTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedRelation/ class=md-nav__link> UnresolvedRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedTableValuedFunction/ class=md-nav__link> UnresolvedTableValuedFunction </a> </li> <li class=md-nav__item> <a href=../logical-operators/UseStatement/ class=md-nav__link> UseStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/Window/ class=md-nav__link> Window </a> </li> <li class=md-nav__item> <a href=../logical-operators/WithWindowDefinition/ class=md-nav__link> WithWindowDefinition </a> </li> <li class=md-nav__item> <a href=../logical-operators/WriteToDataSourceV2/ class=md-nav__link> WriteToDataSourceV2 Logical Operator </a> </li> <li class=md-nav__item> <a href=../logical-operators/View/ class=md-nav__link> View </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9 type=checkbox id=nav-1-9> <label class=md-nav__link for=nav-1-9> SparkSession Registries <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SparkSession Registries" data-md-level=2> <label class=md-nav__title for=nav-1-9> <span class="md-nav__icon md-icon"></span> SparkSession Registries </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-1 type=checkbox id=nav-1-9-1> <label class=md-nav__link for=nav-1-9-1> Catalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Catalog data-md-level=3> <label class=md-nav__title for=nav-1-9-1> <span class="md-nav__icon md-icon"></span> Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Catalog/ class=md-nav__link> Catalog </a> </li> <li class=md-nav__item> <a href=../CatalogImpl/ class=md-nav__link> CatalogImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ExecutionListenerManager/ class=md-nav__link> ExecutionListenerManager </a> </li> <li class=md-nav__item> <a href=../ExperimentalMethods/ class=md-nav__link> ExperimentalMethods </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-4 type=checkbox id=nav-1-9-4> <label class=md-nav__link for=nav-1-9-4> ExternalCatalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ExternalCatalog data-md-level=3> <label class=md-nav__title for=nav-1-9-4> <span class="md-nav__icon md-icon"></span> ExternalCatalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ExternalCatalog/ class=md-nav__link> ExternalCatalog </a> </li> <li class=md-nav__item> <a href=../InMemoryCatalog/ class=md-nav__link> InMemoryCatalog </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../FunctionRegistry/ class=md-nav__link> FunctionRegistry </a> </li> <li class=md-nav__item> <a href=../spark-sql-GlobalTempViewManager/ class=md-nav__link> GlobalTempViewManager </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-7 type=checkbox id=nav-1-9-7> <label class=md-nav__link for=nav-1-9-7> SessionCatalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SessionCatalog data-md-level=3> <label class=md-nav__title for=nav-1-9-7> <span class="md-nav__icon md-icon"></span> SessionCatalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SessionCatalog/ class=md-nav__link> SessionCatalog </a> </li> <li class=md-nav__item> <a href=../CatalogTable/ class=md-nav__link> CatalogTable </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalogStorageFormat/ class=md-nav__link> CatalogStorageFormat </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalogTablePartition/ class=md-nav__link> CatalogTablePartition </a> </li> <li class=md-nav__item> <a href=../spark-sql-BucketSpec/ class=md-nav__link> BucketSpec </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../V2SessionCatalog/ class=md-nav__link> V2SessionCatalog </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-9 type=checkbox id=nav-1-9-9> <label class=md-nav__link for=nav-1-9-9> SessionState <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SessionState data-md-level=3> <label class=md-nav__title for=nav-1-9-9> <span class="md-nav__icon md-icon"></span> SessionState </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SessionState/ class=md-nav__link> SessionState </a> </li> <li class=md-nav__item> <a href=../BaseSessionStateBuilder/ class=md-nav__link> BaseSessionStateBuilder </a> </li> <li class=md-nav__item> <a href=../SessionStateBuilder/ class=md-nav__link> SessionStateBuilder </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-10 type=checkbox id=nav-1-9-10> <label class=md-nav__link for=nav-1-9-10> CacheManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CacheManager data-md-level=3> <label class=md-nav__title for=nav-1-9-10> <span class="md-nav__icon md-icon"></span> CacheManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CacheManager/ class=md-nav__link> CacheManager </a> </li> <li class=md-nav__item> <a href=../CachedRDDBuilder/ class=md-nav__link> CachedRDDBuilder </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-RuntimeConfig/ class=md-nav__link> RuntimeConfig </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-12 type=checkbox id=nav-1-9-12> <label class=md-nav__link for=nav-1-9-12> SQLConf <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SQLConf data-md-level=3> <label class=md-nav__title for=nav-1-9-12> <span class="md-nav__icon md-icon"></span> SQLConf </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SQLConf/ class=md-nav__link> SQLConf </a> </li> <li class=md-nav__item> <a href=../StaticSQLConf/ class=md-nav__link> StaticSQLConf </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalystConf/ class=md-nav__link> CatalystConf </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../UDFRegistration/ class=md-nav__link> UDFRegistration </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10 type=checkbox id=nav-1-10> <label class=md-nav__link for=nav-1-10> Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Sources" data-md-level=2> <label class=md-nav__title for=nav-1-10> <span class="md-nav__icon md-icon"></span> Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-1 type=checkbox id=nav-1-10-1> <label class=md-nav__link for=nav-1-10-1> Files <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Files data-md-level=3> <label class=md-nav__title for=nav-1-10-1> <span class="md-nav__icon md-icon"></span> Files </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../FileFormat/ class=md-nav__link> FileFormat </a> </li> <li class=md-nav__item> <a href=../TextFileFormat/ class=md-nav__link> TextFileFormat </a> </li> <li class=md-nav__item> <a href=../TextBasedFileFormat/ class=md-nav__link> TextBasedFileFormat </a> </li> <li class=md-nav__item> <a href=../HadoopFsRelation/ class=md-nav__link> HadoopFsRelation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-1-5 type=checkbox id=nav-1-10-1-5> <label class=md-nav__link for=nav-1-10-1-5> FileIndex <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=FileIndex data-md-level=4> <label class=md-nav__title for=nav-1-10-1-5> <span class="md-nav__icon md-icon"></span> FileIndex </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../FileIndex/ class=md-nav__link> FileIndex </a> </li> <li class=md-nav__item> <a href=../CatalogFileIndex/ class=md-nav__link> CatalogFileIndex </a> </li> <li class=md-nav__item> <a href=../InMemoryFileIndex/ class=md-nav__link> InMemoryFileIndex </a> </li> <li class=md-nav__item> <a href=../PartitioningAwareFileIndex/ class=md-nav__link> PartitioningAwareFileIndex </a> </li> <li class=md-nav__item> <a href=../PrunedInMemoryFileIndex/ class=md-nav__link> PrunedInMemoryFileIndex </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../FileFormatWriter/ class=md-nav__link> FileFormatWriter </a> </li> <li class=md-nav__item> <a href=../SQLHadoopMapReduceCommitProtocol/ class=md-nav__link> SQLHadoopMapReduceCommitProtocol </a> </li> <li class=md-nav__item> <a href=../PartitionedFile/ class=md-nav__link> PartitionedFile </a> </li> <li class=md-nav__item> <a href=../RecordReaderIterator/ class=md-nav__link> RecordReaderIterator </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-2 type=checkbox id=nav-1-10-2> <label class=md-nav__link for=nav-1-10-2> Kafka <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Kafka data-md-level=3> <label class=md-nav__title for=nav-1-10-2> <span class="md-nav__icon md-icon"></span> Kafka </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/ class=md-nav__link> Kafka Data Source </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceProvider/ class=md-nav__link> KafkaSourceProvider </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaTable/ class=md-nav__link> KafkaTable </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/options/ class=md-nav__link> Options </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaScan/ class=md-nav__link> KafkaScan </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaBatch/ class=md-nav__link> KafkaBatch </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaBatchWrite/ class=md-nav__link> KafkaBatchWrite </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaRelation/ class=md-nav__link> KafkaRelation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-2-10 type=checkbox id=nav-1-10-2-10> <label class=md-nav__link for=nav-1-10-2-10> KafkaSourceRDD <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaSourceRDD data-md-level=4> <label class=md-nav__title for=nav-1-10-2-10> <span class="md-nav__icon md-icon"></span> KafkaSourceRDD </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceRDD/ class=md-nav__link> KafkaSourceRDD </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceRDDPartition/ class=md-nav__link> KafkaSourceRDDPartition </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../datasources/kafka/ConsumerStrategy/ class=md-nav__link> ConsumerStrategy </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetReader/ class=md-nav__link> KafkaOffsetReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetRangeLimit/ class=md-nav__link> KafkaOffsetRangeLimit </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaDataConsumer/ class=md-nav__link> KafkaDataConsumer </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/InternalKafkaConsumer/ class=md-nav__link> InternalKafkaConsumer </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaWriter/ class=md-nav__link> KafkaWriter </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaWriteTask/ class=md-nav__link> KafkaWriteTask </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/JsonUtils/ class=md-nav__link> JsonUtils </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/InternalKafkaProducerPool/ class=md-nav__link> InternalKafkaProducerPool </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-3 type=checkbox id=nav-1-10-3> <label class=md-nav__link for=nav-1-10-3> Avro <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Avro data-md-level=3> <label class=md-nav__title for=nav-1-10-3> <span class="md-nav__icon md-icon"></span> Avro </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-avro/ class=md-nav__link> Avro Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-AvroOptions/ class=md-nav__link> Options </a> </li> <li class=md-nav__item> <a href=../AvroFileFormat/ class=md-nav__link> AvroFileFormat </a> </li> <li class=md-nav__item> <a href=../spark-sql-Expression-CatalystDataToAvro/ class=md-nav__link> CatalystDataToAvro </a> </li> <li class=md-nav__item> <a href=../spark-sql-Expression-AvroDataToCatalyst/ class=md-nav__link> AvroDataToCatalyst </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-4 type=checkbox id=nav-1-10-4> <label class=md-nav__link for=nav-1-10-4> JDBC <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=JDBC data-md-level=3> <label class=md-nav__title for=nav-1-10-4> <span class="md-nav__icon md-icon"></span> JDBC </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-jdbc/ class=md-nav__link> JDBC Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-JDBCOptions/ class=md-nav__link> JDBCOptions </a> </li> <li class=md-nav__item> <a href=../spark-sql-JdbcRelationProvider/ class=md-nav__link> JdbcRelationProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-JDBCRelation/ class=md-nav__link> JDBCRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-JDBCRDD/ class=md-nav__link> JDBCRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-JdbcDialect/ class=md-nav__link> JdbcDialect </a> </li> <li class=md-nav__item> <a href=../spark-sql-JdbcUtils/ class=md-nav__link> JdbcUtils Utility </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-5 type=checkbox id=nav-1-10-5> <label class=md-nav__link for=nav-1-10-5> Console <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Console data-md-level=3> <label class=md-nav__title for=nav-1-10-5> <span class="md-nav__icon md-icon"></span> Console </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-console/ class=md-nav__link> Console Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-ConsoleSinkProvider/ class=md-nav__link> ConsoleSinkProvider </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-CSVFileFormat/ class=md-nav__link> CSV </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-7 type=checkbox id=nav-1-10-7> <label class=md-nav__link for=nav-1-10-7> JSON <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=JSON data-md-level=3> <label class=md-nav__title for=nav-1-10-7> <span class="md-nav__icon md-icon"></span> JSON </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-JsonFileFormat/ class=md-nav__link> JsonFileFormat </a> </li> <li class=md-nav__item> <a href=../spark-sql-JsonDataSource/ class=md-nav__link> JsonDataSource </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../OrcFileFormat/ class=md-nav__link> ORC </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10-9 type=checkbox id=nav-1-10-9> <label class=md-nav__link for=nav-1-10-9> Parquet <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Parquet data-md-level=3> <label class=md-nav__title for=nav-1-10-9> <span class="md-nav__icon md-icon"></span> Parquet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/parquet/ParquetTable/ class=md-nav__link> ParquetTable </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/ParquetFileFormat/ class=md-nav__link> ParquetFileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/ParquetReadSupport/ class=md-nav__link> ParquetReadSupport </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/VectorizedParquetRecordReader/ class=md-nav__link> VectorizedParquetRecordReader </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/VectorizedColumnReader/ class=md-nav__link> VectorizedColumnReader </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/SpecificParquetRecordReaderBase/ class=md-nav__link> SpecificParquetRecordReaderBase </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-11 type=checkbox id=nav-1-11> <label class=md-nav__link for=nav-1-11> Hive Integration <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Hive Integration" data-md-level=2> <label class=md-nav__title for=nav-1-11> <span class="md-nav__icon md-icon"></span> Hive Integration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../hive/ class=md-nav__link> Hive Data Source </a> </li> <li class=md-nav__item> <a href=../hive/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../hive/spark-sql-hive-metastore/ class=md-nav__link> Hive Metastore </a> </li> <li class=md-nav__item> <a href=../hive/spark-sql-DataSinks/ class=md-nav__link> DataSinks </a> </li> <li class=md-nav__item> <a href=../hive/HiveFileFormat/ class=md-nav__link> HiveFileFormat </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-11-6 type=checkbox id=nav-1-11-6> <label class=md-nav__link for=nav-1-11-6> HiveClient <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=HiveClient data-md-level=3> <label class=md-nav__title for=nav-1-11-6> <span class="md-nav__icon md-icon"></span> HiveClient </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../hive/HiveClient/ class=md-nav__link> HiveClient </a> </li> <li class=md-nav__item> <a href=../hive/HiveClientImpl/ class=md-nav__link> HiveClientImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../hive/HiveUtils/ class=md-nav__link> HiveUtils </a> </li> <li class=md-nav__item> <a href=../hive/IsolatedClientLoader/ class=md-nav__link> IsolatedClientLoader </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableRelation/ class=md-nav__link> HiveTableRelation </a> </li> <li class=md-nav__item> <a href=../hive/CreateHiveTableAsSelectCommand/ class=md-nav__link> CreateHiveTableAsSelectCommand </a> </li> <li class=md-nav__item> <a href=../hive/SaveAsHiveFile/ class=md-nav__link> SaveAsHiveFile </a> </li> <li class=md-nav__item> <a href=../hive/InsertIntoHiveDirCommand/ class=md-nav__link> InsertIntoHiveDirCommand </a> </li> <li class=md-nav__item> <a href=../hive/InsertIntoHiveTable/ class=md-nav__link> InsertIntoHiveTable </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableScans/ class=md-nav__link> HiveTableScans </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableScanExec/ class=md-nav__link> HiveTableScanExec </a> </li> <li class=md-nav__item> <a href=../hive/TableReader/ class=md-nav__link> TableReader </a> </li> <li class=md-nav__item> <a href=../hive/HadoopTableReader/ class=md-nav__link> HadoopTableReader </a> </li> <li class=md-nav__item> <a href=../hive/HiveSessionStateBuilder/ class=md-nav__link> HiveSessionStateBuilder </a> </li> <li class=md-nav__item> <a href=../hive/HiveExternalCatalog/ class=md-nav__link> HiveExternalCatalog </a> </li> <li class=md-nav__item> <a href=../hive/HiveSessionCatalog/ class=md-nav__link> HiveSessionCatalog </a> </li> <li class=md-nav__item> <a href=../hive/HiveMetastoreCatalog/ class=md-nav__link> HiveMetastoreCatalog </a> </li> <li class=md-nav__item> <a href=../hive/RelationConversions/ class=md-nav__link> RelationConversions </a> </li> <li class=md-nav__item> <a href=../hive/ResolveHiveSerdeTable/ class=md-nav__link> ResolveHiveSerdeTable </a> </li> <li class=md-nav__item> <a href=../hive/DetermineTableStats/ class=md-nav__link> DetermineTableStats </a> </li> <li class=md-nav__item> <a href=../hive/HiveAnalysis/ class=md-nav__link> HiveAnalysis </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12 type=checkbox id=nav-1-12> <label class=md-nav__link for=nav-1-12> Extending Spark SQL <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Extending Spark SQL" data-md-level=2> <label class=md-nav__title for=nav-1-12> <span class="md-nav__icon md-icon"></span> Extending Spark SQL </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-1 type=checkbox id=nav-1-12-1> <label class=md-nav__link for=nav-1-12-1> DataSource V2 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="DataSource V2" data-md-level=3> <label class=md-nav__title for=nav-1-12-1> <span class="md-nav__icon md-icon"></span> DataSource V2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-DataSourceV2/ class=md-nav__link> DataSourceV2 &mdash; Data Sources in DataSource V2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-ReadSupport/ class=md-nav__link> ReadSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-WriteSupport/ class=md-nav__link> WriteSupport </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-1-4 type=checkbox id=nav-1-12-1-4> <label class=md-nav__link for=nav-1-12-1-4> DataSourceReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=DataSourceReader data-md-level=4> <label class=md-nav__title for=nav-1-12-1-4> <span class="md-nav__icon md-icon"></span> DataSourceReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-DataSourceReader/ class=md-nav__link> DataSourceReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-SupportsPushDownFilters/ class=md-nav__link> SupportsPushDownFilters </a> </li> <li class=md-nav__item> <a href=../spark-sql-SupportsPushDownRequiredColumns/ class=md-nav__link> SupportsPushDownRequiredColumns </a> </li> <li class=md-nav__item> <a href=../spark-sql-SupportsReportPartitioning/ class=md-nav__link> SupportsReportPartitioning </a> </li> <li class=md-nav__item> <a href=../spark-sql-SupportsReportStatistics/ class=md-nav__link> SupportsReportStatistics </a> </li> <li class=md-nav__item> <a href=../spark-sql-SupportsScanColumnarBatch/ class=md-nav__link> SupportsScanColumnarBatch </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-DataSourceWriter/ class=md-nav__link> DataSourceWriter </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataWriter/ class=md-nav__link> DataWriter </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-1-7 type=checkbox id=nav-1-12-1-7> <label class=md-nav__link for=nav-1-12-1-7> DataWriterFactory <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=DataWriterFactory data-md-level=4> <label class=md-nav__title for=nav-1-12-1-7> <span class="md-nav__icon md-icon"></span> DataWriterFactory </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-DataWriterFactory/ class=md-nav__link> DataWriterFactory </a> </li> <li class=md-nav__item> <a href=../spark-sql-InternalRowDataWriterFactory/ class=md-nav__link> InternalRowDataWriterFactory </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-DataSourceV2StringFormat/ class=md-nav__link> DataSourceV2StringFormat </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-1-9 type=checkbox id=nav-1-12-1-9> <label class=md-nav__link for=nav-1-12-1-9> DataSourceRDD <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=DataSourceRDD data-md-level=4> <label class=md-nav__title for=nav-1-12-1-9> <span class="md-nav__icon md-icon"></span> DataSourceRDD </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataSourceRDD/ class=md-nav__link> DataSourceRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataSourceRDDPartition/ class=md-nav__link> DataSourceRDDPartition </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-DataWritingSparkTask/ class=md-nav__link> DataWritingSparkTask </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-2 type=checkbox id=nav-1-12-2> <label class=md-nav__link for=nav-1-12-2> Data Source API V1 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Source API V1" data-md-level=3> <label class=md-nav__title for=nav-1-12-2> <span class="md-nav__icon md-icon"></span> Data Source API V1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataSource/ class=md-nav__link> DataSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-datasource-custom-formats/ class=md-nav__link> Custom Data Source Formats </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-2-3 type=checkbox id=nav-1-12-2-3> <label class=md-nav__link for=nav-1-12-2-3> Relation Providers <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Relation Providers" data-md-level=4> <label class=md-nav__title for=nav-1-12-2-3> <span class="md-nav__icon md-icon"></span> Relation Providers </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CreatableRelationProvider/ class=md-nav__link> CreatableRelationProvider </a> </li> <li class=md-nav__item> <a href=../DataSourceRegister/ class=md-nav__link> DataSourceRegister </a> </li> <li class=md-nav__item> <a href=../RelationProvider/ class=md-nav__link> RelationProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-SchemaRelationProvider/ class=md-nav__link> SchemaRelationProvider &mdash; Relation Providers With Mandatory User-Defined Schema </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-2-4 type=checkbox id=nav-1-12-2-4> <label class=md-nav__link for=nav-1-12-2-4> Extension Contracts <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Extension Contracts" data-md-level=4> <label class=md-nav__title for=nav-1-12-2-4> <span class="md-nav__icon md-icon"></span> Extension Contracts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-BaseRelation/ class=md-nav__link> BaseRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-FileRelation/ class=md-nav__link> FileRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalystScan/ class=md-nav__link> CatalystScan </a> </li> <li class=md-nav__item> <a href=../spark-sql-InsertableRelation/ class=md-nav__link> InsertableRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-PrunedFilteredScan/ class=md-nav__link> PrunedFilteredScan </a> </li> <li class=md-nav__item> <a href=../spark-sql-PrunedScan/ class=md-nav__link> PrunedScan </a> </li> <li class=md-nav__item> <a href=../spark-sql-TableScan/ class=md-nav__link> TableScan </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-Filter/ class=md-nav__link> Data Source Filter Predicate (For Filter Pushdown) </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13 type=checkbox id=nav-1-13> <label class=md-nav__link for=nav-1-13> Catalyst <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Catalyst data-md-level=2> <label class=md-nav__title for=nav-1-13> <span class="md-nav__icon md-icon"></span> Catalyst </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../catalyst/ class=md-nav__link> Tree Manipulation Framework </a> </li> <li class=md-nav__item> <a href=../catalyst/TreeNode/ class=md-nav__link> TreeNode </a> </li> <li class=md-nav__item> <a href=../catalyst/QueryPlan/ class=md-nav__link> QueryPlan </a> </li> <li class=md-nav__item> <a href=../catalyst/RuleExecutor/ class=md-nav__link> RuleExecutor </a> </li> <li class=md-nav__item> <a href=../catalyst/Rule/ class=md-nav__link> Rule </a> </li> <li class=md-nav__item> <a href=../catalyst/QueryPlanner/ class=md-nav__link> QueryPlanner </a> </li> <li class=md-nav__item> <a href=../catalyst/Optimizer/ class=md-nav__link> Optimizer </a> </li> <li class=md-nav__item> <a href=../catalyst/GenericStrategy/ class=md-nav__link> GenericStrategy </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14 type=checkbox id=nav-1-14> <label class=md-nav__link for=nav-1-14> Catalyst Expressions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Catalyst Expressions" data-md-level=2> <label class=md-nav__title for=nav-1-14> <span class="md-nav__icon md-icon"></span> Catalyst Expressions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../expressions/Expression/ class=md-nav__link> Expression </a> </li> <li class=md-nav__item> <a href=../expressions/ExecSubqueryExpression/ class=md-nav__link> ExecSubqueryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/PlanExpression/ class=md-nav__link> PlanExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Unevaluable/ class=md-nav__link> Unevaluable </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-5 type=checkbox id=nav-1-14-5> <label class=md-nav__link for=nav-1-14-5> Concrete Expressions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Concrete Expressions" data-md-level=3> <label class=md-nav__title for=nav-1-14-5> <span class="md-nav__icon md-icon"></span> Concrete Expressions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../expressions/AggregateExpression/ class=md-nav__link> AggregateExpression </a> </li> <li class=md-nav__item> <a href=../expressions/AggregateFunction/ class=md-nav__link> AggregateFunction </a> </li> <li class=md-nav__item> <a href=../expressions/AggregateWindowFunction/ class=md-nav__link> AggregateWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/AttributeReference/ class=md-nav__link> AttributeReference </a> </li> <li class=md-nav__item> <a href=../expressions/Alias/ class=md-nav__link> Alias </a> </li> <li class=md-nav__item> <a href=../expressions/Attribute/ class=md-nav__link> Attribute </a> </li> <li class=md-nav__item> <a href=../expressions/BoundReference/ class=md-nav__link> BoundReference </a> </li> <li class=md-nav__item> <a href=../expressions/CallMethodViaReflection/ class=md-nav__link> CallMethodViaReflection </a> </li> <li class=md-nav__item> <a href=../expressions/Coalesce/ class=md-nav__link> Coalesce </a> </li> <li class=md-nav__item> <a href=../expressions/CodegenFallback/ class=md-nav__link> CodegenFallback </a> </li> <li class=md-nav__item> <a href=../expressions/CollectionGenerator/ class=md-nav__link> CollectionGenerator </a> </li> <li class=md-nav__item> <a href=../expressions/ComplexTypedAggregateExpression/ class=md-nav__link> ComplexTypedAggregateExpression </a> </li> <li class=md-nav__item> <a href=../expressions/CreateArray/ class=md-nav__link> CreateArray </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStruct/ class=md-nav__link> CreateNamedStruct </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStructLike/ class=md-nav__link> CreateNamedStructLike </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStructUnsafe/ class=md-nav__link> CreateNamedStructUnsafe </a> </li> <li class=md-nav__item> <a href=../expressions/CumeDist/ class=md-nav__link> CumeDist </a> </li> <li class=md-nav__item> <a href=../expressions/DeclarativeAggregate/ class=md-nav__link> DeclarativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/DynamicPruningExpression/ class=md-nav__link> DynamicPruningExpression </a> </li> <li class=md-nav__item> <a href=../expressions/DynamicPruningSubquery/ class=md-nav__link> DynamicPruningSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/Exists/ class=md-nav__link> Exists </a> </li> <li class=md-nav__item> <a href=../expressions/ExpectsInputTypes/ class=md-nav__link> ExpectsInputTypes </a> </li> <li class=md-nav__item> <a href=../expressions/ExplodeBase/ class=md-nav__link> ExplodeBase </a> </li> <li class=md-nav__item> <a href=../expressions/First/ class=md-nav__link> First </a> </li> <li class=md-nav__item> <a href=../expressions/Generator/ class=md-nav__link> Generator </a> </li> <li class=md-nav__item> <a href=../expressions/GetArrayStructFields/ class=md-nav__link> GetArrayStructFields </a> </li> <li class=md-nav__item> <a href=../expressions/GetArrayItem/ class=md-nav__link> GetArrayItem </a> </li> <li class=md-nav__item> <a href=../expressions/GetMapValue/ class=md-nav__link> GetMapValue </a> </li> <li class=md-nav__item> <a href=../expressions/GetStructField/ class=md-nav__link> GetStructField </a> </li> <li class=md-nav__item> <a href=../expressions/HashPartitioning/ class=md-nav__link> HashPartitioning </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-14-5-31 type=checkbox id=nav-1-14-5-31> <label class=md-nav__link for=nav-1-14-5-31> Higher-Order Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Higher-Order Functions" data-md-level=4> <label class=md-nav__title for=nav-1-14-5-31> <span class="md-nav__icon md-icon"></span> Higher-Order Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../expressions/HigherOrderFunction/ class=md-nav__link> HigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/ArrayBasedSimpleHigherOrderFunction/ class=md-nav__link> ArrayBasedSimpleHigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/MapBasedSimpleHigherOrderFunction/ class=md-nav__link> MapBasedSimpleHigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/SimpleHigherOrderFunction/ class=md-nav__link> SimpleHigherOrderFunction </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../expressions/ImperativeAggregate/ class=md-nav__link> ImperativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/In/ class=md-nav__link> In </a> </li> <li class=md-nav__item> <a href=../expressions/Inline/ class=md-nav__link> Inline </a> </li> <li class=md-nav__item> <a href=../expressions/InSet/ class=md-nav__link> InSet </a> </li> <li class=md-nav__item> <a href=../expressions/InSubquery/ class=md-nav__link> InSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/InSubqueryExec/ class=md-nav__link> InSubqueryExec </a> </li> <li class=md-nav__item> <a href=../expressions/JsonToStructs/ class=md-nav__link> JsonToStructs </a> </li> <li class=md-nav__item> <a href=../expressions/JsonTuple/ class=md-nav__link> JsonTuple </a> </li> <li class=md-nav__item> <a href=../expressions/ListQuery/ class=md-nav__link> ListQuery </a> </li> <li class=md-nav__item> <a href=../expressions/Literal/ class=md-nav__link> Literal </a> </li> <li class=md-nav__item> <a href=../expressions/MonotonicallyIncreasingID/ class=md-nav__link> MonotonicallyIncreasingID </a> </li> <li class=md-nav__item> <a href=../expressions/Murmur3Hash/ class=md-nav__link> Murmur3Hash </a> </li> <li class=md-nav__item> <a href=../expressions/NamedExpression/ class=md-nav__link> NamedExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Nondeterministic/ class=md-nav__link> Nondeterministic </a> </li> <li class=md-nav__item> <a href=../expressions/OffsetWindowFunction/ class=md-nav__link> OffsetWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/ParseToDate/ class=md-nav__link> ParseToDate </a> </li> <li class=md-nav__item> <a href=../expressions/ParseToTimestamp/ class=md-nav__link> ParseToTimestamp </a> </li> <li class=md-nav__item> <a href=../expressions/PrettyAttribute/ class=md-nav__link> PrettyAttribute </a> </li> <li class=md-nav__item> <a href=../expressions/RangePartitioning/ class=md-nav__link> RangePartitioning </a> </li> <li class=md-nav__item> <a href=../expressions/RankLike/ class=md-nav__link> RankLike </a> </li> <li class=md-nav__item> <a href=../expressions/ResolvedStar/ class=md-nav__link> ResolvedStar </a> </li> <li class=md-nav__item> <a href=../expressions/RowNumberLike/ class=md-nav__link> RowNumberLike </a> </li> <li class=md-nav__item> <a href=../expressions/RuntimeReplaceable/ class=md-nav__link> RuntimeReplaceable </a> </li> <li class=md-nav__item> <a href=../expressions/ScalarSubquery/ class=md-nav__link> ScalarSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/spark-sql-Expression-ExecSubqueryExpression-ScalarSubquery/ class=md-nav__link> ScalarSubquery (ExecSubqueryExpression) Expression </a> </li> <li class=md-nav__item> <a href=../expressions/ScalaUDF/ class=md-nav__link> ScalaUDF </a> </li> <li class=md-nav__item> <a href=../expressions/ScalaUDAF/ class=md-nav__link> ScalaUDAF </a> </li> <li class=md-nav__item> <a href=../expressions/SimpleTypedAggregateExpression/ class=md-nav__link> SimpleTypedAggregateExpression </a> </li> <li class=md-nav__item> <a href=../expressions/SizeBasedWindowFunction/ class=md-nav__link> SizeBasedWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/SortOrder/ class=md-nav__link> SortOrder </a> </li> <li class=md-nav__item> <a href=../expressions/Stack/ class=md-nav__link> Stack </a> </li> <li class=md-nav__item> <a href=../expressions/Star/ class=md-nav__link> Star </a> </li> <li class=md-nav__item> <a href=../expressions/StaticInvoke/ class=md-nav__link> StaticInvoke </a> </li> <li class=md-nav__item> <a href=../expressions/SubqueryExpression/ class=md-nav__link> SubqueryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/TimeWindow/ class=md-nav__link> TimeWindow </a> </li> <li class=md-nav__item> <a href=../expressions/TypedAggregateExpression/ class=md-nav__link> TypedAggregateExpression </a> </li> <li class=md-nav__item> <a href=../expressions/TypedImperativeAggregate/ class=md-nav__link> TypedImperativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/UnaryExpression/ class=md-nav__link> UnaryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/UnixTimestamp/ class=md-nav__link> UnixTimestamp </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedAttribute/ class=md-nav__link> UnresolvedAttribute </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedFunction/ class=md-nav__link> UnresolvedFunction </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedGenerator/ class=md-nav__link> UnresolvedGenerator </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedOrdinal/ class=md-nav__link> UnresolvedOrdinal </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedRegex/ class=md-nav__link> UnresolvedRegex </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedStar/ class=md-nav__link> UnresolvedStar </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedWindowExpression/ class=md-nav__link> UnresolvedWindowExpression </a> </li> <li class=md-nav__item> <a href=../expressions/WindowExpression/ class=md-nav__link> WindowExpression </a> </li> <li class=md-nav__item> <a href=../expressions/WindowFunction/ class=md-nav__link> WindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/WindowSpecDefinition/ class=md-nav__link> WindowSpecDefinition </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15 type=checkbox id=nav-1-15> <label class=md-nav__link for=nav-1-15> Physical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Operators" data-md-level=2> <label class=md-nav__title for=nav-1-15> <span class="md-nav__icon md-icon"></span> Physical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/SparkPlan/ class=md-nav__link> SparkPlan </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseLimitExec/ class=md-nav__link> BaseLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseSubqueryExec/ class=md-nav__link> BaseSubqueryExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BinaryExecNode/ class=md-nav__link> BinaryExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/CodegenSupport/ class=md-nav__link> CodegenSupport </a> </li> <li class=md-nav__item> <a href=../physical-operators/ColumnarBatchScan/ class=md-nav__link> ColumnarBatchScan </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceScanExec/ class=md-nav__link> DataSourceScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceV2ScanExecBase/ class=md-nav__link> DataSourceV2ScanExecBase </a> </li> <li class=md-nav__item> <a href=../physical-operators/EvalPythonExec/ class=md-nav__link> EvalPythonExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/Exchange/ class=md-nav__link> Exchange </a> </li> <li class=md-nav__item> <a href=../physical-operators/LeafExecNode/ class=md-nav__link> LeafExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/LimitExec/ class=md-nav__link> LimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectConsumerExec/ class=md-nav__link> ObjectConsumerExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/QueryStageExec/ class=md-nav__link> QueryStageExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnaryExecNode/ class=md-nav__link> UnaryExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/V2CommandExec/ class=md-nav__link> V2CommandExec </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-17 type=checkbox id=nav-1-15-17> <label class=md-nav__link for=nav-1-15-17> Projection <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Projection data-md-level=3> <label class=md-nav__title for=nav-1-15-17> <span class="md-nav__icon md-icon"></span> Projection </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/Projection/ class=md-nav__link> Projection </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnsafeProjection/ class=md-nav__link> UnsafeProjection </a> </li> <li class=md-nav__item> <a href=../physical-operators/GenerateUnsafeProjection/ class=md-nav__link> GenerateUnsafeProjection </a> </li> <li class=md-nav__item> <a href=../physical-operators/GenerateMutableProjection/ class=md-nav__link> GenerateMutableProjection </a> </li> <li class=md-nav__item> <a href=../physical-operators/InterpretedProjection/ class=md-nav__link> InterpretedProjection </a> </li> <li class=md-nav__item> <a href=../physical-operators/CodeGeneratorWithInterpretedFallback/ class=md-nav__link> CodeGeneratorWithInterpretedFallback </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../physical-operators/SQLMetric/ class=md-nav__link> SQLMetric </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-19 type=checkbox id=nav-1-15-19> <label class=md-nav__link for=nav-1-15-19> Distribution and Partitioning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Distribution and Partitioning" data-md-level=3> <label class=md-nav__title for=nav-1-15-19> <span class="md-nav__icon md-icon"></span> Distribution and Partitioning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/Distribution/ class=md-nav__link> Distribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/Partitioning/ class=md-nav__link> Partitioning </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-19-3 type=checkbox id=nav-1-15-19-3> <label class=md-nav__link for=nav-1-15-19-3> Distribution Specifications <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Distribution Specifications" data-md-level=4> <label class=md-nav__title for=nav-1-15-19-3> <span class="md-nav__icon md-icon"></span> Distribution Specifications </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/AllTuples/ class=md-nav__link> AllTuples </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastDistribution/ class=md-nav__link> BroadcastDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/ClusteredDistribution/ class=md-nav__link> ClusteredDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashClusteredDistribution/ class=md-nav__link> HashClusteredDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/OrderedDistribution/ class=md-nav__link> OrderedDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnspecifiedDistribution/ class=md-nav__link> UnspecifiedDistribution </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-19-4 type=checkbox id=nav-1-15-19-4> <label class=md-nav__link for=nav-1-15-19-4> Broadcast Modes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Broadcast Modes" data-md-level=4> <label class=md-nav__title for=nav-1-15-19-4> <span class="md-nav__icon md-icon"></span> Broadcast Modes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/BroadcastMode/ class=md-nav__link> BroadcastMode </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashedRelationBroadcastMode/ class=md-nav__link> HashedRelationBroadcastMode </a> </li> <li class=md-nav__item> <a href=../physical-operators/IdentityBroadcastMode/ class=md-nav__link> IdentityBroadcastMode </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-20 type=checkbox id=nav-1-15-20> <label class=md-nav__link for=nav-1-15-20> HashedRelation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=HashedRelation data-md-level=3> <label class=md-nav__title for=nav-1-15-20> <span class="md-nav__icon md-icon"></span> HashedRelation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/HashedRelation/ class=md-nav__link> HashedRelation </a> </li> <li class=md-nav__item> <a href=../physical-operators/LongHashedRelation/ class=md-nav__link> LongHashedRelation </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnsafeHashedRelation/ class=md-nav__link> UnsafeHashedRelation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../physical-operators/HashJoin/ class=md-nav__link> HashJoin </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-22 type=checkbox id=nav-1-15-22> <label class=md-nav__link for=nav-1-15-22> Concrete Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Concrete Operators" data-md-level=3> <label class=md-nav__title for=nav-1-15-22> <span class="md-nav__icon md-icon"></span> Concrete Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/AdaptiveSparkPlanExec/ class=md-nav__link> AdaptiveSparkPlanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AlterTableExec/ class=md-nav__link> AlterTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicCreateTableAsSelectExec/ class=md-nav__link> AtomicCreateTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicReplaceTableAsSelectExec/ class=md-nav__link> AtomicReplaceTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicReplaceTableExec/ class=md-nav__link> AtomicReplaceTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BatchScanExec/ class=md-nav__link> BatchScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BlockingOperatorWithCodegen/ class=md-nav__link> BlockingOperatorWithCodegen </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastExchangeExec/ class=md-nav__link> BroadcastExchangeExec Unary Physical Operator for Broadcast Joins </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastHashJoinExec/ class=md-nav__link> BroadcastHashJoinExec Binary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastNestedLoopJoinExec/ class=md-nav__link> BroadcastNestedLoopJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastQueryStageExec/ class=md-nav__link> BroadcastQueryStageExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CartesianProductExec/ class=md-nav__link> CartesianProductExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CoalesceExec/ class=md-nav__link> CoalesceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CoGroupExec/ class=md-nav__link> CoGroupExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CollectLimitExec/ class=md-nav__link> CollectLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CollectMetricsExec/ class=md-nav__link> CollectMetricsExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ColumnarToRowExec/ class=md-nav__link> ColumnarToRowExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CreateTableAsSelectExec/ class=md-nav__link> CreateTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CreateTableExec/ class=md-nav__link> CreateTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CustomShuffleReaderExec/ class=md-nav__link> CustomShuffleReaderExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceV2ScanExec/ class=md-nav__link> DataSourceV2ScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataWritingCommandExec/ class=md-nav__link> DataWritingCommandExec Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/DebugExec/ class=md-nav__link> DebugExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DescribeTableExec/ class=md-nav__link> DescribeTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DeserializeToObjectExec/ class=md-nav__link> DeserializeToObjectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DropNamespaceExec/ class=md-nav__link> DropNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DropTableExec/ class=md-nav__link> DropTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ExecutedCommandExec/ class=md-nav__link> ExecutedCommandExec Leaf Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/ExpandExec/ class=md-nav__link> ExpandExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ExternalRDDScanExec/ class=md-nav__link> ExternalRDDScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/FileSourceScanExec/ class=md-nav__link> FileSourceScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/FilterExec/ class=md-nav__link> FilterExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/GenerateExec/ class=md-nav__link> GenerateExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashAggregateExec/ class=md-nav__link> HashAggregateExec Aggregate Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/InMemoryTableScanExec/ class=md-nav__link> InMemoryTableScanExec Leaf Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/InputAdapter/ class=md-nav__link> InputAdapter </a> </li> <li class=md-nav__item> <a href=../physical-operators/InputRDDCodegen/ class=md-nav__link> InputRDDCodegen </a> </li> <li class=md-nav__item> <a href=../physical-operators/LocalTableScanExec/ class=md-nav__link> LocalTableScanExec Leaf Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/MapElementsExec/ class=md-nav__link> MapElementsExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectHashAggregateExec/ class=md-nav__link> ObjectHashAggregateExec Aggregate Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectProducerExec/ class=md-nav__link> ObjectProducerExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ProjectExec/ class=md-nav__link> ProjectExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/RangeExec/ class=md-nav__link> RangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RDDScanExec/ class=md-nav__link> RDDScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RefreshTableExec/ class=md-nav__link> RefreshTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RenameTableExec/ class=md-nav__link> RenameTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReplaceTableAsSelectExec/ class=md-nav__link> ReplaceTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReplaceTableExec/ class=md-nav__link> ReplaceTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReusedExchangeExec/ class=md-nav__link> ReusedExchangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RowDataSourceScanExec/ class=md-nav__link> RowDataSourceScanExec Leaf Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/RowToColumnarExec/ class=md-nav__link> RowToColumnarExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SampleExec/ class=md-nav__link> SampleExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SerializeFromObjectExec/ class=md-nav__link> SerializeFromObjectExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/SetCatalogAndNamespaceExec/ class=md-nav__link> SetCatalogAndNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShowCurrentNamespaceExec/ class=md-nav__link> ShowCurrentNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShowTablesExec/ class=md-nav__link> ShowTablesExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffleExchangeExec/ class=md-nav__link> ShuffleExchangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffledHashJoinExec/ class=md-nav__link> ShuffledHashJoinExec Binary Physical Operator for Shuffled Hash Join </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffleQueryStageExec/ class=md-nav__link> ShuffleQueryStageExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortAggregateExec/ class=md-nav__link> SortAggregateExec Aggregate Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortMergeJoinExec/ class=md-nav__link> SortMergeJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortExec/ class=md-nav__link> SortExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/SubqueryExec/ class=md-nav__link> SubqueryExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SubqueryBroadcastExec/ class=md-nav__link> SubqueryBroadcastExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/TakeOrderedAndProjectExec/ class=md-nav__link> TakeOrderedAndProjectExec </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-15-22-66 type=checkbox id=nav-1-15-22-66> <label class=md-nav__link for=nav-1-15-22-66> WindowExec <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WindowExec data-md-level=4> <label class=md-nav__title for=nav-1-15-22-66> <span class="md-nav__icon md-icon"></span> WindowExec </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/WindowExec/ class=md-nav__link> WindowExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/AggregateProcessor/ class=md-nav__link> AggregateProcessor </a> </li> <li class=md-nav__item> <a href=../physical-operators/WindowFunctionFrame/ class=md-nav__link> WindowFunctionFrame </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../physical-operators/WholeStageCodegenExec/ class=md-nav__link> WholeStageCodegenExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../physical-operators/WriteToDataSourceV2Exec/ class=md-nav__link> WriteToDataSourceV2Exec Physical Operator </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16 type=checkbox id=nav-1-16> <label class=md-nav__link for=nav-1-16> Logical Analysis Rules <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Analysis Rules" data-md-level=2> <label class=md-nav__title for=nav-1-16> <span class="md-nav__icon md-icon"></span> Logical Analysis Rules </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-analysis-rules/AliasViewChild/ class=md-nav__link> AliasViewChild </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/CleanupAliases/ class=md-nav__link> CleanupAliases </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/DataSourceAnalysis/ class=md-nav__link> DataSourceAnalysis </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ExtractWindowExpressions/ class=md-nav__link> ExtractWindowExpressions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/FindDataSourceTable/ class=md-nav__link> FindDataSourceTable </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/HandleNullInputsForUDF/ class=md-nav__link> HandleNullInputsForUDF </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/InConversion/ class=md-nav__link> InConversion </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/LookupFunctions/ class=md-nav__link> LookupFunctions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/PreprocessTableCreation/ class=md-nav__link> PreprocessTableCreation </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/PreWriteCheck/ class=md-nav__link> PreWriteCheck </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveAliases/ class=md-nav__link> ResolveAliases </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCatalogs/ class=md-nav__link> ResolveCatalogs </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCoalesceHints/ class=md-nav__link> ResolveCoalesceHints </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCreateNamedStruct/ class=md-nav__link> ResolveCreateNamedStruct </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveFunctions/ class=md-nav__link> ResolveFunctions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveInlineTables/ class=md-nav__link> ResolveInlineTables </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveJoinStrategyHints/ class=md-nav__link> ResolveJoinStrategyHints </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveMissingReferences/ class=md-nav__link> ResolveMissingReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/ class=md-nav__link> ResolveOrdinalInOrderByAndGroupBy </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveOutputRelation/ class=md-nav__link> ResolveOutputRelation </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveReferences/ class=md-nav__link> ResolveReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveRelations/ class=md-nav__link> ResolveRelations </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSessionCatalog/ class=md-nav__link> ResolveSessionCatalog </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSQLOnFile/ class=md-nav__link> ResolveSQLOnFile </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSubquery/ class=md-nav__link> ResolveSubquery </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveTables/ class=md-nav__link> ResolveTables </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveTimeZone/ class=md-nav__link> ResolveTimeZone </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveWindowFrame/ class=md-nav__link> ResolveWindowFrame </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveWindowOrder/ class=md-nav__link> ResolveWindowOrder </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/TableCapabilityCheck/ class=md-nav__link> TableCapabilityCheck </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/TimeWindowing/ class=md-nav__link> TimeWindowing </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/UpdateOuterReferences/ class=md-nav__link> UpdateOuterReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WindowFrameCoercion/ class=md-nav__link> WindowFrameCoercion </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WidenSetOperationTypes/ class=md-nav__link> WidenSetOperationTypes </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WindowsSubstitution/ class=md-nav__link> WindowsSubstitution </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-17 type=checkbox id=nav-1-17> <label class=md-nav__link for=nav-1-17> Logical Optimizations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Optimizations" data-md-level=2> <label class=md-nav__title for=nav-1-17> <span class="md-nav__icon md-icon"></span> Logical Optimizations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-optimizations/CleanupDynamicPruningFilters/ class=md-nav__link> CleanupDynamicPruningFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CollapseWindow/ class=md-nav__link> CollapseWindow </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ColumnPruning/ class=md-nav__link> ColumnPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CombineTypedFilters/ class=md-nav__link> CombineTypedFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CombineUnions/ class=md-nav__link> CombineUnions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ComputeCurrentTime/ class=md-nav__link> ComputeCurrentTime </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ConstantFolding/ class=md-nav__link> ConstantFolding </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CostBasedJoinReorder/ class=md-nav__link> CostBasedJoinReorder </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/DecimalAggregates/ class=md-nav__link> DecimalAggregates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/DemoteBroadcastHashJoin/ class=md-nav__link> DemoteBroadcastHashJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateSerialization/ class=md-nav__link> EliminateSerialization </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateSubqueryAliases/ class=md-nav__link> EliminateSubqueryAliases </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateView/ class=md-nav__link> EliminateView </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ExtractPythonUDFFromAggregate/ class=md-nav__link> ExtractPythonUDFFromAggregate </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/GetCurrentDatabase/ class=md-nav__link> GetCurrentDatabase </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/InferFiltersFromConstraints/ class=md-nav__link> InferFiltersFromConstraints </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/LimitPushDown/ class=md-nav__link> LimitPushDown </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/NullPropagation/ class=md-nav__link> NullPropagation </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeIn/ class=md-nav__link> OptimizeIn </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeMetadataOnlyQuery/ class=md-nav__link> OptimizeMetadataOnlyQuery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeSubqueries/ class=md-nav__link> OptimizeSubqueries </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PartitionPruning/ class=md-nav__link> PartitionPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PropagateEmptyRelation/ class=md-nav__link> PropagateEmptyRelation </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PruneFileSourcePartitions/ class=md-nav__link> PruneFileSourcePartitions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PruneFilters/ class=md-nav__link> PruneFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PullupCorrelatedPredicates/ class=md-nav__link> PullupCorrelatedPredicates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownOperatorsToDataSource/ class=md-nav__link> PushDownOperatorsToDataSource </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownPredicate/ class=md-nav__link> PushDownPredicate </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownPredicates/ class=md-nav__link> PushDownPredicates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushPredicateThroughJoin/ class=md-nav__link> PushPredicateThroughJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReorderJoin/ class=md-nav__link> ReorderJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExpressions/ class=md-nav__link> ReplaceExpressions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExceptWithAntiJoin/ class=md-nav__link> ReplaceExceptWithAntiJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExceptWithFilter/ class=md-nav__link> ReplaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewriteCorrelatedScalarSubquery/ class=md-nav__link> RewriteCorrelatedScalarSubquery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewriteExceptAll/ class=md-nav__link> RewriteExceptAll </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewritePredicateSubquery/ class=md-nav__link> RewritePredicateSubquery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/SchemaPruning/ class=md-nav__link> SchemaPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/SimplifyCasts/ class=md-nav__link> SimplifyCasts </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/V2ScanRelationPushDown/ class=md-nav__link> V2ScanRelationPushDown </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-18 type=checkbox id=nav-1-18> <label class=md-nav__link for=nav-1-18> Execution Planning Strategies <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Execution Planning Strategies" data-md-level=2> <label class=md-nav__title for=nav-1-18> <span class="md-nav__icon md-icon"></span> Execution Planning Strategies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../execution-planning-strategies/SparkStrategy/ class=md-nav__link> SparkStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/Aggregation/ class=md-nav__link> Aggregation </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/BasicOperators/ class=md-nav__link> BasicOperators </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/DataSourceStrategy/ class=md-nav__link> DataSourceStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/DataSourceV2Strategy/ class=md-nav__link> DataSourceV2Strategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/FileSourceStrategy/ class=md-nav__link> FileSourceStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/InMemoryScans/ class=md-nav__link> InMemoryScans </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/JoinSelection/ class=md-nav__link> JoinSelection </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/LogicalQueryStageStrategy/ class=md-nav__link> LogicalQueryStageStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/SpecialLimits/ class=md-nav__link> SpecialLimits </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/SparkStrategies/ class=md-nav__link> SparkStrategies </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-19 type=checkbox id=nav-1-19> <label class=md-nav__link for=nav-1-19> Physical Optimizations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Optimizations" data-md-level=2> <label class=md-nav__title for=nav-1-19> <span class="md-nav__icon md-icon"></span> Physical Optimizations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-optimizations/ApplyColumnarRulesAndInsertTransitions/ class=md-nav__link> ApplyColumnarRulesAndInsertTransitions </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/CoalesceShufflePartitions/ class=md-nav__link> CoalesceShufflePartitions </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/CollapseCodegenStages/ class=md-nav__link> CollapseCodegenStages </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/EnsureRequirements/ class=md-nav__link> EnsureRequirements </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ExtractPythonUDFs/ class=md-nav__link> ExtractPythonUDFs </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/InsertAdaptiveSparkPlan/ class=md-nav__link> InsertAdaptiveSparkPlan </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/OptimizeLocalShuffleReader/ class=md-nav__link> OptimizeLocalShuffleReader </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/OptimizeSkewedJoin/ class=md-nav__link> OptimizeSkewedJoin </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/PlanAdaptiveSubqueries/ class=md-nav__link> PlanAdaptiveSubqueries </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/PlanDynamicPruningFilters/ class=md-nav__link> PlanDynamicPruningFilters </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/PlanSubqueries/ class=md-nav__link> PlanSubqueries </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ReuseAdaptiveSubquery/ class=md-nav__link> ReuseAdaptiveSubquery </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ReuseExchange/ class=md-nav__link> ReuseExchange </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ReuseSubquery/ class=md-nav__link> ReuseSubquery </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20 type=checkbox id=nav-1-20> <label class=md-nav__link for=nav-1-20> Project Tungsten <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Project Tungsten" data-md-level=2> <label class=md-nav__title for=nav-1-20> <span class="md-nav__icon md-icon"></span> Project Tungsten </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-tungsten/ class=md-nav__link> Tungsten Execution Backend </a> </li> <li class=md-nav__item> <a href=../InternalRow/ class=md-nav__link> InternalRow </a> </li> <li class=md-nav__item> <a href=../UnsafeRow/ class=md-nav__link> UnsafeRow </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20-4 type=checkbox id=nav-1-20-4> <label class=md-nav__link for=nav-1-20-4> AggregationIterator <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=AggregationIterator data-md-level=3> <label class=md-nav__title for=nav-1-20-4> <span class="md-nav__icon md-icon"></span> AggregationIterator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-AggregationIterator/ class=md-nav__link> AggregationIterator </a> </li> <li class=md-nav__item> <a href=../spark-sql-ObjectAggregationIterator/ class=md-nav__link> ObjectAggregationIterator </a> </li> <li class=md-nav__item> <a href=../spark-sql-SortBasedAggregationIterator/ class=md-nav__link> SortBasedAggregationIterator </a> </li> <li class=md-nav__item> <a href=../spark-sql-TungstenAggregationIterator/ class=md-nav__link> TungstenAggregationIterator </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-CatalystSerde/ class=md-nav__link> CatalystSerde </a> </li> <li class=md-nav__item> <a href=../spark-sql-ExternalAppendOnlyUnsafeRowArray/ class=md-nav__link> ExternalAppendOnlyUnsafeRowArray </a> </li> <li class=md-nav__item> <a href=../spark-sql-UnsafeFixedWidthAggregationMap/ class=md-nav__link> UnsafeFixedWidthAggregationMap </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> New &amp; Noteworthy <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="New & Noteworthy" data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> New &amp; Noteworthy </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../new-and-noteworthy/adaptive-query-execution/ class=md-nav__link> Adaptive Query Execution </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/ class=md-nav__link> Catalog Plugin API and Multi-Catalog Support </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/dynamic-partition-pruning/ class=md-nav__link> Dynamic Partition Pruning </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/explain-command-improved/ class=md-nav__link> Explaining Query Plans Improved </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/hint-framework/ class=md-nav__link> Hint Framework </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/join-strategy-hints/ class=md-nav__link> Join Strategy Hints </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/observable-metrics/ class=md-nav__link> Observable Metrics </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/columnar-processing/ class=md-nav__link> Columnar Processing </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/datasource-v2/ class=md-nav__link> DataSource V2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-hive-integration/ class=md-nav__link> Hive Integration </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-11 type=checkbox id=nav-2-11> <label class=md-nav__link for=nav-2-11> Vectorized Parquet Decoding <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Vectorized Parquet Decoding" data-md-level=2> <label class=md-nav__title for=nav-2-11> <span class="md-nav__icon md-icon"></span> Vectorized Parquet Decoding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../vectorized-parquet-reader/ class=md-nav__link> Vectorized Parquet Decoding </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-11-2 type=checkbox id=nav-2-11-2> <label class=md-nav__link for=nav-2-11-2> ColumnVectors <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ColumnVectors data-md-level=3> <label class=md-nav__title for=nav-2-11-2> <span class="md-nav__icon md-icon"></span> ColumnVectors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ColumnVector/ class=md-nav__link> ColumnVector </a> </li> <li class=md-nav__item> <a href=../WritableColumnVector/ class=md-nav__link> WritableColumnVector </a> </li> <li class=md-nav__item> <a href=../OnHeapColumnVector/ class=md-nav__link> OnHeapColumnVector </a> </li> <li class=md-nav__item> <a href=../OffHeapColumnVector/ class=md-nav__link> OffHeapColumnVector </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-dynamic-partition-inserts/ class=md-nav__link> Dynamic Partition Inserts </a> </li> <li class=md-nav__item> <a href=../spark-sql-bucketing/ class=md-nav__link> Bucketing </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-14 type=checkbox id=nav-2-14> <label class=md-nav__link for=nav-2-14> Whole-Stage CodeGen <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Whole-Stage CodeGen" data-md-level=2> <label class=md-nav__title for=nav-2-14> <span class="md-nav__icon md-icon"></span> Whole-Stage CodeGen </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-whole-stage-codegen/ class=md-nav__link> Whole-Stage Java Code Generation </a> </li> <li class=md-nav__item> <a href=../spark-sql-CodegenContext/ class=md-nav__link> CodegenContext </a> </li> <li class=md-nav__item> <a href=../spark-sql-CodeGenerator/ class=md-nav__link> CodeGenerator </a> </li> <li class=md-nav__item> <a href=../spark-sql-GenerateColumnAccessor/ class=md-nav__link> GenerateColumnAccessor </a> </li> <li class=md-nav__item> <a href=../spark-sql-GenerateOrdering/ class=md-nav__link> GenerateOrdering </a> </li> <li class=md-nav__item> <a href=../spark-sql-GeneratePredicate/ class=md-nav__link> GeneratePredicate </a> </li> <li class=md-nav__item> <a href=../spark-sql-GenerateSafeProjection/ class=md-nav__link> GenerateSafeProjection </a> </li> <li class=md-nav__item> <a href=../spark-sql-BytesToBytesMap/ class=md-nav__link> BytesToBytesMap </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-14-9 type=checkbox id=nav-2-14-9> <label class=md-nav__link for=nav-2-14-9> Subexpression Elimination <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Subexpression Elimination" data-md-level=3> <label class=md-nav__title for=nav-2-14-9> <span class="md-nav__icon md-icon"></span> Subexpression Elimination </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-subexpression-elimination/ class=md-nav__link> Subexpression Elimination In Code-Generated Expression Evaluation (Common Expression Reuse) </a> </li> <li class=md-nav__item> <a href=../spark-sql-EquivalentExpressions/ class=md-nav__link> EquivalentExpressions </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-15 type=checkbox id=nav-2-15> <label class=md-nav__link for=nav-2-15> Vectorized Query Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Vectorized Query Execution" data-md-level=2> <label class=md-nav__title for=nav-2-15> <span class="md-nav__icon md-icon"></span> Vectorized Query Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-vectorized-query-execution/ class=md-nav__link> Vectorized Query Execution </a> </li> <li class=md-nav__item> <a href=../ColumnarBatch/ class=md-nav__link> ColumnarBatch </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-subqueries/ class=md-nav__link> Subqueries </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2-17 type=checkbox id=nav-2-17> <label class=md-nav__link for=nav-2-17> Cost-Based Optimization <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Cost-Based Optimization" data-md-level=2> <label class=md-nav__title for=nav-2-17> <span class="md-nav__icon md-icon"></span> Cost-Based Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-cost-based-optimization/ class=md-nav__link> Cost-Based Optimization (CBO) of Logical Query Plan </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalogStatistics/ class=md-nav__link> CatalogStatistics </a> </li> <li class=md-nav__item> <a href=../spark-sql-ColumnStat/ class=md-nav__link> ColumnStat </a> </li> <li class=md-nav__item> <a href=../CommandUtils/ class=md-nav__link> CommandUtils </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-catalyst-dsl/ class=md-nav__link> Catalyst DSL </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> New in Spark 3.0.0 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="New in Spark 3.0.0" data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> New in Spark 3.0.0 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../FileScanBuilder/ class=md-nav__link> FileScanBuilder </a> </li> <li class=md-nav__item> <a href=../FileDataSourceV2/ class=md-nav__link> FileDataSourceV2 </a> </li> <li class=md-nav__item> <a href=../Block/ class=md-nav__link> Block </a> </li> <li class=md-nav__item> <a href=../FileScan/ class=md-nav__link> FileScan </a> </li> <li class=md-nav__item> <a href=../new-in-300/DataFrameWriterV2/ class=md-nav__link> DataFrameWriterV2 </a> </li> <li class=md-nav__item> <a href=../new-in-300/CreateTableWriter/ class=md-nav__link> CreateTableWriter </a> </li> <li class=md-nav__item> <a href=../new-in-300/WriteConfigMethods/ class=md-nav__link> WriteConfigMethods </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> RDDs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=RDDs data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> RDDs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rdds/FileScanRDD/ class=md-nav__link> FileScanRDD </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> PySpark <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=PySpark data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> PySpark </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../pyspark/Setup/ class=md-nav__link> Setup </a> </li> <li class=md-nav__item> <a href=../pyspark/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonWorkerFactory/ class=md-nav__link> PythonWorkerFactory </a> </li> <li class=md-nav__item> <a href=../pyspark/MonitorThread/ class=md-nav__link> MonitorThread </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonUDF/ class=md-nav__link> PythonUDF </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonFunction/ class=md-nav__link> PythonFunction </a> </li> <li class=md-nav__item> <a href=../pyspark/AggregateInPandasExec/ class=md-nav__link> AggregateInPandasExec </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonRDD/ class=md-nav__link> PythonRDD </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonForeachWriter/ class=md-nav__link> PythonForeachWriter </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5-10 type=checkbox id=nav-5-10> <label class=md-nav__link for=nav-5-10> Python Runners <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Python Runners" data-md-level=2> <label class=md-nav__title for=nav-5-10> <span class="md-nav__icon md-icon"></span> Python Runners </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../pyspark/BasePythonRunner/ class=md-nav__link> BasePythonRunner </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonRunner/ class=md-nav__link> PythonRunner </a> </li> <li class=md-nav__item> <a href=../pyspark/ArrowPythonRunner/ class=md-nav__link> ArrowPythonRunner </a> </li> <li class=md-nav__item> <a href=../pyspark/PythonUDFRunner/ class=md-nav__link> PythonUDFRunner </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> Demos <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Demos data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../demo/connecting-spark-sql-to-hive-metastore/ class=md-nav__link> Connecting Spark SQL to Hive Metastore </a> </li> <li class=md-nav__item> <a href=../demo/hive-partitioned-parquet-table-partition-pruning/ class=md-nav__link> Hive Partitioned Parquet Table and Partition Pruning </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7 type=checkbox id=nav-7> <label class=md-nav__link for=nav-7> Extras <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Extras data-md-level=1> <label class=md-nav__title for=nav-7> <span class="md-nav__icon md-icon"></span> Extras </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql/ class=md-nav__link> Spark SQL </a> </li> <li class=md-nav__item> <a href=../spark-sql-dataset-rdd/ class=md-nav__link> Dataset, DataFrame and RDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-dataset-vs-sql/ class=md-nav__link> Dataset and SQL </a> </li> <li class=md-nav__item> <a href=../DataSourceV2Utils/ class=md-nav__link> DataSourceV2Utils </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5 type=checkbox id=nav-7-5> <label class=md-nav__link for=nav-7-5> Developing Applications <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Developing Applications" data-md-level=2> <label class=md-nav__title for=nav-7-5> <span class="md-nav__icon md-icon"></span> Developing Applications </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-fundamentals-spark-application-development/ class=md-nav__link> Fundamentals of Spark SQL Application Development </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-2 type=checkbox id=nav-7-5-2> <label class=md-nav__link for=nav-7-5-2> Dataset <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Dataset data-md-level=3> <label class=md-nav__title for=nav-7-5-2> <span class="md-nav__icon md-icon"></span> Dataset </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Dataset/ class=md-nav__link> Dataset </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-2-2 type=checkbox id=nav-7-5-2-2> <label class=md-nav__link for=nav-7-5-2-2> Encoder <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Encoder data-md-level=4> <label class=md-nav__title for=nav-7-5-2-2> <span class="md-nav__icon md-icon"></span> Encoder </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-Encoder/ class=md-nav__link> Encoder </a> </li> <li class=md-nav__item> <a href=../spark-sql-Encoders/ class=md-nav__link> Encoders </a> </li> <li class=md-nav__item> <a href=../spark-sql-ExpressionEncoder/ class=md-nav__link> ExpressionEncoder </a> </li> <li class=md-nav__item> <a href=../RowEncoder/ class=md-nav__link> RowEncoder </a> </li> <li class=md-nav__item> <a href=../spark-sql-ExpressionEncoder-LocalDateTime/ class=md-nav__link> LocalDateTimeEncoder </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-DataFrame/ class=md-nav__link> DataFrame &mdash; Dataset of Rows with RowEncoder </a> </li> <li class=md-nav__item> <a href=../spark-sql-Row/ class=md-nav__link> Row </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-3 type=checkbox id=nav-7-5-3> <label class=md-nav__link for=nav-7-5-3> Data Source API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Source API" data-md-level=3> <label class=md-nav__title for=nav-7-5-3> <span class="md-nav__icon md-icon"></span> Data Source API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-datasource-api/ class=md-nav__link> DataSource API </a> </li> <li class=md-nav__item> <a href=../DataFrameReader/ class=md-nav__link> DataFrameReader </a> </li> <li class=md-nav__item> <a href=../DataFrameWriter/ class=md-nav__link> DataFrameWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-4 type=checkbox id=nav-7-5-4> <label class=md-nav__link for=nav-7-5-4> Dataset API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Dataset API" data-md-level=3> <label class=md-nav__title for=nav-7-5-4> <span class="md-nav__icon md-icon"></span> Dataset API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-dataset-operators/ class=md-nav__link> Dataset API &mdash; Dataset Operators </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-typed-transformations/ class=md-nav__link> Dataset API &mdash; Typed Transformations </a> </li> <li class=md-nav__item> <a href=../Dataset-untyped-transformations/ class=md-nav__link> Untyped Transformations </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-basic-actions/ class=md-nav__link> Dataset API &mdash; Basic Actions </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-actions/ class=md-nav__link> Actions </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataFrameNaFunctions/ class=md-nav__link> DataFrameNaFunctions &mdash; Working With Missing Data </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataFrameStatFunctions/ class=md-nav__link> DataFrameStatFunctions </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-5 type=checkbox id=nav-7-5-5> <label class=md-nav__link for=nav-7-5-5> Column <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Column data-md-level=3> <label class=md-nav__title for=nav-7-5-5> <span class="md-nav__icon md-icon"></span> Column </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-Column/ class=md-nav__link> Column </a> </li> <li class=md-nav__item> <a href=../spark-sql-column-operators/ class=md-nav__link> Column Operators </a> </li> <li class=md-nav__item> <a href=../spark-sql-TypedColumn/ class=md-nav__link> TypedColumn </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-6 type=checkbox id=nav-7-5-6> <label class=md-nav__link for=nav-7-5-6> Basic Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Basic Aggregation" data-md-level=3> <label class=md-nav__title for=nav-7-5-6> <span class="md-nav__icon md-icon"></span> Basic Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-basic-aggregation/ class=md-nav__link> Basic Aggregation </a> </li> <li class=md-nav__item> <a href=../spark-sql-RelationalGroupedDataset/ class=md-nav__link> RelationalGroupedDataset &mdash; Untyped Row-based Grouping </a> </li> <li class=md-nav__item> <a href=../spark-sql-KeyValueGroupedDataset/ class=md-nav__link> KeyValueGroupedDataset </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-7 type=checkbox id=nav-7-5-7> <label class=md-nav__link for=nav-7-5-7> Joins <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Joins data-md-level=3> <label class=md-nav__title for=nav-7-5-7> <span class="md-nav__icon md-icon"></span> Joins </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-joins/ class=md-nav__link> Joins </a> </li> <li class=md-nav__item> <a href=../spark-sql-joins-broadcast/ class=md-nav__link> Broadcast Joins </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-8 type=checkbox id=nav-7-5-8> <label class=md-nav__link for=nav-7-5-8> Window Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Window Aggregation" data-md-level=3> <label class=md-nav__title for=nav-7-5-8> <span class="md-nav__icon md-icon"></span> Window Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-window-aggregation/ class=md-nav__link> Window Aggregation </a> </li> <li class=md-nav__item> <a href=../spark-sql-WindowSpec/ class=md-nav__link> WindowSpec </a> </li> <li class=md-nav__item> <a href=../spark-sql-WindowSpec-Window/ class=md-nav__link> Window Utility </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-9 type=checkbox id=nav-7-5-9> <label class=md-nav__link for=nav-7-5-9> Standard Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Standard Functions" data-md-level=3> <label class=md-nav__title for=nav-7-5-9> <span class="md-nav__icon md-icon"></span> Standard Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-functions/ class=md-nav__link> Standard Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-aggregate-functions/ class=md-nav__link> Aggregate Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-collection/ class=md-nav__link> Standard Functions for Collections (Collection Functions) </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-datetime/ class=md-nav__link> Date and Time Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-regular-functions/ class=md-nav__link> Regular Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-windows/ class=md-nav__link> Window Functions </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-10 type=checkbox id=nav-7-5-10> <label class=md-nav__link for=nav-7-5-10> User-Defined Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="User-Defined Functions" data-md-level=3> <label class=md-nav__title for=nav-7-5-10> <span class="md-nav__icon md-icon"></span> User-Defined Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-udfs/ class=md-nav__link> User-Defined Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-udfs-blackbox/ class=md-nav__link> UDFs are Blackbox </a> </li> <li class=md-nav__item> <a href=../spark-sql-UserDefinedFunction/ class=md-nav__link> UserDefinedFunction </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-11 type=checkbox id=nav-7-5-11> <label class=md-nav__link for=nav-7-5-11> Schema <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Schema data-md-level=3> <label class=md-nav__title for=nav-7-5-11> <span class="md-nav__icon md-icon"></span> Schema </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-schema/ class=md-nav__link> Schema &mdash; Structure of Data </a> </li> <li class=md-nav__item> <a href=../StructType/ class=md-nav__link> StructType </a> </li> <li class=md-nav__item> <a href=../spark-sql-StructField/ class=md-nav__link> StructField &mdash; Single Field in StructType </a> </li> <li class=md-nav__item> <a href=../DataType/ class=md-nav__link> DataType </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-multi-dimensional-aggregation/ class=md-nav__link> Multi-Dimensional Aggregation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-5-13 type=checkbox id=nav-7-5-13> <label class=md-nav__link for=nav-7-5-13> Caching and Persistence <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Caching and Persistence" data-md-level=3> <label class=md-nav__title for=nav-7-5-13> <span class="md-nav__icon md-icon"></span> Caching and Persistence </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-caching-and-persistence/ class=md-nav__link> Caching and Persistence </a> </li> <li class=md-nav__item> <a href=../spark-sql-caching-webui-storage/ class=md-nav__link> User-Friendly Names Of Cached Queries in web UI's Storage Tab </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-checkpointing/ class=md-nav__link> Checkpointing </a> </li> <li class=md-nav__item> <a href=../UserDefinedAggregateFunction/ class=md-nav__link> UserDefinedAggregateFunction </a> </li> <li class=md-nav__item> <a href=../Aggregator/ class=md-nav__link> Aggregator </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-6 type=checkbox id=nav-7-6> <label class=md-nav__link for=nav-7-6> Monitoring <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Monitoring data-md-level=2> <label class=md-nav__title for=nav-7-6> <span class="md-nav__icon md-icon"></span> Monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-6-1 type=checkbox id=nav-7-6-1> <label class=md-nav__link for=nav-7-6-1> Web UI and SQL Tab <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Web UI and SQL Tab" data-md-level=3> <label class=md-nav__title for=nav-7-6-1> <span class="md-nav__icon md-icon"></span> Web UI and SQL Tab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-webui/ class=md-nav__link> SQLTab </a> </li> <li class=md-nav__item> <a href=../spark-sql-SQLListener/ class=md-nav__link> SQLListener </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-QueryExecutionListener/ class=md-nav__link> QueryExecutionListener </a> </li> <li class=md-nav__item> <a href=../spark-sql-SQLAppStatusListener/ class=md-nav__link> SQLAppStatusListener </a> </li> <li class=md-nav__item> <a href=../spark-sql-SQLAppStatusPlugin/ class=md-nav__link> SQLAppStatusPlugin </a> </li> <li class=md-nav__item> <a href=../spark-sql-SQLAppStatusStore/ class=md-nav__link> SQLAppStatusStore </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-6-6 type=checkbox id=nav-7-6-6> <label class=md-nav__link for=nav-7-6-6> WriteTaskStats <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteTaskStats data-md-level=3> <label class=md-nav__title for=nav-7-6-6> <span class="md-nav__icon md-icon"></span> WriteTaskStats </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-WriteTaskStats/ class=md-nav__link> WriteTaskStats </a> </li> <li class=md-nav__item> <a href=../spark-sql-BasicWriteTaskStats/ class=md-nav__link> BasicWriteTaskStats </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-6-7 type=checkbox id=nav-7-6-7> <label class=md-nav__link for=nav-7-6-7> WriteTaskStatsTracker <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteTaskStatsTracker data-md-level=3> <label class=md-nav__title for=nav-7-6-7> <span class="md-nav__icon md-icon"></span> WriteTaskStatsTracker </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-WriteTaskStatsTracker/ class=md-nav__link> WriteTaskStatsTracker </a> </li> <li class=md-nav__item> <a href=../spark-sql-BasicWriteTaskStatsTracker/ class=md-nav__link> BasicWriteTaskStatsTracker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-6-8 type=checkbox id=nav-7-6-8> <label class=md-nav__link for=nav-7-6-8> WriteJobStatsTracker <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteJobStatsTracker data-md-level=3> <label class=md-nav__title for=nav-7-6-8> <span class="md-nav__icon md-icon"></span> WriteJobStatsTracker </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-WriteJobStatsTracker/ class=md-nav__link> WriteJobStatsTracker </a> </li> <li class=md-nav__item> <a href=../spark-sql-BasicWriteJobStatsTracker/ class=md-nav__link> BasicWriteJobStatsTracker </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-logging/ class=md-nav__link> Logging </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-7 type=checkbox id=nav-7-7> <label class=md-nav__link for=nav-7-7> Performance Tuning and Debugging <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Performance Tuning and Debugging" data-md-level=2> <label class=md-nav__title for=nav-7-7> <span class="md-nav__icon md-icon"></span> Performance Tuning and Debugging </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-performance-tuning/ class=md-nav__link> Performance Tuning </a> </li> <li class=md-nav__item> <a href=../spark-sql-performance-tuning-groupBy-aggregation/ class=md-nav__link> Case Study </a> </li> <li class=md-nav__item> <a href=../spark-sql-debugging-query-execution/ class=md-nav__link> Debugging Query Execution </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../tools/spark-sql-spark-sql/ class=md-nav__link> Tools </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-7-9 type=checkbox id=nav-7-9> <label class=md-nav__link for=nav-7-9> Spark Thrift Server <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Spark Thrift Server" data-md-level=2> <label class=md-nav__title for=nav-7-9> <span class="md-nav__icon md-icon"></span> Spark Thrift Server </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../thrift-server/spark-sql-thrift-server/ class=md-nav__link> HiveThriftServer2 </a> </li> <li class=md-nav__item> <a href=../thrift-server/spark-sql-thriftserver-SparkSQLEnv/ class=md-nav__link> SparkSQLEnv </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-SQLExecution/ class=md-nav__link> SQLExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-RDDConversions/ class=md-nav__link> RDDConversions </a> </li> <li class=md-nav__item> <a href=../ShuffledRowRDD/ class=md-nav__link> ShuffledRowRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-UnsupportedOperationChecker/ class=md-nav__link> UnsupportedOperationChecker </a> </li> <li class=md-nav__item> <a href=../spark-sql-Analyzer-CheckAnalysis/ class=md-nav__link> CheckAnalysis </a> </li> <li class=md-nav__item> <a href=../CatalystTypeConverters/ class=md-nav__link> CatalystTypeConverters </a> </li> <li class=md-nav__item> <a href=../spark-sql-StatFunctions/ class=md-nav__link> StatFunctions </a> </li> <li class=md-nav__item> <a href=../spark-sql-SubExprUtils/ class=md-nav__link> SubExprUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-PredicateHelper/ class=md-nav__link> PredicateHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-DDLUtils/ class=md-nav__link> DDLUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-SchemaUtils/ class=md-nav__link> SchemaUtils </a> </li> <li class=md-nav__item> <a href=../AggUtils/ class=md-nav__link> AggUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-ScalaReflection/ class=md-nav__link> ScalaReflection </a> </li> <li class=md-nav__item> <a href=../spark-sql-CreateStruct/ class=md-nav__link> CreateStruct </a> </li> <li class=md-nav__item> <a href=../spark-sql-MultiInstanceRelation/ class=md-nav__link> MultiInstanceRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-TypeCoercion/ class=md-nav__link> TypeCoercion </a> </li> <li class=md-nav__item> <a href=../TypeCoercionRule/ class=md-nav__link> TypeCoercionRule </a> </li> <li class=md-nav__item> <a href=../spark-sql-ExtractEquiJoinKeys/ class=md-nav__link> ExtractEquiJoinKeys </a> </li> <li class=md-nav__item> <a href=../spark-sql-PhysicalAggregation/ class=md-nav__link> PhysicalAggregation </a> </li> <li class=md-nav__item> <a href=../spark-sql-PhysicalOperation/ class=md-nav__link> PhysicalOperation </a> </li> <li class=md-nav__item> <a href=../KnownSizeEstimation/ class=md-nav__link> KnownSizeEstimation </a> </li> <li class=md-nav__item> <a href=../spark-sql-SizeEstimator/ class=md-nav__link> SizeEstimator </a> </li> <li class=md-nav__item> <a href=../spark-sql-PartitioningUtils/ class=md-nav__link> PartitioningUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-spark-HadoopFileLinesReader/ class=md-nav__link> HadoopFileLinesReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalogUtils/ class=md-nav__link> CatalogUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-ExternalCatalogUtils/ class=md-nav__link> ExternalCatalogUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-BufferedRowIterator/ class=md-nav__link> BufferedRowIterator </a> </li> <li class=md-nav__item> <a href=../spark-sql-CompressionCodecs/ class=md-nav__link> CompressionCodecs </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/AdaptiveExecutionContext/ class=md-nav__link> AdaptiveExecutionContext </a> </li> <li class=md-nav__item> <a href=../JoinStrategyHint/ class=md-nav__link> JoinStrategyHint </a> </li> <li class=md-nav__item> <a href=../spark-sql-SQLContext/ class=md-nav__link> SQLContext </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#sparksqladaptiveforceapply class=md-nav__link> spark.sql.adaptive.forceApply </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveloglevel class=md-nav__link> spark.sql.adaptive.logLevel </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsenabled class=md-nav__link> spark.sql.adaptive.coalescePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveadvisorypartitionsizeinbytes class=md-nav__link> spark.sql.adaptive.advisoryPartitionSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsinitialpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.initialPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveenabled class=md-nav__link> spark.sql.adaptive.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivefetchshuffleblocksinbatch class=md-nav__link> spark.sql.adaptive.fetchShuffleBlocksInBatch </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivelocalshufflereaderenabled class=md-nav__link> spark.sql.adaptive.localShuffleReader.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinenabled class=md-nav__link> spark.sql.adaptive.skewJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionfactor class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin class=md-nav__link> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzermaxiterations class=md-nav__link> spark.sql.analyzer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzerfailambiguousselfjoin class=md-nav__link> spark.sql.analyzer.failAmbiguousSelfJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlansienabled class=md-nav__link> spark.sql.ansi.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenwholestage class=md-nav__link> spark.sql.codegen.wholeStage </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmethodsplitthreshold class=md-nav__link> spark.sql.codegen.methodSplitThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqldebugmaxtostringfields class=md-nav__link> spark.sql.debug.maxToStringFields </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultcatalog class=md-nav__link> spark.sql.defaultCatalog </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionreusesubquery class=md-nav__link> spark.sql.execution.reuseSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionsortbeforerepartition class=md-nav__link> spark.sql.execution.sortBeforeRepartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionrangeexchangesamplesizeperpartition class=md-nav__link> spark.sql.execution.rangeExchange.sampleSizePerPartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkfallbackenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.fallback.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowsparkrenabled class=md-nav__link> spark.sql.execution.arrow.sparkr.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasudfbuffersize class=md-nav__link> spark.sql.execution.pandas.udf.buffer.size </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasconverttoarrowarraysafely class=md-nav__link> spark.sql.execution.pandas.convertToArrowArraySafely </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramenabled class=md-nav__link> spark.sql.statistics.histogram.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsessiontimezone class=md-nav__link> spark.sql.session.timeZone </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcescommitprotocolclass class=md-nav__link> spark.sql.sources.commitProtocolClass </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesignoredatalocality class=md-nav__link> spark.sql.sources.ignoreDataLocality </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesvalidatepartitioncolumns class=md-nav__link> spark.sql.sources.validatePartitionColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesusev1sourcelist class=md-nav__link> spark.sql.sources.useV1SourceList </a> </li> <li class=md-nav__item> <a href=#sparksqlstoreassignmentpolicy class=md-nav__link> spark.sql.storeAssignmentPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetswitchthreshold class=md-nav__link> spark.sql.optimizer.inSetSwitchThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangeloglevel class=md-nav__link> spark.sql.optimizer.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogrules class=md-nav__link> spark.sql.optimizer.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogbatches class=md-nav__link> spark.sql.optimizer.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningenabled class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningusestats class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.useStats </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources class=md-nav__link> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerserializernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexpressionnestedpruningenabled class=md-nav__link> spark.sql.optimizer.expression.nestedPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcmergeschema class=md-nav__link> spark.sql.orc.mergeSchema </a> </li> <li class=md-nav__item> <a href=#sparksqldatetimejava8apienabled class=md-nav__link> spark.sql.datetime.java8API.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbinaryfilemaxlength class=md-nav__link> spark.sql.sources.binaryFile.maxLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmapkeydeduppolicy class=md-nav__link> spark.sql.mapKeyDedupPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlmavenadditionalremoterepositories class=md-nav__link> spark.sql.maven.additionalRemoteRepositories </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxplanstringlength class=md-nav__link> spark.sql.maxPlanStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqladdpartitioninbatchsize class=md-nav__link> spark.sql.addPartitionInBatch.size </a> </li> <li class=md-nav__item> <a href=#sparksqlscripttransformationexittimeoutinseconds class=md-nav__link> spark.sql.scriptTransformation.exitTimeoutInSeconds </a> </li> <li class=md-nav__item> <a href=#sparksqlautobroadcastjointhreshold class=md-nav__link> spark.sql.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqlavrocompressioncodec class=md-nav__link> spark.sql.avro.compression.codec </a> </li> <li class=md-nav__item> <a href=#sparksqlbroadcasttimeout class=md-nav__link> spark.sql.broadcastTimeout </a> </li> <li class=md-nav__item> <a href=#sparksqlcasesensitive class=md-nav__link> spark.sql.caseSensitive </a> </li> <li class=md-nav__item> <a href=#sparksqlcatalogspark_catalog class=md-nav__link> spark.sql.catalog.spark_catalog </a> </li> <li class=md-nav__item> <a href=#sparksqlcboenabled class=md-nav__link> spark.sql.cbo.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbojoinreorderenabled class=md-nav__link> spark.sql.cbo.joinReorder.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcboplanstatsenabled class=md-nav__link> spark.sql.cbo.planStats.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbostarschemadetection class=md-nav__link> spark.sql.cbo.starSchemaDetection </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatemapvectorizedenable class=md-nav__link> spark.sql.codegen.aggregate.map.vectorized.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatesplitaggregatefuncenabled class=md-nav__link> spark.sql.codegen.aggregate.splitAggregateFunc.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegencomments class=md-nav__link> spark.sql.codegen.comments </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfactorymode class=md-nav__link> spark.sql.codegen.factoryMode </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfallback class=md-nav__link> spark.sql.codegen.fallback </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenhugemethodlimit class=md-nav__link> spark.sql.codegen.hugeMethodLimit </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenuseidinclassname class=md-nav__link> spark.sql.codegen.useIdInClassName </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmaxfields class=md-nav__link> spark.sql.codegen.maxFields </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegensplitconsumefuncbyoperator class=md-nav__link> spark.sql.codegen.splitConsumeFuncByOperator </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnvectoroffheapenabled class=md-nav__link> spark.sql.columnVector.offheap.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnnameofcorruptrecord class=md-nav__link> spark.sql.columnNameOfCorruptRecord </a> </li> <li class=md-nav__item> <a href=#sparksqlconstraintpropagationenabled class=md-nav__link> spark.sql.constraintPropagation.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcsvfilterpushdownenabled class=md-nav__link> spark.sql.csv.filterPushdown.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultsizeinbytes class=md-nav__link> spark.sql.defaultSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqldialect class=md-nav__link> spark.sql.dialect </a> </li> <li class=md-nav__item> <a href=#sparksqlexchangereuse class=md-nav__link> spark.sql.exchange.reuse </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionuseobjecthashaggregateexec class=md-nav__link> spark.sql.execution.useObjectHashAggregateExec </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignorecorruptfiles class=md-nav__link> spark.sql.files.ignoreCorruptFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignoremissingfiles class=md-nav__link> spark.sql.files.ignoreMissingFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxrecordsperfile class=md-nav__link> spark.sql.files.maxRecordsPerFile </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxpartitionbytes class=md-nav__link> spark.sql.files.maxPartitionBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesopencostinbytes class=md-nav__link> spark.sql.files.openCostInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragecompressed class=md-nav__link> spark.sql.inMemoryColumnarStorage.compressed </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragebatchsize class=md-nav__link> spark.sql.inMemoryColumnarStorage.batchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorytablescanstatisticsenable class=md-nav__link> spark.sql.inMemoryTableScanStatistics.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstorageenablevectorizedreader class=md-nav__link> spark.sql.inMemoryColumnarStorage.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragepartitionpruning class=md-nav__link> spark.sql.inMemoryColumnarStorage.partitionPruning </a> </li> <li class=md-nav__item> <a href=#sparksqljoinprefersortmergejoin class=md-nav__link> spark.sql.join.preferSortMergeJoin </a> </li> <li class=md-nav__item> <a href=#sparksqljsongeneratorignorenullfields class=md-nav__link> spark.sql.jsonGenerator.ignoreNullFields </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydolooseupcast class=md-nav__link> spark.sql.legacy.doLooseUpcast </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycteprecedencepolicy class=md-nav__link> spark.sql.legacy.ctePrecedencePolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytimeparserpolicy class=md-nav__link> spark.sql.legacy.timeParserPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfollowthreevaluedlogicinarrayexists class=md-nav__link> spark.sql.legacy.followThreeValuedLogicInArrayExists </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfromdaytimestringenabled class=md-nav__link> spark.sql.legacy.fromDayTimeString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacynotreserveproperties class=md-nav__link> spark.sql.legacy.notReserveProperties </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyaddsinglefileinaddfile class=md-nav__link> spark.sql.legacy.addSingleFileInAddFile </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyexponentliteralasdecimalenabled class=md-nav__link> spark.sql.legacy.exponentLiteralAsDecimal.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownegativescaleofdecimal class=md-nav__link> spark.sql.legacy.allowNegativeScaleOfDecimal </a> </li> <li class=md-nav__item> <a href=#sparksqllegacybucketedtablescanoutputordering class=md-nav__link> spark.sql.legacy.bucketedTableScan.outputOrdering </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyjsonallowemptystringenabled class=md-nav__link> spark.sql.legacy.json.allowEmptyString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycreateemptycollectionusingstringtype class=md-nav__link> spark.sql.legacy.createEmptyCollectionUsingStringType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowuntypedscalaudf class=md-nav__link> spark.sql.legacy.allowUntypedScalaUDF </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue class=md-nav__link> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue </a> </li> <li class=md-nav__item> <a href=#sparksqllegacysetcommandrejectssparkcoreconfs class=md-nav__link> spark.sql.legacy.setCommandRejectsSparkCoreConfs </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytypecoerciondatetimetostringenabled class=md-nav__link> spark.sql.legacy.typeCoercion.datetimeToString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowhashonmaptype class=md-nav__link> spark.sql.legacy.allowHashOnMapType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyrddapplyconf class=md-nav__link> spark.sql.legacy.rdd.applyConf </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyreplacedatabrickssparkavroenabled class=md-nav__link> spark.sql.legacy.replaceDatabricksSparkAvro.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllimitscaleupfactor class=md-nav__link> spark.sql.limit.scaleUpFactor </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexcludedrules class=md-nav__link> spark.sql.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetconversionthreshold class=md-nav__link> spark.sql.optimizer.inSetConversionThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizermaxiterations class=md-nav__link> spark.sql.optimizer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerreplaceexceptwithfilter class=md-nav__link> spark.sql.optimizer.replaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcimpl class=md-nav__link> spark.sql.orc.impl </a> </li> <li class=md-nav__item> <a href=#sparksqlpysparkjvmstacktraceenabled class=md-nav__link> spark.sql.pyspark.jvmStacktrace.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetbinaryasstring class=md-nav__link> spark.sql.parquet.binaryAsString </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetcolumnarreaderbatchsize class=md-nav__link> spark.sql.parquet.columnarReaderBatchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96astimestamp class=md-nav__link> spark.sql.parquet.int96AsTimestamp </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetenablevectorizedreader class=md-nav__link> spark.sql.parquet.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdown class=md-nav__link> spark.sql.parquet.filterPushdown </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdowndate class=md-nav__link> spark.sql.parquet.filterPushdown.date </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96timestampconversion class=md-nav__link> spark.sql.parquet.int96TimestampConversion </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetrecordlevelfilterenabled class=md-nav__link> spark.sql.parquet.recordLevelFilter.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparserquotedregexcolumnnames class=md-nav__link> spark.sql.parser.quotedRegexColumnNames </a> </li> <li class=md-nav__item> <a href=#sparksqlpivotmaxvalues class=md-nav__link> spark.sql.pivotMaxValues </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionoptionsregex class=md-nav__link> spark.sql.redaction.options.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionstringregex class=md-nav__link> spark.sql.redaction.string.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlretaingroupcolumns class=md-nav__link> spark.sql.retainGroupColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlrunsqlonfiles class=md-nav__link> spark.sql.runSQLOnFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlselfjoinautoresolveambiguity class=md-nav__link> spark.sql.selfJoinAutoResolveAmbiguity </a> </li> <li class=md-nav__item> <a href=#sparksqlsortenableradixsort class=md-nav__link> spark.sql.sort.enableRadixSort </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingenabled class=md-nav__link> spark.sql.sources.bucketing.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesdefault class=md-nav__link> spark.sql.sources.default </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsfallbacktohdfs class=md-nav__link> spark.sql.statistics.fallBackToHdfs </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramnumbins class=md-nav__link> spark.sql.statistics.histogram.numBins </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled class=md-nav__link> spark.sql.statisticsparallelFileListingInStatsComputation.enabled* </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsndvmaxerror class=md-nav__link> spark.sql.statistics.ndv.maxError </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticspercentileaccuracy class=md-nav__link> spark.sql.statistics.percentile.accuracy </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticssizeautoupdateenabled class=md-nav__link> spark.sql.statistics.size.autoUpdate.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsubexpressioneliminationenabled class=md-nav__link> spark.sql.subexpressionElimination.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlshufflepartitions class=md-nav__link> spark.sql.shuffle.partitions </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesfilecompressionfactor class=md-nav__link> spark.sql.sources.fileCompressionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcespartitionoverwritemode class=md-nav__link> spark.sql.sources.partitionOverwriteMode </a> </li> <li class=md-nav__item> <a href=#sparksqltruncatetableignorepermissionaclenabled class=md-nav__link> spark.sql.truncateTable.ignorePermissionAcl.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqluiretainedexecutions class=md-nav__link> spark.sql.ui.retainedExecutions </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferinmemorythreshold class=md-nav__link> spark.sql.windowExec.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferspillthreshold class=md-nav__link> spark.sql.windowExec.buffer.spill.threshold </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/edit/master/docs/configuration-properties.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=configuration-properties>Configuration Properties<a class=headerlink href=#configuration-properties title="Permanent link">&para;</a></h1> <p><strong>Configuration properties</strong> (aka <strong>settings</strong>) allow you to fine-tune a Spark SQL application.</p> <p>Configuration properties are configured in a <a href=../SparkSession/ >SparkSession</a> while creating a new instance using <a href=../SparkSession-Builder/#config>config</a> method (e.g. <a href=../StaticSQLConf/#spark.sql.warehouse.dir>spark.sql.warehouse.dir</a>).</p> <div class=highlight><pre><span></span><code><span class=k>import</span> <span class=nn>org.apache.spark.sql.SparkSession</span>
<span class=k>val</span> <span class=n>spark</span><span class=k>:</span> <span class=kt>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=o>.</span><span class=n>builder</span>
  <span class=o>.</span><span class=n>master</span><span class=o>(</span><span class=s>&quot;local[*]&quot;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>appName</span><span class=o>(</span><span class=s>&quot;My Spark Application&quot;</span><span class=o>)</span>
  <span class=o>.</span><span class=n>config</span><span class=o>(</span><span class=s>&quot;spark.sql.warehouse.dir&quot;</span><span class=o>,</span> <span class=s>&quot;c:/Temp&quot;</span><span class=o>)</span> <span class=c1>// &lt;1&gt;</span>
  <span class=o>.</span><span class=n>getOrCreate</span>
</code></pre></div> <p>You can also set a property using SQL <code>SET</code> command.</p> <div class=highlight><pre><span></span><code><span class=n>assert</span><span class=o>(</span><span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>getOption</span><span class=o>(</span><span class=s>&quot;spark.sql.hive.metastore.version&quot;</span><span class=o>).</span><span class=n>isEmpty</span><span class=o>)</span>

<span class=n>scala</span><span class=o>&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span><span class=s>&quot;SET spark.sql.hive.metastore.version=2.3.2&quot;</span><span class=o>).</span><span class=n>show</span><span class=o>(</span><span class=n>truncate</span> <span class=o>=</span> <span class=kc>false</span><span class=o>)</span>
<span class=o>+--------------------------------+-----+</span>
<span class=o>|</span><span class=n>key</span>                             <span class=o>|</span><span class=n>value</span><span class=o>|</span>
<span class=o>+--------------------------------+-----+</span>
<span class=o>|</span><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=o>.</span><span class=n>hive</span><span class=o>.</span><span class=n>metastore</span><span class=o>.</span><span class=n>version</span><span class=o>|</span><span class=mf>2.3</span><span class=o>.</span><span class=mi>2</span><span class=o>|</span>
<span class=o>+--------------------------------+-----+</span>

<span class=n>assert</span><span class=o>(</span><span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=o>(</span><span class=s>&quot;spark.sql.hive.metastore.version&quot;</span><span class=o>)</span> <span class=o>==</span> <span class=s>&quot;2.3.2&quot;</span><span class=o>)</span>
</code></pre></div> <h2 id=sparksqladaptiveforceapply><span id=spark.sql.adaptive.forceApply> spark.sql.adaptive.forceApply<a class=headerlink href=#sparksqladaptiveforceapply title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code> (together with <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> enabled), Spark will <a href=../physical-optimizations/InsertAdaptiveSparkPlan/#shouldApplyAQE>force apply adaptive query execution for all supported queries</a>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#ADAPTIVE_EXECUTION_FORCE_APPLY>SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY</a> method to access the property (in a type-safe way).</p> <h2 id=sparksqladaptiveloglevel><span id=spark.sql.adaptive.logLevel> spark.sql.adaptive.logLevel<a class=headerlink href=#sparksqladaptiveloglevel title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Log level for adaptive execution logging of plan changes. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>DEBUG</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#adaptiveExecutionLogLevel>SQLConf.adaptiveExecutionLogLevel</a> method to access the current value.</p> <h2 id=sparksqladaptivecoalescepartitionsenabled><span id=spark.sql.adaptive.coalescePartitions.enabled> spark.sql.adaptive.coalescePartitions.enabled<a class=headerlink href=#sparksqladaptivecoalescepartitionsenabled title="Permanent link">&para;</a></h2> <p>Controls coalescing shuffle partitions</p> <p>When <code>true</code> and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark will coalesce contiguous shuffle partitions according to the target size (specified by <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a>), to avoid too many small tasks.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#coalesceShufflePartitionsEnabled>SQLConf.coalesceShufflePartitionsEnabled</a> method to access the current value.</p> <h2 id=sparksqladaptiveadvisorypartitionsizeinbytes><span id=spark.sql.adaptive.advisoryPartitionSizeInBytes> spark.sql.adaptive.advisoryPartitionSizeInBytes<a class=headerlink href=#sparksqladaptiveadvisorypartitionsizeinbytes title="Permanent link">&para;</a></h2> <p>The advisory size in bytes of the shuffle partition during adaptive optimization (when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.</p> <p>Default: <code>64MB</code></p> <p>Since: <code>3.0.0</code></p> <p>Fallback Property: <code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code></p> <p>Use <a href=../SQLConf/#ADVISORY_PARTITION_SIZE_IN_BYTES>SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES</a> to reference the name.</p> <h2 id=sparksqladaptivecoalescepartitionsminpartitionnum><span id=spark.sql.adaptive.coalescePartitions.minPartitionNum> spark.sql.adaptive.coalescePartitions.minPartitionNum<a class=headerlink href=#sparksqladaptivecoalescepartitionsminpartitionnum title="Permanent link">&para;</a></h2> <p>The minimum number of shuffle partitions after coalescing. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptivecoalescepartitionsinitialpartitionnum><span id=spark.sql.adaptive.coalescePartitions.initialPartitionNum> spark.sql.adaptive.coalescePartitions.initialPartitionNum<a class=headerlink href=#sparksqladaptivecoalescepartitionsinitialpartitionnum title="Permanent link">&para;</a></h2> <p>The initial number of shuffle partitions before coalescing.</p> <p>By default it equals to <a href=#spark.sql.shuffle.partitions>spark.sql.shuffle.partitions</a>. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveenabled><span id=spark.sql.adaptive.enabled> spark.sql.adaptive.enabled<a class=headerlink href=#sparksqladaptiveenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../new-and-noteworthy/adaptive-query-execution/ >Adaptive Query Execution</a></p> <p>Default: <code>false</code></p> <p>Since: 1.6.0</p> <p>Use <a href=../SQLConf/#adaptiveExecutionEnabled>SQLConf.adaptiveExecutionEnabled</a> method to access the current value.</p> <h2 id=sparksqladaptivefetchshuffleblocksinbatch><span id=spark.sql.adaptive.fetchShuffleBlocksInBatch> spark.sql.adaptive.fetchShuffleBlocksInBatch<a class=headerlink href=#sparksqladaptivefetchshuffleblocksinbatch title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether to fetch the contiguous shuffle blocks in batch. Instead of fetching blocks one by one, fetching contiguous shuffle blocks for the same map task in batch can reduce IO and improve performance. Note, multiple contiguous blocks exist in single "fetch request only happen when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled. This feature also depends on a relocatable serializer, the concatenation support codec in use and the new version shuffle fetch protocol.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#fetchShuffleBlocksInBatch>SQLConf.fetchShuffleBlocksInBatch</a> method to access the current value.</p> <h2 id=sparksqladaptivelocalshufflereaderenabled><span id=spark.sql.adaptive.localShuffleReader.enabled> spark.sql.adaptive.localShuffleReader.enabled<a class=headerlink href=#sparksqladaptivelocalshufflereaderenabled title="Permanent link">&para;</a></h2> <p>When true and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveskewjoinenabled><span id=spark.sql.adaptive.skewJoin.enabled> spark.sql.adaptive.skewJoin.enabled<a class=headerlink href=#sparksqladaptiveskewjoinenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code> and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#SKEW_JOIN_ENABLED>SQLConf.SKEW_JOIN_ENABLED</a> to reference the property.</p> <h2 id=sparksqladaptiveskewjoinskewedpartitionfactor><span id=spark.sql.adaptive.skewJoin.skewedPartitionFactor> spark.sql.adaptive.skewJoin.skewedPartitionFactor<a class=headerlink href=#sparksqladaptiveskewjoinskewedpartitionfactor title="Permanent link">&para;</a></h2> <p>A partition is considered skewed if its size is larger than this factor multiplying the median partition size and also larger than <a href=#spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</a>.</p> <p>Default: <code>5</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveskewjoinskewedpartitionthresholdinbytes><span id=spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes<a class=headerlink href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes title="Permanent link">&para;</a></h2> <p>A partition is considered skewed if its size in bytes is larger than this threshold and also larger than <a href=#spark.sql.adaptive.skewJoin.skewedPartitionFactor>spark.sql.adaptive.skewJoin.skewedPartitionFactor</a> multiplying the median partition size. Ideally this config should be set larger than <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a>.</p> <p>Default: <code>256MB</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptivenonemptypartitionratioforbroadcastjoin><span id=spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin<a class=headerlink href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A relation with a non-empty partition ratio (the number of non-empty partitions to all partitions) lower than this config will not be considered as the build side of a broadcast-hash join in <a href=../new-and-noteworthy/adaptive-query-execution/ >Adaptive Query Execution</a> regardless of the size.</p> <p>This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is <code>true</code>.</p> <p>Default: <code>0.2</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#nonEmptyPartitionRatioForBroadcastJoin>SQLConf.nonEmptyPartitionRatioForBroadcastJoin</a> method to access the current value.</p> <h2 id=sparksqlanalyzermaxiterations><span id=spark.sql.analyzer.maxIterations> spark.sql.analyzer.maxIterations<a class=headerlink href=#sparksqlanalyzermaxiterations title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The max number of iterations the analyzer runs.</p> <p>Default: <code>100</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlanalyzerfailambiguousselfjoin><span id=spark.sql.analyzer.failAmbiguousSelfJoin> spark.sql.analyzer.failAmbiguousSelfJoin<a class=headerlink href=#sparksqlanalyzerfailambiguousselfjoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, fail the Dataset query if it contains ambiguous self-join.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlansienabled><span id=spark.sql.ansi.enabled> spark.sql.ansi.enabled<a class=headerlink href=#sparksqlansienabled title="Permanent link">&para;</a></h2> <p>When true, Spark tries to conform to the ANSI SQL specification:</p> <ol> <li>Spark will throw a runtime exception if an overflow occurs in any operation on integral/decimal field.</li> <li>Spark will forbid using the reserved keywords of ANSI SQL as identifiers in the SQL parser.</li> </ol> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcodegenwholestage><span id=spark.sql.codegen.wholeStage> spark.sql.codegen.wholeStage<a class=headerlink href=#sparksqlcodegenwholestage title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether the whole stage (of multiple physical operators) will be compiled into a single Java method (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageEnabled>SQLConf.wholeStageEnabled</a> method to access the current value.</p> <h2 id=sparksqlcodegenmethodsplitthreshold><span id=spark.sql.codegen.methodSplitThreshold> spark.sql.codegen.methodSplitThreshold<a class=headerlink href=#sparksqlcodegenmethodsplitthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The threshold of source-code splitting in the codegen. When the number of characters in a single Java function (without comment) exceeds the threshold, the function will be automatically split to multiple smaller ones. We cannot know how many bytecode will be generated, so use the code length as metric. When running on HotSpot, a function's bytecode should not go beyond 8KB, otherwise it will not be JITted; it also should not be too small, otherwise there will be many function calls.</p> <p>Default: <code>1024</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqldebugmaxtostringfields><span id=spark.sql.debug.maxToStringFields> spark.sql.debug.maxToStringFields<a class=headerlink href=#sparksqldebugmaxtostringfields title="Permanent link">&para;</a></h2> <p>Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a "... N more fields" placeholder.</p> <p>Default: <code>25</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#maxToStringFields>SQLConf.maxToStringFields</a> method to access the current value.</p> <h2 id=sparksqldefaultcatalog><span id=spark.sql.defaultCatalog> spark.sql.defaultCatalog<a class=headerlink href=#sparksqldefaultcatalog title="Permanent link">&para;</a></h2> <p>Name of the default catalog</p> <p>Default: <a href=../connector/catalog/CatalogManager/#SESSION_CATALOG_NAME>spark_catalog</a></p> <p>Use <a href=../SQLConf/#DEFAULT_CATALOG>SQLConf.DEFAULT_CATALOG</a> to access the current value.</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionarrowpysparkenabled><span id=spark.sql.execution.arrow.pyspark.enabled> spark.sql.execution.arrow.pyspark.enabled<a class=headerlink href=#sparksqlexecutionarrowpysparkenabled title="Permanent link">&para;</a></h2> <p>When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to:</p> <ol> <li>pyspark.sql.DataFrame.toPandas</li> <li>pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame</li> </ol> <p>The following data types are unsupported: BinaryType, MapType, ArrayType of TimestampType, and nested StructType.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionreusesubquery><span id=spark.sql.execution.reuseSubquery> spark.sql.execution.reuseSubquery<a class=headerlink href=#sparksqlexecutionreusesubquery title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the planner will try to find out duplicated subqueries and re-use them.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionsortbeforerepartition><span id=spark.sql.execution.sortBeforeRepartition> spark.sql.execution.sortBeforeRepartition<a class=headerlink href=#sparksqlexecutionsortbeforerepartition title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When perform a repartition following a shuffle, the output row ordering would be nondeterministic. If some downstream stages fail and some tasks of the repartition stage retry, these tasks may generate different data, and that can lead to correctness issues. Turn on this config to insert a local sort before actually doing repartition to generate consistent repartition results. The performance of <code>repartition()</code> may go down since we insert extra local sort before it.</p> <p>Default: <code>true</code></p> <p>Since: <code>2.1.4</code></p> <p>Use <a href=../SQLConf/#sortBeforeRepartition>SQLConf.sortBeforeRepartition</a> method to access the current value.</p> <h2 id=sparksqlexecutionrangeexchangesamplesizeperpartition><span id=spark.sql.execution.rangeExchange.sampleSizePerPartition> spark.sql.execution.rangeExchange.sampleSizePerPartition<a class=headerlink href=#sparksqlexecutionrangeexchangesamplesizeperpartition title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Number of points to sample per partition in order to determine the range boundaries for range partitioning, typically used in global sorting (without limit).</p> <p>Default: <code>100</code></p> <p>Since: <code>2.3.0</code></p> <p>Use <a href=../SQLConf/#rangeExchangeSampleSizePerPartition>SQLConf.rangeExchangeSampleSizePerPartition</a> method to access the current value.</p> <h2 id=sparksqlexecutionarrowpysparkfallbackenabled><span id=spark.sql.execution.arrow.pyspark.fallback.enabled> spark.sql.execution.arrow.pyspark.fallback.enabled<a class=headerlink href=#sparksqlexecutionarrowpysparkfallbackenabled title="Permanent link">&para;</a></h2> <p>When true, optimizations enabled by <a href=#spark.sql.execution.arrow.pyspark.enabled>spark.sql.execution.arrow.pyspark.enabled</a> will fallback automatically to non-optimized implementations if an error occurs.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionarrowsparkrenabled><span id=spark.sql.execution.arrow.sparkr.enabled> spark.sql.execution.arrow.sparkr.enabled<a class=headerlink href=#sparksqlexecutionarrowsparkrenabled title="Permanent link">&para;</a></h2> <p>When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to:</p> <ol> <li>createDataFrame when its input is an R DataFrame</li> <li>collect</li> <li>dapply</li> <li>gapply</li> </ol> <p>The following data types are unsupported: FloatType, BinaryType, ArrayType, StructType and MapType.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionpandasudfbuffersize><span id=spark.sql.execution.pandas.udf.buffer.size> spark.sql.execution.pandas.udf.buffer.size<a class=headerlink href=#sparksqlexecutionpandasudfbuffersize title="Permanent link">&para;</a></h2> <p>Same as <code>${BUFFER_SIZE.key}</code> but only applies to Pandas UDF executions. If it is not set, the fallback is <code>${BUFFER_SIZE.key}</code>. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.</p> <p>Default: <code>65536</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionpandasconverttoarrowarraysafely><span id=spark.sql.execution.pandas.convertToArrowArraySafely> spark.sql.execution.pandas.convertToArrowArraySafely<a class=headerlink href=#sparksqlexecutionpandasconverttoarrowarraysafely title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, Arrow will perform safe type conversion when converting Pandas. Series to Arrow array during serialization. Arrow will raise errors when detecting unsafe type conversion like overflow. When false, disabling Arrow's type check and do type conversions anyway. This config only works for Arrow 0.11.0+.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlstatisticshistogramenabled><span id=spark.sql.statistics.histogram.enabled> spark.sql.statistics.histogram.enabled<a class=headerlink href=#sparksqlstatisticshistogramenabled title="Permanent link">&para;</a></h2> <p>Enables generating histograms for <a href=../sql/AstBuilder/#visitAnalyze>ANALYZE TABLE</a> SQL statement</p> <p>Default: <code>false</code></p> <div class="admonition note"> <p class=admonition-title>Equi-Height Histogram</p> <p>Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</p> </div> <p>Use <a href=../SQLConf/#histogramEnabled>SQLConf.histogramEnabled</a> method to access the current value.</p> <h2 id=sparksqlsessiontimezone><span id=spark.sql.session.timeZone> spark.sql.session.timeZone<a class=headerlink href=#sparksqlsessiontimezone title="Permanent link">&para;</a></h2> <p>The ID of session-local timezone (e.g. "GMT", "America/Los_Angeles")</p> <p>Default: Java's <code>TimeZone.getDefault.getID</code></p> <p>Use <a href=../SQLConf/#sessionLocalTimeZone>SQLConf.sessionLocalTimeZone</a> method to access the current value.</p> <h2 id=sparksqlsourcescommitprotocolclass><span id=spark.sql.sources.commitProtocolClass> spark.sql.sources.commitProtocolClass<a class=headerlink href=#sparksqlsourcescommitprotocolclass title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Fully-qualified class name of the <code>FileCommitProtocol</code></p> <p>Default: <a href=../SQLHadoopMapReduceCommitProtocol/ >SQLHadoopMapReduceCommitProtocol</a></p> <p>Use <a href=../SQLConf/#fileCommitProtocolClass>SQLConf.fileCommitProtocolClass</a> method to access the current value.</p> <h2 id=sparksqlsourcesignoredatalocality><span id=spark.sql.sources.ignoreDataLocality> spark.sql.sources.ignoreDataLocality<a class=headerlink href=#sparksqlsourcesignoredatalocality title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, Spark will not fetch the block locations for each file on listing files. This speeds up file listing, but the scheduler cannot schedule tasks to take advantage of data locality. It can be particularly useful if data is read from a remote cluster so the scheduler could never take advantage of locality anyway.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesvalidatepartitioncolumns><span id=spark.sql.sources.validatePartitionColumns> spark.sql.sources.validatePartitionColumns<a class=headerlink href=#sparksqlsourcesvalidatepartitioncolumns title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When this option is set to true, partition column values will be validated with user-specified schema. If the validation fails, a runtime exception is thrown. When this option is set to false, the partition column value will be converted to null if it can not be casted to corresponding user-specified schema.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesusev1sourcelist><span id=spark.sql.sources.useV1SourceList> spark.sql.sources.useV1SourceList<a class=headerlink href=#sparksqlsourcesusev1sourcelist title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A comma-separated list of data source short names or fully qualified data source implementation class names for which DataSource V2 code path is disabled. These data sources will fallback to Data Source V1 code path.</p> <p>Default: <code>avro,csv,json,kafka,orc,parquet,text</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlstoreassignmentpolicy><span id=spark.sql.storeAssignmentPolicy> spark.sql.storeAssignmentPolicy<a class=headerlink href=#sparksqlstoreassignmentpolicy title="Permanent link">&para;</a></h2> <p>When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code>. With legacy policy, Spark allows the type coercion as long as it is a valid <code>Cast</code>, which is very loose. e.g. converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code> is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting <code>double</code> to <code>int</code> or <code>decimal</code> to <code>double</code> is not allowed.</p> <p>Possible values: <code>ANSI</code>, <code>LEGACY</code>, <code>STRICT</code></p> <p>Default: <code>ANSI</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerinsetswitchthreshold><span id=spark.sql.optimizer.inSetSwitchThreshold> spark.sql.optimizer.inSetSwitchThreshold<a class=headerlink href=#sparksqloptimizerinsetswitchthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures the max set size in InSet for which Spark will generate code with switch statements. This is applicable only to bytes, shorts, ints, dates.</p> <p>Must be non-negative and less than or equal to 600.</p> <p>Default: <code>400</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangeloglevel><span id=spark.sql.optimizer.planChangeLog.level> spark.sql.optimizer.planChangeLog.level<a class=headerlink href=#sparksqloptimizerplanchangeloglevel title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures the log level for logging the change from the original plan to the new plan after a rule or batch is applied. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>TRACE</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangelogrules><span id=spark.sql.optimizer.planChangeLog.rules> spark.sql.optimizer.planChangeLog.rules<a class=headerlink href=#sparksqloptimizerplanchangelogrules title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures a list of rules to be logged in the optimizer, in which the rules are specified by their rule names and separated by comma.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangelogbatches><span id=spark.sql.optimizer.planChangeLog.batches> spark.sql.optimizer.planChangeLog.batches<a class=headerlink href=#sparksqloptimizerplanchangelogbatches title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures a list of batches to be logged in the optimizer, in which the batches are specified by their batch names and separated by comma.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerdynamicpartitionpruningenabled><span id=spark.sql.optimizer.dynamicPartitionPruning.enabled> spark.sql.optimizer.dynamicPartitionPruning.enabled<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code> (default), Spark SQL will generate predicate for partition column when used as a join key.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningEnabled>SQLConf.dynamicPartitionPruningEnabled</a> to access the current value.</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerdynamicpartitionpruningusestats><span id=spark.sql.optimizer.dynamicPartitionPruning.useStats> spark.sql.optimizer.dynamicPartitionPruning.useStats<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningusestats title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningUseStats>SQLConf.dynamicPartitionPruningUseStats</a> method to access the current value.</p> <h2 id=sparksqloptimizerdynamicpartitionpruningfallbackfilterratio><span id=spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>0.5</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningFallbackFilterRatio>SQLConf.dynamicPartitionPruningFallbackFilterRatio</a> method to access the current value.</p> <h2 id=sparksqloptimizerdynamicpartitionpruningreusebroadcastonly><span id=spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, dynamic partition pruning will only apply when the broadcast exchange of a broadcast hash join operation can be reused as the dynamic pruning filter.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningReuseBroadcastOnly>SQLConf.dynamicPartitionPruningReuseBroadcastOnly</a> method to access the current value.</p> <h2 id=sparksqloptimizernestedpredicatepushdownsupportedfilesources><span id=spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources<a class=headerlink href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A comma-separated list of data source short names or fully qualified data source implementation class names for which Spark tries to push down predicates for nested columns and/or names containing <code>dots</code> to data sources. This configuration is only effective with file-based data source in DSv1. Currently, Parquet implements both optimizations while ORC only supports predicates for names containing <code>dots</code>. The other data sources don't support this feature yet.</p> <p>Default: <code>parquet,orc</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerserializernestedschemapruningenabled><span id=spark.sql.optimizer.serializer.nestedSchemaPruning.enabled> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled<a class=headerlink href=#sparksqloptimizerserializernestedschemapruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from object serialization operator which are unnecessary in satisfying a query. This optimization allows object serializers to avoid executing unnecessary nested expressions.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerexpressionnestedpruningenabled><span id=spark.sql.optimizer.expression.nestedPruning.enabled> spark.sql.optimizer.expression.nestedPruning.enabled<a class=headerlink href=#sparksqloptimizerexpressionnestedpruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from expressions in an operator which are unnecessary in satisfying a query. Note that this optimization doesn't prune nested fields from physical data source scanning. For pruning nested fields from scanning, please use <a href=#spark.sql.optimizer.nestedSchemaPruning.enabled>spark.sql.optimizer.nestedSchemaPruning.enabled</a> config.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlorcmergeschema><span id=spark.sql.orc.mergeSchema> spark.sql.orc.mergeSchema<a class=headerlink href=#sparksqlorcmergeschema title="Permanent link">&para;</a></h2> <p>When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqldatetimejava8apienabled><span id=spark.sql.datetime.java8API.enabled> spark.sql.datetime.java8API.enabled<a class=headerlink href=#sparksqldatetimejava8apienabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. When <code>false</code>, java.sql.Timestamp and java.sql.Date are used for the same purpose.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesbinaryfilemaxlength><span id=spark.sql.sources.binaryFile.maxLength> spark.sql.sources.binaryFile.maxLength<a class=headerlink href=#sparksqlsourcesbinaryfilemaxlength title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The max length of a file that can be read by the binary file data source. Spark will fail fast and not attempt to read the file if its length exceeds this value. The theoretical max is Int.MaxValue, though VMs might implement a smaller max.</p> <p>Default: <code>Int.MaxValue</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmapkeydeduppolicy><span id=spark.sql.mapKeyDedupPolicy> spark.sql.mapKeyDedupPolicy<a class=headerlink href=#sparksqlmapkeydeduppolicy title="Permanent link">&para;</a></h2> <p>The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LAST_WIN</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmavenadditionalremoterepositories><span id=spark.sql.maven.additionalRemoteRepositories> spark.sql.maven.additionalRemoteRepositories<a class=headerlink href=#sparksqlmavenadditionalremoterepositories title="Permanent link">&para;</a></h2> <p>A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.</p> <p>Default: <code>https://maven-central.storage-download.googleapis.com/maven2/</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmaxplanstringlength><span id=spark.sql.maxPlanStringLength> spark.sql.maxPlanStringLength<a class=headerlink href=#sparksqlmaxplanstringlength title="Permanent link">&para;</a></h2> <p>Maximum number of characters to output for a plan string. If the plan is longer, further output will be truncated. The default setting always generates a full plan. Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.</p> <p>Default: <code>Integer.MAX_VALUE - 15</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladdpartitioninbatchsize><span id=spark.sql.addPartitionInBatch.size> spark.sql.addPartitionInBatch.size<a class=headerlink href=#sparksqladdpartitioninbatchsize title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The number of partitions to be handled in one turn when use <code>AlterTableAddPartitionCommand</code> to add partitions into table. The smaller batch size is, the less memory is required for the real handler, e.g. Hive Metastore.</p> <p>Default: <code>100</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlscripttransformationexittimeoutinseconds><span id=spark.sql.scriptTransformation.exitTimeoutInSeconds> spark.sql.scriptTransformation.exitTimeoutInSeconds<a class=headerlink href=#sparksqlscripttransformationexittimeoutinseconds title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Timeout for executor to wait for the termination of transformation script when EOF.</p> <p>Default: <code>10</code> seconds</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlautobroadcastjointhreshold><span id=spark.sql.autoBroadcastJoinThreshold> spark.sql.autoBroadcastJoinThreshold<a class=headerlink href=#sparksqlautobroadcastjointhreshold title="Permanent link">&para;</a></h2> <p>Maximum size (in bytes) for a table that will be broadcast to all worker nodes when performing a join.</p> <p>Default: <code>10L * 1024 * 1024</code> (10M)</p> <p>If the size of the statistics of the logical plan of a table is at most the setting, the DataFrame is broadcast for join.</p> <p>Negative values or <code>0</code> disable broadcasting.</p> <p>Use <a href=../SQLConf/#autoBroadcastJoinThreshold>SQLConf.autoBroadcastJoinThreshold</a> method to access the current value.</p> <h2 id=sparksqlavrocompressioncodec><span id=spark.sql.avro.compression.codec> spark.sql.avro.compression.codec<a class=headerlink href=#sparksqlavrocompressioncodec title="Permanent link">&para;</a></h2> <p>The compression codec to use when writing Avro data to disk</p> <p>Default: <code>snappy</code></p> <p>The supported codecs are:</p> <ul> <li><code>uncompressed</code></li> <li><code>deflate</code></li> <li><code>snappy</code></li> <li><code>bzip2</code></li> <li><code>xz</code></li> </ul> <p>Use <a href=../SQLConf/#avroCompressionCodec>SQLConf.avroCompressionCodec</a> method to access the current value.</p> <h2 id=sparksqlbroadcasttimeout><span id=spark.sql.broadcastTimeout> spark.sql.broadcastTimeout<a class=headerlink href=#sparksqlbroadcasttimeout title="Permanent link">&para;</a></h2> <p>Timeout in seconds for the broadcast wait time in broadcast joins.</p> <p>Default: <code>5 * 60</code></p> <p>When negative, it is assumed infinite (i.e. <code>Duration.Inf</code>)</p> <p>Use <a href=../SQLConf/#broadcastTimeout>SQLConf.broadcastTimeout</a> method to access the current value.</p> <h2 id=sparksqlcasesensitive><span id=spark.sql.caseSensitive> spark.sql.caseSensitive<a class=headerlink href=#sparksqlcasesensitive title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether the query analyzer should be case sensitive (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p> <p>It is highly discouraged to turn on case sensitive mode.</p> <p>Use <a href=../SQLConf/#caseSensitiveAnalysis>SQLConf.caseSensitiveAnalysis</a> method to access the current value.</p> <h2 id=sparksqlcatalogspark_catalog><span id=spark.sql.catalog.spark_catalog> spark.sql.catalog.spark_catalog<a class=headerlink href=#sparksqlcatalogspark_catalog title="Permanent link">&para;</a></h2> <p>A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: <code>spark_catalog</code>. This catalog shares its identifier namespace with the <code>spark_catalog</code> and must be consistent with it; for example, if a table can be loaded by the <code>spark_catalog</code>, this catalog must also return the table metadata. To delegate operations to the <code>spark_catalog</code>, implementations can extend 'CatalogExtension'.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcboenabled><span id=spark.sql.cbo.enabled> spark.sql.cbo.enabled<a class=headerlink href=#sparksqlcboenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-cost-based-optimization/ >Cost-Based Optimization</a> (CBO) for estimation of plan statistics when <code>true</code>.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#cboEnabled>SQLConf.cboEnabled</a> method to access the current value.</p> <h2 id=sparksqlcbojoinreorderenabled><span id=spark.sql.cbo.joinReorder.enabled> spark.sql.cbo.joinReorder.enabled<a class=headerlink href=#sparksqlcbojoinreorderenabled title="Permanent link">&para;</a></h2> <p>Enables join reorder for cost-based optimization (CBO).</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#joinReorderEnabled>SQLConf.joinReorderEnabled</a> method to access the current value.</p> <h2 id=sparksqlcboplanstatsenabled><span id=spark.sql.cbo.planStats.enabled> spark.sql.cbo.planStats.enabled<a class=headerlink href=#sparksqlcboplanstatsenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, the logical plan will fetch row counts and column statistics from catalog.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcbostarschemadetection><span id=spark.sql.cbo.starSchemaDetection> spark.sql.cbo.starSchemaDetection<a class=headerlink href=#sparksqlcbostarschemadetection title="Permanent link">&para;</a></h2> <p>Enables <em>join reordering</em> based on star schema detection for cost-based optimization (CBO) in <a href=../logical-optimizations/ReorderJoin/ >ReorderJoin</a> logical plan optimization.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#starSchemaDetection>SQLConf.starSchemaDetection</a> method to access the current value.</p> <h2 id=sparksqlcodegenaggregatemapvectorizedenable><span id=spark.sql.codegen.aggregate.map.vectorized.enable> spark.sql.codegen.aggregate.map.vectorized.enable<a class=headerlink href=#sparksqlcodegenaggregatemapvectorizedenable title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables vectorized aggregate hash map. This is for testing/benchmarking only.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcodegenaggregatesplitaggregatefuncenabled><span id=spark.sql.codegen.aggregate.splitAggregateFunc.enabled> spark.sql.codegen.aggregate.splitAggregateFunc.enabled<a class=headerlink href=#sparksqlcodegenaggregatesplitaggregatefuncenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the code generator would split aggregate code into individual methods instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcodegencomments><span id=spark.sql.codegen.comments> spark.sql.codegen.comments<a class=headerlink href=#sparksqlcodegencomments title="Permanent link">&para;</a></h2> <p>Controls whether <code>CodegenContext</code> should <a href=../physical-operators/CodegenSupport/#registerComment>register comments</a> (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p> <h2 id=sparksqlcodegenfactorymode><span id=spark.sql.codegen.factoryMode> spark.sql.codegen.factoryMode<a class=headerlink href=#sparksqlcodegenfactorymode title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Determines the codegen generator fallback behavior</p> <p>Default: <code>FALLBACK</code></p> <p>Acceptable values:</p> <ul> <li><span id=spark.sql.codegen.factoryMode-CODEGEN_ONLY> <code>CODEGEN_ONLY</code> - disable fallback mode</li> <li><span id=spark.sql.codegen.factoryMode-FALLBACK> <code>FALLBACK</code> - try codegen first and, if any compile error happens, fallback to interpreted mode</li> <li><span id=spark.sql.codegen.factoryMode-NO_CODEGEN> <code>NO_CODEGEN</code> - skips codegen and always uses interpreted path</li> </ul> <p>Used when <code>CodeGeneratorWithInterpretedFallback</code> is requested to <a href=../physical-operators/CodeGeneratorWithInterpretedFallback/#createObject>createObject</a> (when <code>UnsafeProjection</code> is requested to <a href=../physical-operators/UnsafeProjection/#create>create an UnsafeProjection for Catalyst expressions</a>)</p> <h2 id=sparksqlcodegenfallback><span id=spark.sql.codegen.fallback> spark.sql.codegen.fallback<a class=headerlink href=#sparksqlcodegenfallback title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether the whole stage codegen could be temporary disabled for the part of a query that has failed to compile generated code (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageFallback>SQLConf.wholeStageFallback</a> method to access the current value.</p> <h2 id=sparksqlcodegenhugemethodlimit><span id=spark.sql.codegen.hugeMethodLimit> spark.sql.codegen.hugeMethodLimit<a class=headerlink href=#sparksqlcodegenhugemethodlimit title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The maximum bytecode size of a single compiled Java function generated by whole-stage codegen.</p> <p>Default: <code>65535</code></p> <p>The default value <code>65535</code> is the largest bytecode size possible for a valid Java method. When running on HotSpot, it may be preferable to set the value to <code>8000</code> (which is the value of <code>HugeMethodLimit</code> in the OpenJDK JVM settings)</p> <p>Use <a href=../SQLConf/#hugeMethodLimit>SQLConf.hugeMethodLimit</a> method to access the current value.</p> <h2 id=sparksqlcodegenuseidinclassname><span id=spark.sql.codegen.useIdInClassName> spark.sql.codegen.useIdInClassName<a class=headerlink href=#sparksqlcodegenuseidinclassname title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether to embed the (whole-stage) codegen stage ID into the class name of the generated class as a suffix (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageUseIdInClassName>SQLConf.wholeStageUseIdInClassName</a> method to access the current value.</p> <h2 id=sparksqlcodegenmaxfields><span id=spark.sql.codegen.maxFields> spark.sql.codegen.maxFields<a class=headerlink href=#sparksqlcodegenmaxfields title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Maximum number of output fields (including nested fields) that whole-stage codegen supports. Going above the number deactivates whole-stage codegen.</p> <p>Default: <code>100</code></p> <p>Use <a href=../SQLConf/#wholeStageMaxNumFields>SQLConf.wholeStageMaxNumFields</a> method to access the current value.</p> <h2 id=sparksqlcodegensplitconsumefuncbyoperator><span id=spark.sql.codegen.splitConsumeFuncByOperator> spark.sql.codegen.splitConsumeFuncByOperator<a class=headerlink href=#sparksqlcodegensplitconsumefuncbyoperator title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether whole stage codegen puts the logic of consuming rows of each physical operator into individual methods, instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageSplitConsumeFuncByOperator>SQLConf.wholeStageSplitConsumeFuncByOperator</a> method to access the current value.</p> <h2 id=sparksqlcolumnvectoroffheapenabled><span id=spark.sql.columnVector.offheap.enabled> spark.sql.columnVector.offheap.enabled<a class=headerlink href=#sparksqlcolumnvectoroffheapenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables <a href=../OffHeapColumnVector/ >OffHeapColumnVector</a> in <a href=../ColumnarBatch/ >ColumnarBatch</a> (<code>true</code>) or not (<code>false</code>). When <code>false</code>, <a href=../OnHeapColumnVector/ >OnHeapColumnVector</a> is used instead.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#offHeapColumnVectorEnabled>SQLConf.offHeapColumnVectorEnabled</a> method to access the current value.</p> <h2 id=sparksqlcolumnnameofcorruptrecord><span id=spark.sql.columnNameOfCorruptRecord> spark.sql.columnNameOfCorruptRecord<a class=headerlink href=#sparksqlcolumnnameofcorruptrecord title="Permanent link">&para;</a></h2> <h2 id=sparksqlconstraintpropagationenabled><span id=spark.sql.constraintPropagation.enabled> spark.sql.constraintPropagation.enabled<a class=headerlink href=#sparksqlconstraintpropagationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the query optimizer will infer and propagate data constraints in the query plan to optimize them. Constraint propagation can sometimes be computationally expensive for certain kinds of query plans (such as those with a large number of predicates and aliases) which might negatively impact overall runtime.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#constraintPropagationEnabled>SQLConf.constraintPropagationEnabled</a> method to access the current value.</p> <h2 id=sparksqlcsvfilterpushdownenabled><span id=spark.sql.csv.filterPushdown.enabled> spark.sql.csv.filterPushdown.enabled<a class=headerlink href=#sparksqlcsvfilterpushdownenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, enable filter pushdown to CSV datasource.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqldefaultsizeinbytes><span id=spark.sql.defaultSizeInBytes> spark.sql.defaultSizeInBytes<a class=headerlink href=#sparksqldefaultsizeinbytes title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Estimated size of a table or relation used in query planning</p> <p>Default: Java's <code>Long.MaxValue</code></p> <p>Set to Java's <code>Long.MaxValue</code> which is larger than <a href=#spark.sql.autoBroadcastJoinThreshold>spark.sql.autoBroadcastJoinThreshold</a> to be more conservative. That is to say by default the optimizer will not choose to broadcast a table unless it knows for sure that the table size is small enough.</p> <p>Used by the planner to decide when it is safe to broadcast a relation. By default, the system will assume that tables are too large to broadcast.</p> <p>Use <a href=../SQLConf/#defaultSizeInBytes>SQLConf.defaultSizeInBytes</a> method to access the current value.</p> <h2 id=sparksqldialect><span id=spark.sql.dialect> spark.sql.dialect<a class=headerlink href=#sparksqldialect title="Permanent link">&para;</a></h2> <h2 id=sparksqlexchangereuse><span id=spark.sql.exchange.reuse> spark.sql.exchange.reuse<a class=headerlink href=#sparksqlexchangereuse title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When enabled (<code>true</code>), the <a href=../SparkPlanner/ >Spark planner</a> will find duplicated exchanges and subqueries and re-use them.</p> <p>When disabled (<code>false</code>), <a href=../physical-optimizations/ReuseExchange/ >ReuseExchange</a> and <a href=../physical-optimizations/ReuseSubquery/ >ReuseSubquery</a> physical optimizations (that the Spark planner uses for physical query plan optimization) do nothing.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#exchangeReuseEnabled>SQLConf.exchangeReuseEnabled</a> method to access the current value.</p> <h2 id=sparksqlexecutionuseobjecthashaggregateexec><span id=spark.sql.execution.useObjectHashAggregateExec> spark.sql.execution.useObjectHashAggregateExec<a class=headerlink href=#sparksqlexecutionuseobjecthashaggregateexec title="Permanent link">&para;</a></h2> <p>Enables <a href=../physical-operators/ObjectHashAggregateExec/ >ObjectHashAggregateExec</a> when <a href=../execution-planning-strategies/Aggregation/ >Aggregation</a> execution planning strategy is executed.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#useObjectHashAggregation>SQLConf.useObjectHashAggregation</a> method to access the current value.</p> <h2 id=sparksqlfilesignorecorruptfiles><span id=spark.sql.files.ignoreCorruptFiles> spark.sql.files.ignoreCorruptFiles<a class=headerlink href=#sparksqlfilesignorecorruptfiles title="Permanent link">&para;</a></h2> <p>Controls whether to ignore corrupt files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#ignoreCorruptFiles>SQLConf.ignoreCorruptFiles</a> method to access the current value.</p> <h2 id=sparksqlfilesignoremissingfiles><span id=spark.sql.files.ignoreMissingFiles> spark.sql.files.ignoreMissingFiles<a class=headerlink href=#sparksqlfilesignoremissingfiles title="Permanent link">&para;</a></h2> <p>Controls whether to ignore missing files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#ignoreMissingFiles>SQLConf.ignoreMissingFiles</a> method to access the current value.</p> <h2 id=sparksqlfilesmaxrecordsperfile><span id=spark.sql.files.maxRecordsPerFile> spark.sql.files.maxRecordsPerFile<a class=headerlink href=#sparksqlfilesmaxrecordsperfile title="Permanent link">&para;</a></h2> <p>Maximum number of records to write out to a single file. If this value is <code>0</code> or negative, there is no limit.</p> <p>Default: <code>0</code></p> <p>Use <a href=../SQLConf/#maxRecordsPerFile>SQLConf.maxRecordsPerFile</a> method to access the current value.</p> <h2 id=sparksqlfilesmaxpartitionbytes><span id=spark.sql.files.maxPartitionBytes> spark.sql.files.maxPartitionBytes<a class=headerlink href=#sparksqlfilesmaxpartitionbytes title="Permanent link">&para;</a></h2> <p>The maximum number of bytes to pack into a single partition when reading files.</p> <p>Default: <code>128 * 1024 * 1024</code> (which corresponds to <code>parquet.block.size</code>)</p> <p>Use <a href=../SQLConf/#filesMaxPartitionBytes>SQLConf.filesMaxPartitionBytes</a> method to access the current value.</p> <h2 id=sparksqlfilesopencostinbytes><span id=spark.sql.files.openCostInBytes> spark.sql.files.openCostInBytes<a class=headerlink href=#sparksqlfilesopencostinbytes title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The estimated cost to open a file, measured by the number of bytes could be scanned at the same time (to include multiple files into a partition).</p> <p>Default: <code>4 * 1024 * 1024</code></p> <p>It's better to over estimate it, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</p> <p>Use <a href=../SQLConf/#filesOpenCostInBytes>SQLConf.filesOpenCostInBytes</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragecompressed><span id=spark.sql.inMemoryColumnarStorage.compressed> spark.sql.inMemoryColumnarStorage.compressed<a class=headerlink href=#sparksqlinmemorycolumnarstoragecompressed title="Permanent link">&para;</a></h2> <p>When enabled, Spark SQL will automatically select a compression codec for each column based on statistics of the data.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#useCompression>SQLConf.useCompression</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragebatchsize><span id=spark.sql.inMemoryColumnarStorage.batchSize> spark.sql.inMemoryColumnarStorage.batchSize<a class=headerlink href=#sparksqlinmemorycolumnarstoragebatchsize title="Permanent link">&para;</a></h2> <p>Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</p> <p>Default: <code>10000</code></p> <p>Use <a href=../SQLConf/#columnBatchSize>SQLConf.columnBatchSize</a> method to access the current value.</p> <h2 id=sparksqlinmemorytablescanstatisticsenable><span id=spark.sql.inMemoryTableScanStatistics.enable> spark.sql.inMemoryTableScanStatistics.enable<a class=headerlink href=#sparksqlinmemorytablescanstatisticsenable title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, enable in-memory table scan accumulators.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlinmemorycolumnarstorageenablevectorizedreader><span id=spark.sql.inMemoryColumnarStorage.enableVectorizedReader> spark.sql.inMemoryColumnarStorage.enableVectorizedReader<a class=headerlink href=#sparksqlinmemorycolumnarstorageenablevectorizedreader title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-vectorized-query-execution/ >vectorized reader</a> for columnar caching.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#cacheVectorizedReaderEnabled>SQLConf.cacheVectorizedReaderEnabled</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragepartitionpruning><span id=spark.sql.inMemoryColumnarStorage.partitionPruning> spark.sql.inMemoryColumnarStorage.partitionPruning<a class=headerlink href=#sparksqlinmemorycolumnarstoragepartitionpruning title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables partition pruning for in-memory columnar tables</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#inMemoryPartitionPruning>SQLConf.inMemoryPartitionPruning</a> method to access the current value.</p> <h2 id=sparksqljoinprefersortmergejoin><span id=spark.sql.join.preferSortMergeJoin> spark.sql.join.preferSortMergeJoin<a class=headerlink href=#sparksqljoinprefersortmergejoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether <a href=../execution-planning-strategies/JoinSelection/ >JoinSelection</a> execution planning strategy prefers <a href=../physical-operators/SortMergeJoinExec/ >sort merge join</a> over <a href=../physical-operators/ShuffledHashJoinExec/ >shuffled hash join</a>.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#preferSortMergeJoin>SQLConf.preferSortMergeJoin</a> method to access the current value.</p> <h2 id=sparksqljsongeneratorignorenullfields><span id=spark.sql.jsonGenerator.ignoreNullFields> spark.sql.jsonGenerator.ignoreNullFields<a class=headerlink href=#sparksqljsongeneratorignorenullfields title="Permanent link">&para;</a></h2> <p>Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacydolooseupcast><span id=spark.sql.legacy.doLooseUpcast> spark.sql.legacy.doLooseUpcast<a class=headerlink href=#sparksqllegacydolooseupcast title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the upcast will be loose and allows string to atomic types.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacycteprecedencepolicy><span id=spark.sql.legacy.ctePrecedencePolicy> spark.sql.legacy.ctePrecedencePolicy<a class=headerlink href=#sparksqllegacycteprecedencepolicy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, outer CTE definitions takes precedence over inner definitions. If set to CORRECTED, inner CTE definitions take precedence. The default value is EXCEPTION, AnalysisException is thrown while name conflict is detected in nested CTE. This config will be removed in future versions and CORRECTED will be the only behavior.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacytimeparserpolicy><span id=spark.sql.legacy.timeParserPolicy> spark.sql.legacy.timeParserPolicy<a class=headerlink href=#sparksqllegacytimeparserpolicy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, java.text.SimpleDateFormat is used for formatting and parsing dates/timestamps in a locale-sensitive manner, which is the approach before Spark 3.0. When set to CORRECTED, classes from <code>java.time.*</code> packages are used for the same purpose. The default value is EXCEPTION, RuntimeException is thrown when we will get different results.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyfollowthreevaluedlogicinarrayexists><span id=spark.sql.legacy.followThreeValuedLogicInArrayExists> spark.sql.legacy.followThreeValuedLogicInArrayExists<a class=headerlink href=#sparksqllegacyfollowthreevaluedlogicinarrayexists title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the ArrayExists will follow the three-valued boolean logic.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyfromdaytimestringenabled><span id=spark.sql.legacy.fromDayTimeString.enabled> spark.sql.legacy.fromDayTimeString.enabled<a class=headerlink href=#sparksqllegacyfromdaytimestringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the <code>from</code> bound is not taken into account in conversion of a day-time string to an interval, and the <code>to</code> bound is used to skip all interval units out of the specified range. When <code>false</code>, <code>ParseException</code> is thrown if the input does not match to the pattern defined by <code>from</code> and <code>to</code>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacynotreserveproperties><span id=spark.sql.legacy.notReserveProperties> spark.sql.legacy.notReserveProperties<a class=headerlink href=#sparksqllegacynotreserveproperties title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, all database and table properties are not reserved and available for create/alter syntaxes. But please be aware that the reserved properties will be silently removed.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyaddsinglefileinaddfile><span id=spark.sql.legacy.addSingleFileInAddFile> spark.sql.legacy.addSingleFileInAddFile<a class=headerlink href=#sparksqllegacyaddsinglefileinaddfile title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, only a single file can be added using ADD FILE. If false, then users can add directory by passing directory path to ADD FILE.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyexponentliteralasdecimalenabled><span id=spark.sql.legacy.exponentLiteralAsDecimal.enabled> spark.sql.legacy.exponentLiteralAsDecimal.enabled<a class=headerlink href=#sparksqllegacyexponentliteralasdecimalenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, a literal with an exponent (e.g. 1E-30) would be parsed as Decimal rather than Double.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallownegativescaleofdecimal><span id=spark.sql.legacy.allowNegativeScaleOfDecimal> spark.sql.legacy.allowNegativeScaleOfDecimal<a class=headerlink href=#sparksqllegacyallownegativescaleofdecimal title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, negative scale of Decimal type is allowed. For example, the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is Decimal(11, 0) in non legacy mode.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacybucketedtablescanoutputordering><span id=spark.sql.legacy.bucketedTableScan.outputOrdering> spark.sql.legacy.bucketedTableScan.outputOrdering<a class=headerlink href=#sparksqllegacybucketedtablescanoutputordering title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the bucketed table scan will list files during planning to figure out the output ordering, which is expensive and may make the planning quite slow.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyjsonallowemptystringenabled><span id=spark.sql.legacy.json.allowEmptyString.enabled> spark.sql.legacy.json.allowEmptyString.enabled<a class=headerlink href=#sparksqllegacyjsonallowemptystringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the parser of JSON data source treats empty strings as null for some data types such as <code>IntegerType</code>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacycreateemptycollectionusingstringtype><span id=spark.sql.legacy.createEmptyCollectionUsingStringType> spark.sql.legacy.createEmptyCollectionUsingStringType<a class=headerlink href=#sparksqllegacycreateemptycollectionusingstringtype title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, Spark returns an empty collection with <code>StringType</code> as element type if the <code>array</code>/<code>map</code> function is called without any parameters. Otherwise, Spark returns an empty collection with <code>NullType</code> as element type.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallowuntypedscalaudf><span id=spark.sql.legacy.allowUntypedScalaUDF> spark.sql.legacy.allowUntypedScalaUDF<a class=headerlink href=#sparksqllegacyallowuntypedscalaudf title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, user is allowed to use <code>org.apache.spark.sql.functions.udf(f: AnyRef, dataType: DataType)</code>. Otherwise, an exception will be thrown at runtime.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacydatasetnamenonstructgroupingkeyasvalue><span id=spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue<a class=headerlink href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the key attribute resulted from running <code>Dataset.groupByKey</code> for non-struct key type, will be named as <code>value</code>, following the behavior of Spark version 2.4 and earlier.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacysetcommandrejectssparkcoreconfs><span id=spark.sql.legacy.setCommandRejectsSparkCoreConfs> spark.sql.legacy.setCommandRejectsSparkCoreConfs<a class=headerlink href=#sparksqllegacysetcommandrejectssparkcoreconfs title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> If it is set to true, SET command will fail when the key is registered as a SparkConf entry.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacytypecoerciondatetimetostringenabled><span id=spark.sql.legacy.typeCoercion.datetimeToString.enabled> spark.sql.legacy.typeCoercion.datetimeToString.enabled<a class=headerlink href=#sparksqllegacytypecoerciondatetimetostringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, date/timestamp will cast to string in binary comparisons with String</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallowhashonmaptype><span id=spark.sql.legacy.allowHashOnMapType> spark.sql.legacy.allowHashOnMapType<a class=headerlink href=#sparksqllegacyallowhashonmaptype title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyparquetdatetimerebasemodeinwrite><span id=spark.sql.legacy.parquet.datetimeRebaseModeInWrite> spark.sql.legacy.parquet.datetimeRebaseModeInWrite<a class=headerlink href=#sparksqllegacyparquetdatetimerebasemodeinwrite title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyparquetdatetimerebasemodeinread><span id=spark.sql.legacy.parquet.datetimeRebaseModeInRead> spark.sql.legacy.parquet.datetimeRebaseModeInRead<a class=headerlink href=#sparksqllegacyparquetdatetimerebasemodeinread title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyavrodatetimerebasemodeinwrite><span id=spark.sql.legacy.avro.datetimeRebaseModeInWrite> spark.sql.legacy.avro.datetimeRebaseModeInWrite<a class=headerlink href=#sparksqllegacyavrodatetimerebasemodeinwrite title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyavrodatetimerebasemodeinread><span id=spark.sql.legacy.avro.datetimeRebaseModeInRead> spark.sql.legacy.avro.datetimeRebaseModeInRead<a class=headerlink href=#sparksqllegacyavrodatetimerebasemodeinread title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyrddapplyconf><span id=spark.sql.legacy.rdd.applyConf> spark.sql.legacy.rdd.applyConf<a class=headerlink href=#sparksqllegacyrddapplyconf title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables propagation of <a href=../SQLConf/#getAllConfs>SQL configurations</a> when executing operations on the <a href=../QueryExecution/#toRdd>RDD that represents a structured query</a>. This is the (buggy) behavior up to 2.4.4.</p> <p>Default: <code>true</code></p> <p>This is for cases not tracked by <a href=../spark-sql-SQLExecution/ >SQL execution</a>, when a <code>Dataset</code> is converted to an RDD either using Dataset.md#rdd[rdd] operation or <a href=../QueryExecution/#toRdd>QueryExecution</a>, and then the returned RDD is used to invoke actions on it.</p> <p>This config is deprecated and will be removed in 3.0.0.</p> <h2 id=sparksqllegacyreplacedatabrickssparkavroenabled><span id=spark.sql.legacy.replaceDatabricksSparkAvro.enabled> spark.sql.legacy.replaceDatabricksSparkAvro.enabled<a class=headerlink href=#sparksqllegacyreplacedatabrickssparkavroenabled title="Permanent link">&para;</a></h2> <p>Enables resolving (<em>mapping</em>) the data source provider <code>com.databricks.spark.avro</code> to the built-in (but external) Avro data source module for backward compatibility.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#replaceDatabricksSparkAvroEnabled>SQLConf.replaceDatabricksSparkAvroEnabled</a> method to access the current value.</p> <h2 id=sparksqllimitscaleupfactor><span id=spark.sql.limit.scaleUpFactor> spark.sql.limit.scaleUpFactor<a class=headerlink href=#sparksqllimitscaleupfactor title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Minimal increase rate in the number of partitions between attempts when executing <code>take</code> operator on a structured query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run.</p> <p>Default: <code>4</code></p> <p>Use <a href=../SQLConf/#limitScaleUpFactor>SQLConf.limitScaleUpFactor</a> method to access the current value.</p> <h2 id=sparksqloptimizerexcludedrules><span id=spark.sql.optimizer.excludedRules> spark.sql.optimizer.excludedRules<a class=headerlink href=#sparksqloptimizerexcludedrules title="Permanent link">&para;</a></h2> <p>Comma-separated list of fully-qualified class names of the optimization rules that should be disabled (excluded) from <a href=../catalyst/Optimizer/#spark.sql.optimizer.excludedRules>logical query optimization</a>.</p> <p>Default: <code>(empty)</code></p> <p>Use <a href=../SQLConf/#optimizerExcludedRules>SQLConf.optimizerExcludedRules</a> method to access the current value.</p> <div class="admonition important"> <p class=admonition-title>Important</p> <p>It is not guaranteed that all the rules to be excluded will eventually be excluded, as some rules are <a href=../catalyst/Optimizer/#nonExcludableRules>non-excludable</a>.</p> </div> <h2 id=sparksqloptimizerinsetconversionthreshold><span id=spark.sql.optimizer.inSetConversionThreshold> spark.sql.optimizer.inSetConversionThreshold<a class=headerlink href=#sparksqloptimizerinsetconversionthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The threshold of set size for <code>InSet</code> conversion.</p> <p>Default: <code>10</code></p> <p>Use <a href=../SQLConf/#optimizerInSetConversionThreshold>SQLConf.optimizerInSetConversionThreshold</a> method to access the current value.</p> <h2 id=sparksqloptimizermaxiterations><span id=spark.sql.optimizer.maxIterations> spark.sql.optimizer.maxIterations<a class=headerlink href=#sparksqloptimizermaxiterations title="Permanent link">&para;</a></h2> <p>Maximum number of iterations for <a href=../Analyzer/#fixedPoint>Analyzer</a> and <a href=../catalyst/Optimizer/#fixedPoint>Logical Optimizer</a>.</p> <p>Default: <code>100</code></p> <h2 id=sparksqloptimizerreplaceexceptwithfilter><span id=spark.sql.optimizer.replaceExceptWithFilter> spark.sql.optimizer.replaceExceptWithFilter<a class=headerlink href=#sparksqloptimizerreplaceexceptwithfilter title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the apply function of the rule verifies whether the right node of the except operation is of type Filter or Project followed by Filter. If yes, the rule further verifies 1) Excluding the filter operations from the right (as well as the left node, if any) on the top, whether both the nodes evaluates to a same result. 2) The left and right nodes don't contain any SubqueryExpressions. 3) The output column names of the left node are distinct. If all the conditions are met, the rule will replace the except operation with a Filter by flipping the filter condition(s) of the right node.</p> <p>Default: <code>true</code></p> <h2 id=sparksqloptimizernestedschemapruningenabled><span id=spark.sql.optimizer.nestedSchemaPruning.enabled> spark.sql.optimizer.nestedSchemaPruning.enabled<a class=headerlink href=#sparksqloptimizernestedschemapruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from the output of a logical relation that are not necessary in satisfying a query. This optimization allows columnar file format readers to avoid reading unnecessary nested column data.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#nestedSchemaPruningEnabled>SQLConf.nestedSchemaPruningEnabled</a> method to access the current value.</p> <h2 id=sparksqlorcimpl><span id=spark.sql.orc.impl> spark.sql.orc.impl<a class=headerlink href=#sparksqlorcimpl title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>native</code>, use the native version of ORC support instead of the ORC library in Hive 1.2.1.</p> <p>Default: <code>native</code></p> <p>Acceptable values:</p> <ul> <li><code>hive</code></li> <li><code>native</code></li> </ul> <h2 id=sparksqlpysparkjvmstacktraceenabled><span id=spark.sql.pyspark.jvmStacktrace.enabled> spark.sql.pyspark.jvmStacktrace.enabled<a class=headerlink href=#sparksqlpysparkjvmstacktraceenabled title="Permanent link">&para;</a></h2> <p>When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled and hides JVM stacktrace and shows a Python-friendly exception only.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlparquetbinaryasstring><span id=spark.sql.parquet.binaryAsString> spark.sql.parquet.binaryAsString<a class=headerlink href=#sparksqlparquetbinaryasstring title="Permanent link">&para;</a></h2> <p>Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#isParquetBinaryAsString>SQLConf.isParquetBinaryAsString</a> method to access the current value.</p> <h2 id=sparksqlparquetcolumnarreaderbatchsize><span id=spark.sql.parquet.columnarReaderBatchSize> spark.sql.parquet.columnarReaderBatchSize<a class=headerlink href=#sparksqlparquetcolumnarreaderbatchsize title="Permanent link">&para;</a></h2> <p>The number of rows to include in a parquet vectorized reader batch (the capacity of <a href=../datasources/parquet/VectorizedParquetRecordReader/ >VectorizedParquetRecordReader</a>).</p> <p>Default: <code>4096</code> (4k)</p> <p>The number should be carefully chosen to minimize overhead and avoid OOMs while reading data.</p> <p>Use <a href=../SQLConf/#parquetVectorizedReaderBatchSize>SQLConf.parquetVectorizedReaderBatchSize</a> method to access the current value.</p> <h2 id=sparksqlparquetint96astimestamp><span id=spark.sql.parquet.int96AsTimestamp> spark.sql.parquet.int96AsTimestamp<a class=headerlink href=#sparksqlparquetint96astimestamp title="Permanent link">&para;</a></h2> <p>Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#isParquetINT96AsTimestamp>SQLConf.isParquetINT96AsTimestamp</a> method to access the current value.</p> <h2 id=sparksqlparquetenablevectorizedreader><span id=spark.sql.parquet.enableVectorizedReader> spark.sql.parquet.enableVectorizedReader<a class=headerlink href=#sparksqlparquetenablevectorizedreader title="Permanent link">&para;</a></h2> <p>Enables <a href=../vectorized-parquet-reader/ >vectorized parquet decoding</a>.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetVectorizedReaderEnabled>SQLConf.parquetVectorizedReaderEnabled</a> method to access the current value.</p> <h2 id=sparksqlparquetfilterpushdown><span id=spark.sql.parquet.filterPushdown> spark.sql.parquet.filterPushdown<a class=headerlink href=#sparksqlparquetfilterpushdown title="Permanent link">&para;</a></h2> <p>Controls the <a href=../logical-optimizations/PushDownPredicate/ >filter predicate push-down optimization</a> for data sources using <a href=../datasources/parquet/ParquetFileFormat/ >parquet</a> file format</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetFilterPushDown>SQLConf.parquetFilterPushDown</a> method to access the current value.</p> <h2 id=sparksqlparquetfilterpushdowndate><span id=spark.sql.parquet.filterPushdown.date> spark.sql.parquet.filterPushdown.date<a class=headerlink href=#sparksqlparquetfilterpushdowndate title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables parquet filter push-down optimization for Date (when <a href=#spark.sql.parquet.filterPushdown>spark.sql.parquet.filterPushdown</a> is enabled)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetFilterPushDownDate>SQLConf.parquetFilterPushDownDate</a> method to access the current value.</p> <h2 id=sparksqlparquetint96timestampconversion><span id=spark.sql.parquet.int96TimestampConversion> spark.sql.parquet.int96TimestampConversion<a class=headerlink href=#sparksqlparquetint96timestampconversion title="Permanent link">&para;</a></h2> <p>Controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.</p> <p>Default: <code>false</code></p> <p>This is necessary because Impala stores INT96 data with a different timezone offset than Hive and Spark.</p> <p>Use <a href=../SQLConf/#isParquetINT96TimestampConversion>SQLConf.isParquetINT96TimestampConversion</a> method to access the current value.</p> <h2 id=sparksqlparquetrecordlevelfilterenabled><span id=spark.sql.parquet.recordLevelFilter.enabled> spark.sql.parquet.recordLevelFilter.enabled<a class=headerlink href=#sparksqlparquetrecordlevelfilterenabled title="Permanent link">&para;</a></h2> <p>Enables Parquet's native record-level filtering using the pushed down filters (when <a href=#spark.sql.parquet.filterPushdown>spark.sql.parquet.filterPushdown</a> is enabled).</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#parquetRecordFilterEnabled>SQLConf.parquetRecordFilterEnabled</a> method to access the current value.</p> <h2 id=sparksqlparserquotedregexcolumnnames><span id=spark.sql.parser.quotedRegexColumnNames> spark.sql.parser.quotedRegexColumnNames<a class=headerlink href=#sparksqlparserquotedregexcolumnnames title="Permanent link">&para;</a></h2> <p>Controls whether quoted identifiers (using backticks) in SELECT statements should be interpreted as regular expressions.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#supportQuotedRegexColumnName>SQLConf.supportQuotedRegexColumnName</a> method to access the current value.</p> <h2 id=sparksqlpivotmaxvalues><span id=spark.sql.pivotMaxValues> spark.sql.pivotMaxValues<a class=headerlink href=#sparksqlpivotmaxvalues title="Permanent link">&para;</a></h2> <p>Maximum number of (distinct) values that will be collected without error (when doing a <a href=../spark-sql-RelationalGroupedDataset/#pivot>pivot</a> without specifying the values for the pivot column)</p> <p>Default: <code>10000</code></p> <p>Use <a href=../SQLConf/#dataFramePivotMaxValues>SQLConf.dataFramePivotMaxValues</a> method to access the current value.</p> <h2 id=sparksqlredactionoptionsregex><span id=spark.sql.redaction.options.regex> spark.sql.redaction.options.regex<a class=headerlink href=#sparksqlredactionoptionsregex title="Permanent link">&para;</a></h2> <p>Regular expression to find options of a Spark SQL command with sensitive information</p> <p>Default: <code>(?i)secret!password</code></p> <p>The values of the options matched will be redacted in the explain output.</p> <p>This redaction is applied on top of the global redaction configuration defined by <code>spark.redaction.regex</code> configuration.</p> <p>Used exclusively when <code>SQLConf</code> is requested to <a href=../SQLConf/#redactOptions>redactOptions</a>.</p> <h2 id=sparksqlredactionstringregex><span id=spark.sql.redaction.string.regex> spark.sql.redaction.string.regex<a class=headerlink href=#sparksqlredactionstringregex title="Permanent link">&para;</a></h2> <p>Regular expression to point at sensitive information in text output</p> <p>Default: <code>(undefined)</code></p> <p>When this regex matches a string part, it is replaced by a dummy value (i.e. <code>*********(redacted)</code>). This is currently used to redact the output of SQL explain commands.</p> <p>NOTE: When this conf is not set, the value of <code>spark.redaction.string.regex</code> is used instead.</p> <p>Use <a href=../SQLConf/#stringRedactionPattern>SQLConf.stringRedactionPattern</a> method to access the current value.</p> <h2 id=sparksqlretaingroupcolumns><span id=spark.sql.retainGroupColumns> spark.sql.retainGroupColumns<a class=headerlink href=#sparksqlretaingroupcolumns title="Permanent link">&para;</a></h2> <p>Controls whether to retain columns used for aggregation or not (in <a href=../spark-sql-RelationalGroupedDataset/ >RelationalGroupedDataset</a> operators).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#dataFrameRetainGroupColumns>SQLConf.dataFrameRetainGroupColumns</a> method to access the current value.</p> <h2 id=sparksqlrunsqlonfiles><span id=spark.sql.runSQLOnFiles> spark.sql.runSQLOnFiles<a class=headerlink href=#sparksqlrunsqlonfiles title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether Spark SQL could use <code>datasource</code>.<code>path</code> as a table in a SQL query.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#runSQLonFile>SQLConf.runSQLonFile</a> method to access the current value.</p> <h2 id=sparksqlselfjoinautoresolveambiguity><span id=spark.sql.selfJoinAutoResolveAmbiguity> spark.sql.selfJoinAutoResolveAmbiguity<a class=headerlink href=#sparksqlselfjoinautoresolveambiguity title="Permanent link">&para;</a></h2> <p>Controls whether to resolve ambiguity in join conditions for <a href=../spark-sql-joins/#join>self-joins</a> automatically (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p> <h2 id=sparksqlsortenableradixsort><span id=spark.sql.sort.enableRadixSort> spark.sql.sort.enableRadixSort<a class=headerlink href=#sparksqlsortenableradixsort title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether to use radix sort (<code>true</code>) or not (<code>false</code>) in <a href=../physical-operators/ShuffleExchangeExec/ >ShuffleExchangeExec</a> and <a href=../physical-operators/SortExec/ >SortExec</a> physical operators</p> <p>Default: <code>true</code></p> <p>Radix sort is much faster but requires additional memory to be reserved up-front. The memory overhead may be significant when sorting very small rows (up to 50% more).</p> <p>Use <a href=../SQLConf/#enableRadixSort>SQLConf.enableRadixSort</a> method to access the current value.</p> <h2 id=sparksqlsourcesbucketingenabled><span id=spark.sql.sources.bucketing.enabled> spark.sql.sources.bucketing.enabled<a class=headerlink href=#sparksqlsourcesbucketingenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-bucketing/ >bucketing</a> support. When disabled (i.e. <code>false</code>), bucketed tables are considered regular (non-bucketed) tables.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#bucketingEnabled>SQLConf.bucketingEnabled</a> method to access the current value.</p> <h2 id=sparksqlsourcesdefault><span id=spark.sql.sources.default> spark.sql.sources.default<a class=headerlink href=#sparksqlsourcesdefault title="Permanent link">&para;</a></h2> <p>Default data source to use in input/output.</p> <p>Default: <code>parquet</code></p> <p>Use <a href=../SQLConf/#defaultDataSourceName>SQLConf.defaultDataSourceName</a> method to access the current value.</p> <h2 id=sparksqlstatisticsfallbacktohdfs><span id=spark.sql.statistics.fallBackToHdfs> spark.sql.statistics.fallBackToHdfs<a class=headerlink href=#sparksqlstatisticsfallbacktohdfs title="Permanent link">&para;</a></h2> <p>Enables automatic calculation of table size statistic by falling back to HDFS if the table statistics are not available from table metadata.</p> <p>Default: <code>false</code></p> <p>This can be useful in determining if a table is small enough for auto broadcast joins in query planning.</p> <p>Use <a href=../SQLConf/#fallBackToHdfsForStatsEnabled>SQLConf.fallBackToHdfsForStatsEnabled</a> method to access the current value.</p> <h2 id=sparksqlstatisticshistogramnumbins><span id=spark.sql.statistics.histogram.numBins> spark.sql.statistics.histogram.numBins<a class=headerlink href=#sparksqlstatisticshistogramnumbins title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The number of bins when generating histograms.</p> <p>Default: <code>254</code></p> <p>NOTE: The number of bins must be greater than 1.</p> <p>Use <a href=../SQLConf/#histogramNumBins>SQLConf.histogramNumBins</a> method to access the current value.</p> <h2 id=sparksqlstatisticsparallelfilelistinginstatscomputationenabled><span id=spark.sql.statistics.parallelFileListingInStatsComputation.enabled> spark.sql.statisticsparallelFileListingInStatsComputation.enabled*<a class=headerlink href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables parallel file listing in SQL commands, e.g. <code>ANALYZE TABLE</code> (as opposed to single thread listing that can be particularly slow with tables with hundreds of partitions)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parallelFileListingInStatsComputation>SQLConf.parallelFileListingInStatsComputation</a> method to access the current value.</p> <h2 id=sparksqlstatisticsndvmaxerror><span id=spark.sql.statistics.ndv.maxError> spark.sql.statistics.ndv.maxError<a class=headerlink href=#sparksqlstatisticsndvmaxerror title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The maximum estimation error allowed in HyperLogLog++ algorithm when generating column level statistics.</p> <p>Default: <code>0.05</code></p> <h2 id=sparksqlstatisticspercentileaccuracy><span id=spark.sql.statistics.percentile.accuracy> spark.sql.statistics.percentile.accuracy<a class=headerlink href=#sparksqlstatisticspercentileaccuracy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Accuracy of percentile approximation when generating equi-height histograms. Larger value means better accuracy. The relative error can be deduced by 1.0 / PERCENTILE_ACCURACY.</p> <p>Default: <code>10000</code></p> <h2 id=sparksqlstatisticssizeautoupdateenabled><span id=spark.sql.statistics.size.autoUpdate.enabled> spark.sql.statistics.size.autoUpdate.enabled<a class=headerlink href=#sparksqlstatisticssizeautoupdateenabled title="Permanent link">&para;</a></h2> <p>Enables automatic update of the table size statistic of a table after the table has changed.</p> <p>Default: <code>false</code></p> <p>IMPORTANT: If the total number of files of the table is very large this can be expensive and slow down data change commands.</p> <p>Use <a href=../SQLConf/#autoSizeUpdateEnabled>SQLConf.autoSizeUpdateEnabled</a> method to access the current value.</p> <h2 id=sparksqlsubexpressioneliminationenabled><span id=spark.sql.subexpressionElimination.enabled> spark.sql.subexpressionElimination.enabled<a class=headerlink href=#sparksqlsubexpressioneliminationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables <a href=../spark-sql-subexpression-elimination/ >subexpression elimination</a></p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#subexpressionEliminationEnabled>SQLConf.subexpressionEliminationEnabled</a> method to access the current value.</p> <h2 id=sparksqlshufflepartitions><span id=spark.sql.shuffle.partitions> spark.sql.shuffle.partitions<a class=headerlink href=#sparksqlshufflepartitions title="Permanent link">&para;</a></h2> <p>The default number of partitions to use when shuffling data for joins or aggregations.</p> <p>Default: <code>200</code></p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Corresponds to Apache Hive's <a href=https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-mapred.reduce.tasks>mapred.reduce.tasks</a> property that Spark SQL considers deprecated.</p> </div> <div class="admonition note"> <p class=admonition-title>Spark Structured Streaming</p> <p><code>spark.sql.shuffle.partitions</code> cannot be changed in Spark Structured Streaming between query restarts from the same checkpoint location.</p> </div> <p>Use <a href=../SQLConf/#numShufflePartitions>SQLConf.numShufflePartitions</a> method to access the current value.</p> <h2 id=sparksqlsourcesfilecompressionfactor><span id=spark.sql.sources.fileCompressionFactor> spark.sql.sources.fileCompressionFactor<a class=headerlink href=#sparksqlsourcesfilecompressionfactor title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When estimating the output data size of a table scan, multiply the file size with this factor as the estimated data size, in case the data is compressed in the file and lead to a heavily underestimated result.</p> <p>Default: <code>1.0</code></p> <p>Use <a href=../SQLConf/#fileCompressionFactor>SQLConf.fileCompressionFactor</a> method to access the current value.</p> <h2 id=sparksqlsourcespartitionoverwritemode><span id=spark.sql.sources.partitionOverwriteMode> spark.sql.sources.partitionOverwriteMode<a class=headerlink href=#sparksqlsourcespartitionoverwritemode title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-dynamic-partition-inserts/ >dynamic partition inserts</a> when <code>dynamic</code></p> <p>Default: <code>static</code></p> <p>When <code>INSERT OVERWRITE</code> a partitioned data source table with dynamic partition columns, Spark SQL supports two modes (case-insensitive):</p> <ul> <li> <p><strong>static</strong> - Spark deletes all the partitions that match the partition specification (e.g. <code>PARTITION(a=1,b)</code>) in the INSERT statement, before overwriting</p> </li> <li> <p><strong>dynamic</strong> - Spark doesn't delete partitions ahead, and only overwrites those partitions that have data written into it</p> </li> </ul> <p>The default <code>STATIC</code> overwrite mode is to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode.</p> <p>Use <a href=../SQLConf/#partitionOverwriteMode>SQLConf.partitionOverwriteMode</a> method to access the current value.</p> <h2 id=sparksqltruncatetableignorepermissionaclenabled><span id=spark.sql.truncateTable.ignorePermissionAcl.enabled> spark.sql.truncateTable.ignorePermissionAcl.enabled<a class=headerlink href=#sparksqltruncatetableignorepermissionaclenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Disables setting back original permission and ACLs when re-creating the table/partition paths for <a href=../logical-operators/TruncateTableCommand/ >TRUNCATE TABLE</a> command.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#truncateTableIgnorePermissionAcl>SQLConf.truncateTableIgnorePermissionAcl</a> method to access the current value.</p> <h2 id=sparksqluiretainedexecutions><span id=spark.sql.ui.retainedExecutions> spark.sql.ui.retainedExecutions<a class=headerlink href=#sparksqluiretainedexecutions title="Permanent link">&para;</a></h2> <p>The number of <a href=../spark-sql-SQLListener/#SQLExecutionUIData>SQLExecutionUIData</a> entries to keep in <code>failedExecutions</code> and <code>completedExecutions</code> internal registries.</p> <p>Default: <code>1000</code></p> <p>When a query execution finishes, the execution is removed from the internal <code>activeExecutions</code> registry and stored in <code>failedExecutions</code> or <code>completedExecutions</code> given the end execution status. It is when <code>SQLListener</code> makes sure that the number of <code>SQLExecutionUIData</code> entires does not exceed <code>spark.sql.ui.retainedExecutions</code> Spark property and removes the excess of entries.</p> <h2 id=sparksqlwindowexecbufferinmemorythreshold><span id=spark.sql.windowExec.buffer.in.memory.threshold> spark.sql.windowExec.buffer.in.memory.threshold<a class=headerlink href=#sparksqlwindowexecbufferinmemorythreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Threshold for number of rows guaranteed to be held in memory by <a href=../physical-operators/WindowExec/ >WindowExec</a> physical operator.</p> <p>Default: <code>4096</code></p> <p>Use <a href=../SQLConf/#windowExecBufferInMemoryThreshold>SQLConf.windowExecBufferInMemoryThreshold</a> method to access the current value.</p> <h2 id=sparksqlwindowexecbufferspillthreshold><span id=spark.sql.windowExec.buffer.spill.threshold> spark.sql.windowExec.buffer.spill.threshold<a class=headerlink href=#sparksqlwindowexecbufferspillthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Threshold for number of rows buffered in a <a href=../physical-operators/WindowExec/ >WindowExec</a> physical operator.</p> <p>Default: <code>4096</code></p> <p>Use <a href=../SQLConf/#windowExecBufferSpillThreshold>SQLConf.windowExecBufferSpillThreshold</a> method to access the current value.</p> <hr> <div class=md-source-date> <small> Last update: 2020-11-08 </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../ColumnarRule/ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> ColumnarRule </div> </div> </a> <a href=../sql/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> SQL Parsing Framework </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener>Jacek Laskowski</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/jaceklaskowski target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://linkedin.com/in/jaceklaskowski target=_blank rel=noopener title=linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/vendor.7e0ee788.min.js></script> <script src=../assets/javascripts/bundle.b3a72adc.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs', 'navigation.instant'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> </body> </html>