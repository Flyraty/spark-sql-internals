<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Demystifying inner-workings of Spark SQL"><meta name=author content="Jacek Laskowski"><link href=https://jaceklaskowski.github.io/mastering-spark-sql-book/configuration-properties/ rel=canonical><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.2.2, mkdocs-material-7.2.6+insiders-2.13.3"><title>Configuration Properties - The Internals of Spark SQL</title><link rel=stylesheet href=../assets/stylesheets/main.c94377ce.min.css><link rel=stylesheet href=../assets/stylesheets/palette.1d832a92.min.css><link rel=preload as=style href=../assets/stylesheets/vendor/mermaid.733f213f.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style><script>function __md_scope(e,t,_){return new URL(_||(t===localStorage?"..":".."),location).pathname+"."+e}function __md_get(e,t=localStorage,_){return JSON.parse(t.getItem(__md_scope(e,t,_)))}function __md_set(e,t,_=localStorage,o){try{_.setItem(__md_scope(e,_,o),JSON.stringify(t))}catch(e){}}</script><script id=__analytics>function __md_analytics(){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-151208281-4","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){var e;this.value&&(e=document.location.pathname,ga("send","pageview",e+"?q="+this.value))}),"undefined"!=typeof location$&&location$.subscribe(function(e){ga("send","pageview",e.pathname)})});var e=document.createElement("script");e.async=!0,e.src="https://www.google-analytics.com/analytics.js",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#configuration-properties class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="The Internals of Spark SQL" class="md-header__button md-logo" aria-label="The Internals of Spark SQL" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> The Internals of Spark SQL </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Configuration Properties </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M7 10a2 2 0 0 1 2 2 2 2 0 0 1-2 2 2 2 0 0 1-2-2 2 2 0 0 1 2-2m10-3a5 5 0 0 1 5 5 5 5 0 0 1-5 5H7a5 5 0 0 1-5-5 5 5 0 0 1 5-5h10M7 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3h10a3 3 0 0 0 3-3 3 3 0 0 0-3-3H7z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=blue data-md-color-accent=blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-sql-internals </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../overview/ class="md-tabs__link md-tabs__link--active"> Internals </a> </li> <li class=md-tabs__item> <a href=../common-table-expressions/ class=md-tabs__link> New &amp; Noteworthy </a> </li> <li class=md-tabs__item> <a href=../SparkSession/ class=md-tabs__link> High-Level APIs </a> </li> <li class=md-tabs__item> <a href=../demo/ class=md-tabs__link> Demos </a> </li> <li class=md-tabs__item> <a href=../JoinSelectionHelper/ class=md-tabs__link> Misc </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="The Internals of Spark SQL" class="md-nav__button md-logo" aria-label="The Internals of Spark SQL" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> The Internals of Spark SQL </label> <div class=md-nav__source> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-sql-internals </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Internals <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Internals data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Internals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../overview/ class=md-nav__link> Overview </a> </li> <li class=md-nav__item> <a href=../tags/ class=md-nav__link> Tags </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3 type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3> Configuration Properties <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Configuration Properties" data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Configuration Properties </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Configuration Properties <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Configuration Properties </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#sparksqladaptiveautobroadcastjointhreshold class=md-nav__link> spark.sql.adaptive.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecustomcostevaluatorclass class=md-nav__link> spark.sql.adaptive.customCostEvaluatorClass </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdecorrelateinnerqueryenabled class=md-nav__link> spark.sql.optimizer.decorrelateInnerQuery.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveoptimizeskewsinrebalancepartitionsenabled class=md-nav__link> spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownonemptylocationinctas class=md-nav__link> spark.sql.legacy.allowNonEmptyLocationInCTAS </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowautogeneratedaliasforview class=md-nav__link> spark.sql.legacy.allowAutoGeneratedAliasForView </a> </li> <li class=md-nav__item> <a href=#sparksqlsessionwindowbufferspillthreshold class=md-nav__link> spark.sql.sessionWindow.buffer.spill.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkselfdestructenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.selfDestruct.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowstarwithsingletableidentifierincount class=md-nav__link> spark.sql.legacy.allowStarWithSingleTableIdentifierInCount </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerenablecsvexpressionoptimization class=md-nav__link> spark.sql.optimizer.enableCsvExpressionOptimization </a> </li> <li class=md-nav__item> <a href=#sparksqlsessionwindowbufferinmemorythreshold class=md-nav__link> spark.sql.sessionWindow.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlorcenablenestedcolumnvectorizedreader class=md-nav__link> spark.sql.orc.enableNestedColumnVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizeroptimizeonerowrelationsubquery class=md-nav__link> spark.sql.optimizer.optimizeOneRowRelationSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveforceapply class=md-nav__link> spark.sql.adaptive.forceApply </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsenabled class=md-nav__link> spark.sql.adaptive.coalescePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionsize class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionSize </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsparallelismfirst class=md-nav__link> spark.sql.adaptive.coalescePartitions.parallelismFirst </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveadvisorypartitionsizeinbytes class=md-nav__link> spark.sql.adaptive.advisoryPartitionSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsinitialpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.initialPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveenabled class=md-nav__link> spark.sql.adaptive.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivefetchshuffleblocksinbatch class=md-nav__link> spark.sql.adaptive.fetchShuffleBlocksInBatch </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivelocalshufflereaderenabled class=md-nav__link> spark.sql.adaptive.localShuffleReader.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveloglevel class=md-nav__link> spark.sql.adaptive.logLevel </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivemaxshuffledhashjoinlocalmapthreshold class=md-nav__link> spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveoptimizerexcludedrules class=md-nav__link> spark.sql.adaptive.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinenabled class=md-nav__link> spark.sql.adaptive.skewJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionfactor class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin class=md-nav__link> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzermaxiterations class=md-nav__link> spark.sql.analyzer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzerfailambiguousselfjoin class=md-nav__link> spark.sql.analyzer.failAmbiguousSelfJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlansienabled class=md-nav__link> spark.sql.ansi.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcliprintheader class=md-nav__link> spark.sql.cli.print.header </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenwholestage class=md-nav__link> spark.sql.codegen.wholeStage </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmethodsplitthreshold class=md-nav__link> spark.sql.codegen.methodSplitThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqldebugmaxtostringfields class=md-nav__link> spark.sql.debug.maxToStringFields </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultcatalog class=md-nav__link> spark.sql.defaultCatalog </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionremoveredundantsorts class=md-nav__link> spark.sql.execution.removeRedundantSorts </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionreusesubquery class=md-nav__link> spark.sql.execution.reuseSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionsortbeforerepartition class=md-nav__link> spark.sql.execution.sortBeforeRepartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionrangeexchangesamplesizeperpartition class=md-nav__link> spark.sql.execution.rangeExchange.sampleSizePerPartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkfallbackenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.fallback.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowsparkrenabled class=md-nav__link> spark.sql.execution.arrow.sparkr.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasudfbuffersize class=md-nav__link> spark.sql.execution.pandas.udf.buffer.size </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasconverttoarrowarraysafely class=md-nav__link> spark.sql.execution.pandas.convertToArrowArraySafely </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramenabled class=md-nav__link> spark.sql.statistics.histogram.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsessiontimezone class=md-nav__link> spark.sql.session.timeZone </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcescommitprotocolclass class=md-nav__link> spark.sql.sources.commitProtocolClass </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesignoredatalocality class=md-nav__link> spark.sql.sources.ignoreDataLocality </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesvalidatepartitioncolumns class=md-nav__link> spark.sql.sources.validatePartitionColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesusev1sourcelist class=md-nav__link> spark.sql.sources.useV1SourceList </a> </li> <li class=md-nav__item> <a href=#sparksqlstoreassignmentpolicy class=md-nav__link> spark.sql.storeAssignmentPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlthriftserverinterruptoncancel class=md-nav__link> spark.sql.thriftServer.interruptOnCancel </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetswitchthreshold class=md-nav__link> spark.sql.optimizer.inSetSwitchThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangeloglevel class=md-nav__link> spark.sql.optimizer.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogrules class=md-nav__link> spark.sql.optimizer.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogbatches class=md-nav__link> spark.sql.optimizer.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningenabled class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlhivetablepropertylengththreshold class=md-nav__link> spark.sql.hive.tablePropertyLengthThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizercanchangecachedplanoutputpartitioning class=md-nav__link> spark.sql.optimizer.canChangeCachedPlanOutputPartitioning </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningpruningsideextrafilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningusestats class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.useStats </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources class=md-nav__link> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerserializernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexpressionnestedpruningenabled class=md-nav__link> spark.sql.optimizer.expression.nestedPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcmergeschema class=md-nav__link> spark.sql.orc.mergeSchema </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingautobucketedscanenabled class=md-nav__link> spark.sql.sources.bucketing.autoBucketedScan.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldatetimejava8apienabled class=md-nav__link> spark.sql.datetime.java8API.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyintervalenabled class=md-nav__link> spark.sql.legacy.interval.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbinaryfilemaxlength class=md-nav__link> spark.sql.sources.binaryFile.maxLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmapkeydeduppolicy class=md-nav__link> spark.sql.mapKeyDedupPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxconcurrentoutputfilewriters class=md-nav__link> spark.sql.maxConcurrentOutputFileWriters </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxmetadatastringlength class=md-nav__link> spark.sql.maxMetadataStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmavenadditionalremoterepositories class=md-nav__link> spark.sql.maven.additionalRemoteRepositories </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxplanstringlength class=md-nav__link> spark.sql.maxPlanStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqladdpartitioninbatchsize class=md-nav__link> spark.sql.addPartitionInBatch.size </a> </li> <li class=md-nav__item> <a href=#sparksqlscripttransformationexittimeoutinseconds class=md-nav__link> spark.sql.scriptTransformation.exitTimeoutInSeconds </a> </li> <li class=md-nav__item> <a href=#sparksqlautobroadcastjointhreshold class=md-nav__link> spark.sql.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqlavrocompressioncodec class=md-nav__link> spark.sql.avro.compression.codec </a> </li> <li class=md-nav__item> <a href=#sparksqlbroadcasttimeout class=md-nav__link> spark.sql.broadcastTimeout </a> </li> <li class=md-nav__item> <a href=#sparksqlbucketingcoalescebucketsinjoinenabled class=md-nav__link> spark.sql.bucketing.coalesceBucketsInJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcasesensitive class=md-nav__link> spark.sql.caseSensitive </a> </li> <li class=md-nav__item> <a href=#sparksqlcatalogspark_catalog class=md-nav__link> spark.sql.catalog.spark_catalog </a> </li> <li class=md-nav__item> <a href=#sparksqlcboenabled class=md-nav__link> spark.sql.cbo.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbojoinreorderenabled class=md-nav__link> spark.sql.cbo.joinReorder.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcboplanstatsenabled class=md-nav__link> spark.sql.cbo.planStats.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbostarschemadetection class=md-nav__link> spark.sql.cbo.starSchemaDetection </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatemapvectorizedenable class=md-nav__link> spark.sql.codegen.aggregate.map.vectorized.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatesplitaggregatefuncenabled class=md-nav__link> spark.sql.codegen.aggregate.splitAggregateFunc.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegencomments class=md-nav__link> spark.sql.codegen.comments </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfactorymode class=md-nav__link> spark.sql.codegen.factoryMode </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfallback class=md-nav__link> spark.sql.codegen.fallback </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenhugemethodlimit class=md-nav__link> spark.sql.codegen.hugeMethodLimit </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenuseidinclassname class=md-nav__link> spark.sql.codegen.useIdInClassName </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmaxfields class=md-nav__link> spark.sql.codegen.maxFields </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegensplitconsumefuncbyoperator class=md-nav__link> spark.sql.codegen.splitConsumeFuncByOperator </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnvectoroffheapenabled class=md-nav__link> spark.sql.columnVector.offheap.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnnameofcorruptrecord class=md-nav__link> spark.sql.columnNameOfCorruptRecord </a> </li> <li class=md-nav__item> <a href=#sparksqlconstraintpropagationenabled class=md-nav__link> spark.sql.constraintPropagation.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcsvfilterpushdownenabled class=md-nav__link> spark.sql.csv.filterPushdown.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultsizeinbytes class=md-nav__link> spark.sql.defaultSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqldialect class=md-nav__link> spark.sql.dialect </a> </li> <li class=md-nav__item> <a href=#sparksqlexchangereuse class=md-nav__link> spark.sql.exchange.reuse </a> </li> <li class=md-nav__item> <a href=#executionuseobjecthashaggregateexec class=md-nav__link> execution.useObjectHashAggregateExec </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignorecorruptfiles class=md-nav__link> spark.sql.files.ignoreCorruptFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignoremissingfiles class=md-nav__link> spark.sql.files.ignoreMissingFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxrecordsperfile class=md-nav__link> spark.sql.files.maxRecordsPerFile </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxpartitionbytes class=md-nav__link> spark.sql.files.maxPartitionBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesopencostinbytes class=md-nav__link> spark.sql.files.openCostInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragecompressed class=md-nav__link> spark.sql.inMemoryColumnarStorage.compressed </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragebatchsize class=md-nav__link> spark.sql.inMemoryColumnarStorage.batchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorytablescanstatisticsenable class=md-nav__link> spark.sql.inMemoryTableScanStatistics.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstorageenablevectorizedreader class=md-nav__link> spark.sql.inMemoryColumnarStorage.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragepartitionpruning class=md-nav__link> spark.sql.inMemoryColumnarStorage.partitionPruning </a> </li> <li class=md-nav__item> <a href=#sparksqljoinprefersortmergejoin class=md-nav__link> spark.sql.join.preferSortMergeJoin </a> </li> <li class=md-nav__item> <a href=#sparksqljsongeneratorignorenullfields class=md-nav__link> spark.sql.jsonGenerator.ignoreNullFields </a> </li> <li class=md-nav__item> <a href=#sparksqlleafnodedefaultparallelism class=md-nav__link> spark.sql.leafNodeDefaultParallelism </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydolooseupcast class=md-nav__link> spark.sql.legacy.doLooseUpcast </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycteprecedencepolicy class=md-nav__link> spark.sql.legacy.ctePrecedencePolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytimeparserpolicy class=md-nav__link> spark.sql.legacy.timeParserPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfollowthreevaluedlogicinarrayexists class=md-nav__link> spark.sql.legacy.followThreeValuedLogicInArrayExists </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfromdaytimestringenabled class=md-nav__link> spark.sql.legacy.fromDayTimeString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacynotreserveproperties class=md-nav__link> spark.sql.legacy.notReserveProperties </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyaddsinglefileinaddfile class=md-nav__link> spark.sql.legacy.addSingleFileInAddFile </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyexponentliteralasdecimalenabled class=md-nav__link> spark.sql.legacy.exponentLiteralAsDecimal.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownegativescaleofdecimal class=md-nav__link> spark.sql.legacy.allowNegativeScaleOfDecimal </a> </li> <li class=md-nav__item> <a href=#sparksqllegacybucketedtablescanoutputordering class=md-nav__link> spark.sql.legacy.bucketedTableScan.outputOrdering </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyjsonallowemptystringenabled class=md-nav__link> spark.sql.legacy.json.allowEmptyString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycreateemptycollectionusingstringtype class=md-nav__link> spark.sql.legacy.createEmptyCollectionUsingStringType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowuntypedscalaudf class=md-nav__link> spark.sql.legacy.allowUntypedScalaUDF </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue class=md-nav__link> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue </a> </li> <li class=md-nav__item> <a href=#sparksqllegacysetcommandrejectssparkcoreconfs class=md-nav__link> spark.sql.legacy.setCommandRejectsSparkCoreConfs </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytypecoerciondatetimetostringenabled class=md-nav__link> spark.sql.legacy.typeCoercion.datetimeToString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowhashonmaptype class=md-nav__link> spark.sql.legacy.allowHashOnMapType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyrddapplyconf class=md-nav__link> spark.sql.legacy.rdd.applyConf </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyreplacedatabrickssparkavroenabled class=md-nav__link> spark.sql.legacy.replaceDatabricksSparkAvro.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllimitscaleupfactor class=md-nav__link> spark.sql.limit.scaleUpFactor </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizenullawareantijoin class=md-nav__link> spark.sql.optimizeNullAwareAntiJoin </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexcludedrules class=md-nav__link> spark.sql.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetconversionthreshold class=md-nav__link> spark.sql.optimizer.inSetConversionThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizermaxiterations class=md-nav__link> spark.sql.optimizer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerreplaceexceptwithfilter class=md-nav__link> spark.sql.optimizer.replaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcimpl class=md-nav__link> spark.sql.orc.impl </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangeloglevel class=md-nav__link> spark.sql.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangelogbatches class=md-nav__link> spark.sql.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangelogrules class=md-nav__link> spark.sql.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqlpysparkjvmstacktraceenabled class=md-nav__link> spark.sql.pyspark.jvmStacktrace.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetbinaryasstring class=md-nav__link> spark.sql.parquet.binaryAsString </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetcolumnarreaderbatchsize class=md-nav__link> spark.sql.parquet.columnarReaderBatchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96astimestamp class=md-nav__link> spark.sql.parquet.int96AsTimestamp </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetenablevectorizedreader class=md-nav__link> spark.sql.parquet.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdown class=md-nav__link> spark.sql.parquet.filterPushdown </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdowndate class=md-nav__link> spark.sql.parquet.filterPushdown.date </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96timestampconversion class=md-nav__link> spark.sql.parquet.int96TimestampConversion </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetrecordlevelfilterenabled class=md-nav__link> spark.sql.parquet.recordLevelFilter.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparserquotedregexcolumnnames class=md-nav__link> spark.sql.parser.quotedRegexColumnNames </a> </li> <li class=md-nav__item> <a href=#sparksqlpivotmaxvalues class=md-nav__link> spark.sql.pivotMaxValues </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionoptionsregex class=md-nav__link> spark.sql.redaction.options.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionstringregex class=md-nav__link> spark.sql.redaction.string.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlretaingroupcolumns class=md-nav__link> spark.sql.retainGroupColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlrunsqlonfiles class=md-nav__link> spark.sql.runSQLOnFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlselfjoinautoresolveambiguity class=md-nav__link> spark.sql.selfJoinAutoResolveAmbiguity </a> </li> <li class=md-nav__item> <a href=#sparksqlsortenableradixsort class=md-nav__link> spark.sql.sort.enableRadixSort </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingenabled class=md-nav__link> spark.sql.sources.bucketing.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesdefault class=md-nav__link> spark.sql.sources.default </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsfallbacktohdfs class=md-nav__link> spark.sql.statistics.fallBackToHdfs </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramnumbins class=md-nav__link> spark.sql.statistics.histogram.numBins </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled class=md-nav__link> spark.sql.statisticsparallelFileListingInStatsComputation.enabled* </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsndvmaxerror class=md-nav__link> spark.sql.statistics.ndv.maxError </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticspercentileaccuracy class=md-nav__link> spark.sql.statistics.percentile.accuracy </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticssizeautoupdateenabled class=md-nav__link> spark.sql.statistics.size.autoUpdate.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsubexpressioneliminationenabled class=md-nav__link> spark.sql.subexpressionElimination.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlshufflepartitions class=md-nav__link> spark.sql.shuffle.partitions </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesfilecompressionfactor class=md-nav__link> spark.sql.sources.fileCompressionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcespartitionoverwritemode class=md-nav__link> spark.sql.sources.partitionOverwriteMode </a> </li> <li class=md-nav__item> <a href=#sparksqltruncatetableignorepermissionaclenabled class=md-nav__link> spark.sql.truncateTable.ignorePermissionAcl.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqluiretainedexecutions class=md-nav__link> spark.sql.ui.retainedExecutions </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferinmemorythreshold class=md-nav__link> spark.sql.windowExec.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferspillthreshold class=md-nav__link> spark.sql.windowExec.buffer.spill.threshold </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../SQLConf/ class=md-nav__link> SQLConf </a> </li> <li class=md-nav__item> <a href=../StaticSQLConf/ class=md-nav__link> StaticSQLConf </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../DataSource/ class=md-nav__link> DataSource </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_5 type=checkbox id=__nav_2_5> <div class="md-nav__link md-nav__link--container "> <a href=../connector/ >Connector API</a> <label for=__nav_2_5> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Connector API" data-md-level=2> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> Connector API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/ApplyTransform/ class=md-nav__link> ApplyTransform </a> </li> <li class=md-nav__item> <a href=../connector/Batch/ class=md-nav__link> Batch </a> </li> <li class=md-nav__item> <a href=../connector/BatchWrite/ class=md-nav__link> BatchWrite </a> </li> <li class=md-nav__item> <a href=../connector/DataSourceV2Implicits/ class=md-nav__link> DataSourceV2Implicits </a> </li> <li class=md-nav__item> <a href=../connector/DataWriter/ class=md-nav__link> DataWriter </a> </li> <li class=md-nav__item> <a href=../connector/DataWriterFactory/ class=md-nav__link> DataWriterFactory </a> </li> <li class=md-nav__item> <a href=../connector/InputPartition/ class=md-nav__link> InputPartition </a> </li> <li class=md-nav__item> <a href=../connector/FileTable/ class=md-nav__link> FileTable </a> </li> <li class=md-nav__item> <a href=../connector/MetadataColumn/ class=md-nav__link> MetadataColumn </a> </li> <li class=md-nav__item> <a href=../connector/MetadataColumnHelper/ class=md-nav__link> MetadataColumnHelper </a> </li> <li class=md-nav__item> <a href=../connector/MetadataColumnsHelper/ class=md-nav__link> MetadataColumnsHelper </a> </li> <li class=md-nav__item> <a href=../connector/OptionsHelper/ class=md-nav__link> OptionsHelper </a> </li> <li class=md-nav__item> <a href=../connector/Partitioning/ class=md-nav__link> Partitioning </a> </li> <li class=md-nav__item> <a href=../connector/PartitionReader/ class=md-nav__link> PartitionReader </a> </li> <li class=md-nav__item> <a href=../connector/PartitionReaderFactory/ class=md-nav__link> PartitionReaderFactory </a> </li> <li class=md-nav__item> <a href=../connector/PartitionSpecsHelper/ class=md-nav__link> PartitionSpecsHelper </a> </li> <li class=md-nav__item> <a href=../connector/RewritableTransform/ class=md-nav__link> RewritableTransform </a> </li> <li class=md-nav__item> <a href=../connector/Scan/ class=md-nav__link> Scan </a> </li> <li class=md-nav__item> <a href=../connector/ScanBuilder/ class=md-nav__link> ScanBuilder </a> </li> <li class=md-nav__item> <a href=../connector/SessionConfigSupport/ class=md-nav__link> SessionConfigSupport </a> </li> <li class=md-nav__item> <a href=../connector/SimpleTableProvider/ class=md-nav__link> SimpleTableProvider </a> </li> <li class=md-nav__item> <a href=../connector/StagedTable/ class=md-nav__link> StagedTable </a> </li> <li class=md-nav__item> <a href=../connector/SupportsAtomicPartitionManagement/ class=md-nav__link> SupportsAtomicPartitionManagement </a> </li> <li class=md-nav__item> <a href=../connector/SupportsDelete/ class=md-nav__link> SupportsDelete </a> </li> <li class=md-nav__item> <a href=../connector/SupportsDynamicOverwrite/ class=md-nav__link> SupportsDynamicOverwrite </a> </li> <li class=md-nav__item> <a href=../connector/SupportsMetadataColumns/ class=md-nav__link> SupportsMetadataColumns </a> </li> <li class=md-nav__item> <a href=../connector/SupportsOverwrite/ class=md-nav__link> SupportsOverwrite </a> </li> <li class=md-nav__item> <a href=../connector/SupportsPartitionManagement/ class=md-nav__link> SupportsPartitionManagement </a> </li> <li class=md-nav__item> <a href=../connector/SupportsPushDownFilters/ class=md-nav__link> SupportsPushDownFilters </a> </li> <li class=md-nav__item> <a href=../connector/SupportsPushDownRequiredColumns/ class=md-nav__link> SupportsPushDownRequiredColumns </a> </li> <li class=md-nav__item> <a href=../connector/SupportsRead/ class=md-nav__link> SupportsRead </a> </li> <li class=md-nav__item> <a href=../connector/SupportsReportStatistics/ class=md-nav__link> SupportsReportStatistics </a> </li> <li class=md-nav__item> <a href=../connector/SupportsReportPartitioning/ class=md-nav__link> SupportsReportPartitioning </a> </li> <li class=md-nav__item> <a href=../connector/SupportsStreamingUpdate/ class=md-nav__link> SupportsStreamingUpdate </a> </li> <li class=md-nav__item> <a href=../connector/SupportsTruncate/ class=md-nav__link> SupportsTruncate </a> </li> <li class=md-nav__item> <a href=../connector/SupportsWrite/ class=md-nav__link> SupportsWrite </a> </li> <li class=md-nav__item> <a href=../connector/Table/ class=md-nav__link> Table </a> </li> <li class=md-nav__item> <a href=../connector/TableCapability/ class=md-nav__link> TableCapability </a> </li> <li class=md-nav__item> <a href=../connector/TableHelper/ class=md-nav__link> TableHelper </a> </li> <li class=md-nav__item> <a href=../connector/TableProvider/ class=md-nav__link> TableProvider </a> </li> <li class=md-nav__item> <a href=../connector/Transform/ class=md-nav__link> Transform </a> </li> <li class=md-nav__item> <a href=../connector/V1Scan/ class=md-nav__link> V1Scan </a> </li> <li class=md-nav__item> <a href=../connector/V1Table/ class=md-nav__link> V1Table </a> </li> <li class=md-nav__item> <a href=../connector/V1WriteBuilder/ class=md-nav__link> V1WriteBuilder </a> </li> <li class=md-nav__item> <a href=../connector/WriteBuilder/ class=md-nav__link> WriteBuilder </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_6 type=checkbox id=__nav_2_6> <label class=md-nav__link for=__nav_2_6> DataSource V1 API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="DataSource V1 API" data-md-level=2> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> DataSource V1 API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataSourceRegister/ class=md-nav__link> DataSourceRegister </a> </li> <li class=md-nav__item> <a href=../CreatableRelationProvider/ class=md-nav__link> CreatableRelationProvider </a> </li> <li class=md-nav__item> <a href=../RelationProvider/ class=md-nav__link> RelationProvider </a> </li> <li class=md-nav__item> <a href=../SchemaRelationProvider/ class=md-nav__link> SchemaRelationProvider </a> </li> <li class=md-nav__item> <a href=../BaseRelation/ class=md-nav__link> BaseRelation </a> </li> <li class=md-nav__item> <a href=../FileRelation/ class=md-nav__link> FileRelation </a> </li> <li class=md-nav__item> <a href=../InsertableRelation/ class=md-nav__link> InsertableRelation </a> </li> <li class=md-nav__item> <a href=../PrunedFilteredScan/ class=md-nav__link> PrunedFilteredScan </a> </li> <li class=md-nav__item> <a href=../PrunedScan/ class=md-nav__link> PrunedScan </a> </li> <li class=md-nav__item> <a href=../TableScan/ class=md-nav__link> TableScan </a> </li> <li class=md-nav__item> <a href=../Filter/ class=md-nav__link> Filter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_7 type=checkbox id=__nav_2_7> <label class=md-nav__link for=__nav_2_7> Catalog Plugin API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Catalog Plugin API" data-md-level=2> <label class=md-nav__title for=__nav_2_7> <span class="md-nav__icon md-icon"></span> Catalog Plugin API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/catalog/CatalogManager/ class=md-nav__link> CatalogManager </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogPlugin/ class=md-nav__link> CatalogPlugin </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogHelper/ class=md-nav__link> CatalogHelper </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogExtension/ class=md-nav__link> CatalogExtension </a> </li> <li class=md-nav__item> <a href=../connector/catalog/DelegatingCatalogExtension/ class=md-nav__link> DelegatingCatalogExtension </a> </li> <li class=md-nav__item> <a href=../connector/catalog/StagingTableCatalog/ class=md-nav__link> StagingTableCatalog </a> </li> <li class=md-nav__item> <a href=../connector/catalog/SupportsNamespaces/ class=md-nav__link> SupportsNamespaces </a> </li> <li class=md-nav__item> <a href=../connector/catalog/SupportsCatalogOptions/ class=md-nav__link> SupportsCatalogOptions </a> </li> <li class=md-nav__item> <a href=../connector/catalog/TableCatalog/ class=md-nav__link> TableCatalog </a> </li> <li class=md-nav__item> <a href=../connector/catalog/TableChange/ class=md-nav__link> TableChange </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_7_11 type=checkbox id=__nav_2_7_11> <label class=md-nav__link for=__nav_2_7_11> Utilities <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Utilities data-md-level=3> <label class=md-nav__title for=__nav_2_7_11> <span class="md-nav__icon md-icon"></span> Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../connector/catalog/Catalogs/ class=md-nav__link> Catalogs </a> </li> <li class=md-nav__item> <a href=../connector/catalog/CatalogV2Util/ class=md-nav__link> CatalogV2Util </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_8 type=checkbox id=__nav_2_8> <label class=md-nav__link for=__nav_2_8> Query Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Query Execution" data-md-level=2> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> Query Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../QueryExecution/ class=md-nav__link> QueryExecution </a> </li> <li class=md-nav__item> <a href=../Analyzer/ class=md-nav__link> Logical Analyzer </a> </li> <li class=md-nav__item> <a href=../SparkPlanner/ class=md-nav__link> SparkPlanner </a> </li> <li class=md-nav__item> <a href=../SparkOptimizer/ class=md-nav__link> SparkOptimizer </a> </li> <li class=md-nav__item> <a href=../QueryPlanningTracker/ class=md-nav__link> QueryPlanningTracker </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../SharedState/ class=md-nav__link> SharedState </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_10 type=checkbox id=__nav_2_10> <div class="md-nav__link md-nav__link--container "> <a href=../sql/ >SQL Support</a> <label for=__nav_2_10> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="SQL Support" data-md-level=2> <label class=md-nav__title for=__nav_2_10> <span class="md-nav__icon md-icon"></span> SQL Support </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../sql/ParserInterface/ class=md-nav__link> ParserInterface </a> </li> <li class=md-nav__item> <a href=../sql/AbstractSqlParser/ class=md-nav__link> AbstractSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/AstBuilder/ class=md-nav__link> AstBuilder </a> </li> <li class=md-nav__item> <a href=../sql/CatalystSqlParser/ class=md-nav__link> CatalystSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/SparkSqlParser/ class=md-nav__link> SparkSqlParser </a> </li> <li class=md-nav__item> <a href=../sql/SparkSqlAstBuilder/ class=md-nav__link> SparkSqlAstBuilder </a> </li> <li class=md-nav__item> <a href=../sql/CatalogV2Implicits/ class=md-nav__link> CatalogV2Implicits </a> </li> <li class=md-nav__item> <a href=../sql/LogicalExpressions/ class=md-nav__link> LogicalExpressions </a> </li> <li class=md-nav__item> <a href=../sql/Expressions/ class=md-nav__link> Expressions </a> </li> <li class=md-nav__item> <a href=../sql/FieldReference/ class=md-nav__link> FieldReference </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_11 type=checkbox id=__nav_2_11> <label class=md-nav__link for=__nav_2_11> Statistics and Hints <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Statistics and Hints" data-md-level=2> <label class=md-nav__title for=__nav_2_11> <span class="md-nav__icon md-icon"></span> Statistics and Hints </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/Statistics/ class=md-nav__link> Statistics </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalPlanStats/ class=md-nav__link> LogicalPlanStats </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalPlanVisitor/ class=md-nav__link> LogicalPlanVisitor </a> </li> <li class=md-nav__item> <a href=../logical-operators/SizeInBytesOnlyStatsPlanVisitor/ class=md-nav__link> SizeInBytesOnlyStatsPlanVisitor </a> </li> <li class=md-nav__item> <a href=../logical-operators/BasicStatsPlanVisitor/ class=md-nav__link> BasicStatsPlanVisitor </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_11_6 type=checkbox id=__nav_2_11_6> <label class=md-nav__link for=__nav_2_11_6> Estimation Utilities <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Estimation Utilities" data-md-level=3> <label class=md-nav__title for=__nav_2_11_6> <span class="md-nav__icon md-icon"></span> Estimation Utilities </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/AggregateEstimation/ class=md-nav__link> AggregateEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/EstimationUtils/ class=md-nav__link> EstimationUtils </a> </li> <li class=md-nav__item> <a href=../logical-operators/FilterEstimation/ class=md-nav__link> FilterEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/JoinEstimation/ class=md-nav__link> JoinEstimation </a> </li> <li class=md-nav__item> <a href=../logical-operators/ProjectEstimation/ class=md-nav__link> ProjectEstimation </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_12 type=checkbox id=__nav_2_12> <div class="md-nav__link md-nav__link--container "> <a href=../logical-operators/ >Logical Operators</a> <label for=__nav_2_12> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Logical Operators" data-md-level=2> <label class=md-nav__title for=__nav_2_12> <span class="md-nav__icon md-icon"></span> Logical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-operators/LogicalPlan/ class=md-nav__link> LogicalPlan </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterTable/ class=md-nav__link> AlterTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/CommentOnTable/ class=md-nav__link> CommentOnTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTableAsSelect/ class=md-nav__link> CreateTableAsSelect </a> </li> <li class=md-nav__item> <a href=../logical-operators/CTERelationRef/ class=md-nav__link> CTERelationRef </a> </li> <li class=md-nav__item> <a href=../logical-operators/CacheTableCommand/ class=md-nav__link> CacheTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/IgnoreCachedData/ class=md-nav__link> IgnoreCachedData </a> </li> <li class=md-nav__item> <a href=../logical-operators/MapPartitions/ class=md-nav__link> MapPartitions </a> </li> <li class=md-nav__item> <a href=../logical-operators/ObjectConsumer/ class=md-nav__link> ObjectConsumer </a> </li> <li class=md-nav__item> <a href=../logical-operators/ObjectProducer/ class=md-nav__link> ObjectProducer </a> </li> <li class=md-nav__item> <a href=../logical-operators/Command/ class=md-nav__link> Command </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataWritingCommand/ class=md-nav__link> DataWritingCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/LeafNode/ class=md-nav__link> LeafNode </a> </li> <li class=md-nav__item> <a href=../logical-operators/MultiInstanceRelation/ class=md-nav__link> MultiInstanceRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/NamedRelation/ class=md-nav__link> NamedRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/OrderPreservingUnaryNode/ class=md-nav__link> OrderPreservingUnaryNode </a> </li> <li class=md-nav__item> <a href=../logical-operators/ParsedStatement/ class=md-nav__link> ParsedStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/ class=md-nav__link> RepartitionOperation </a> </li> <li class=md-nav__item> <a href=../logical-operators/ResolvedTable/ class=md-nav__link> ResolvedTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/ResolvedView/ class=md-nav__link> ResolvedView </a> </li> <li class=md-nav__item> <a href=../logical-operators/RunnableCommand/ class=md-nav__link> RunnableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/SupportsSubquery/ class=md-nav__link> SupportsSubquery </a> </li> <li class=md-nav__item> <a href=../logical-operators/V2CreateTablePlan/ class=md-nav__link> V2CreateTablePlan </a> </li> <li class=md-nav__item> <a href=../logical-operators/V2WriteCommand/ class=md-nav__link> V2WriteCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/Aggregate/ class=md-nav__link> Aggregate </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterTableRecoverPartitionsCommand/ class=md-nav__link> AlterTableRecoverPartitionsCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AlterTableRecoverPartitionsStatement/ class=md-nav__link> AlterTableRecoverPartitionsStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeColumn/ class=md-nav__link> AnalyzeColumn </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeColumnCommand/ class=md-nav__link> AnalyzeColumnCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzePartitionCommand/ class=md-nav__link> AnalyzePartitionCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeTable/ class=md-nav__link> AnalyzeTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/AnalyzeTableCommand/ class=md-nav__link> AnalyzeTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/AppendData/ class=md-nav__link> AppendData </a> </li> <li class=md-nav__item> <a href=../logical-operators/ClearCacheCommand/ class=md-nav__link> ClearCacheCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CollectMetrics/ class=md-nav__link> CollectMetrics </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateDataSourceTableAsSelectCommand/ class=md-nav__link> CreateDataSourceTableAsSelectCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateDataSourceTableCommand/ class=md-nav__link> CreateDataSourceTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTable/ class=md-nav__link> CreateTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTableAsSelectStatement/ class=md-nav__link> CreateTableAsSelectStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTableCommand/ class=md-nav__link> CreateTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTableStatement/ class=md-nav__link> CreateTableStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateTempViewUsing/ class=md-nav__link> CreateTempViewUsing </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateV2Table/ class=md-nav__link> CreateV2Table </a> </li> <li class=md-nav__item> <a href=../logical-operators/CreateViewCommand/ class=md-nav__link> CreateViewCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/CTERelationDef/ class=md-nav__link> CTERelationDef </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataSourceV2Relation/ class=md-nav__link> DataSourceV2Relation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DataSourceV2ScanRelation/ class=md-nav__link> DataSourceV2ScanRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DeleteFromTable/ class=md-nav__link> DeleteFromTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeColumnCommand/ class=md-nav__link> DescribeColumnCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeRelation/ class=md-nav__link> DescribeRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/DescribeTableCommand/ class=md-nav__link> DescribeTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/DeserializeToObject/ class=md-nav__link> DeserializeToObject </a> </li> <li class=md-nav__item> <a href=../logical-operators/Except/ class=md-nav__link> Except </a> </li> <li class=md-nav__item> <a href=../logical-operators/Expand/ class=md-nav__link> Expand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ExplainCommand/ class=md-nav__link> ExplainCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ExternalRDD/ class=md-nav__link> ExternalRDD </a> </li> <li class=md-nav__item> <a href=../logical-operators/Filter/ class=md-nav__link> Filter </a> </li> <li class=md-nav__item> <a href=../logical-operators/Generate/ class=md-nav__link> Generate </a> </li> <li class=md-nav__item> <a href=../logical-operators/GlobalLimit/ class=md-nav__link> GlobalLimit </a> </li> <li class=md-nav__item> <a href=../logical-operators/GroupingSets/ class=md-nav__link> GroupingSets </a> </li> <li class=md-nav__item> <a href=../logical-operators/InMemoryRelation/ class=md-nav__link> InMemoryRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoDataSourceCommand/ class=md-nav__link> InsertIntoDataSourceCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoDir/ class=md-nav__link> InsertIntoDir </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoHadoopFsRelationCommand/ class=md-nav__link> InsertIntoHadoopFsRelationCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoStatement/ class=md-nav__link> InsertIntoStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/InsertIntoTable/ class=md-nav__link> InsertIntoTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/Join/ class=md-nav__link> Join </a> </li> <li class=md-nav__item> <a href=../logical-operators/LocalRelation/ class=md-nav__link> LocalRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalRDD/ class=md-nav__link> LogicalRDD </a> </li> <li class=md-nav__item> <a href=../logical-operators/LogicalRelation/ class=md-nav__link> LogicalRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/MergeIntoTable/ class=md-nav__link> MergeIntoTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/OneRowRelation/ class=md-nav__link> OneRowRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/OverwriteByExpression/ class=md-nav__link> OverwriteByExpression </a> </li> <li class=md-nav__item> <a href=../logical-operators/OverwritePartitionsDynamic/ class=md-nav__link> OverwritePartitionsDynamic </a> </li> <li class=md-nav__item> <a href=../logical-operators/Pivot/ class=md-nav__link> Pivot </a> </li> <li class=md-nav__item> <a href=../logical-operators/Project/ class=md-nav__link> Project </a> </li> <li class=md-nav__item> <a href=../logical-operators/Range/ class=md-nav__link> Range </a> </li> <li class=md-nav__item> <a href=../logical-operators/RebalancePartitions/ class=md-nav__link> RebalancePartitions </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepairTableStatement/ class=md-nav__link> RepairTableStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/#Repartition class=md-nav__link> Repartition </a> </li> <li class=md-nav__item> <a href=../logical-operators/RepartitionOperation/#RepartitionByExpression class=md-nav__link> RepartitionByExpression </a> </li> <li class=md-nav__item> <a href=../logical-operators/ResolvedHint/ class=md-nav__link> ResolvedHint </a> </li> <li class=md-nav__item> <a href=../logical-operators/SaveIntoDataSourceCommand/ class=md-nav__link> SaveIntoDataSourceCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/SetCatalogAndNamespace/ class=md-nav__link> SetCatalogAndNamespace </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCreateTableCommand/ class=md-nav__link> ShowCreateTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCurrentNamespace/ class=md-nav__link> ShowCurrentNamespace </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowCurrentNamespaceStatement/ class=md-nav__link> ShowCurrentNamespaceStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowTableProperties/ class=md-nav__link> ShowTableProperties </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowTablePropertiesCommand/ class=md-nav__link> ShowTablePropertiesCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/ShowTables/ class=md-nav__link> ShowTables </a> </li> <li class=md-nav__item> <a href=../logical-operators/Sort/ class=md-nav__link> Sort </a> </li> <li class=md-nav__item> <a href=../logical-operators/Subquery/ class=md-nav__link> Subquery </a> </li> <li class=md-nav__item> <a href=../logical-operators/SubqueryAlias/ class=md-nav__link> SubqueryAlias </a> </li> <li class=md-nav__item> <a href=../logical-operators/TruncateTableCommand/ class=md-nav__link> TruncateTableCommand </a> </li> <li class=md-nav__item> <a href=../logical-operators/TypedFilter/ class=md-nav__link> TypedFilter </a> </li> <li class=md-nav__item> <a href=../logical-operators/Union/ class=md-nav__link> Union </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedCatalogRelation/ class=md-nav__link> UnresolvedCatalogRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedHaving/ class=md-nav__link> UnresolvedHaving </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedHint/ class=md-nav__link> UnresolvedHint </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedInlineTable/ class=md-nav__link> UnresolvedInlineTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedRelation/ class=md-nav__link> UnresolvedRelation </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedTable/ class=md-nav__link> UnresolvedTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedTableOrView/ class=md-nav__link> UnresolvedTableOrView </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedTableValuedFunction/ class=md-nav__link> UnresolvedTableValuedFunction </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedV2Relation/ class=md-nav__link> UnresolvedV2Relation </a> </li> <li class=md-nav__item> <a href=../logical-operators/UnresolvedWith/ class=md-nav__link> UnresolvedWith </a> </li> <li class=md-nav__item> <a href=../logical-operators/UpdateTable/ class=md-nav__link> UpdateTable </a> </li> <li class=md-nav__item> <a href=../logical-operators/UseStatement/ class=md-nav__link> UseStatement </a> </li> <li class=md-nav__item> <a href=../logical-operators/Window/ class=md-nav__link> Window </a> </li> <li class=md-nav__item> <a href=../logical-operators/WithCTE/ class=md-nav__link> WithCTE </a> </li> <li class=md-nav__item> <a href=../logical-operators/WithWindowDefinition/ class=md-nav__link> WithWindowDefinition </a> </li> <li class=md-nav__item> <a href=../logical-operators/WriteToDataSourceV2/ class=md-nav__link> WriteToDataSourceV2 </a> </li> <li class=md-nav__item> <a href=../logical-operators/View/ class=md-nav__link> View </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13 type=checkbox id=__nav_2_13> <label class=md-nav__link for=__nav_2_13> SparkSession Registries <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SparkSession Registries" data-md-level=2> <label class=md-nav__title for=__nav_2_13> <span class="md-nav__icon md-icon"></span> SparkSession Registries </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13_1 type=checkbox id=__nav_2_13_1> <label class=md-nav__link for=__nav_2_13_1> Catalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Catalog data-md-level=3> <label class=md-nav__title for=__nav_2_13_1> <span class="md-nav__icon md-icon"></span> Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Catalog/ class=md-nav__link> Catalog </a> </li> <li class=md-nav__item> <a href=../CatalogImpl/ class=md-nav__link> CatalogImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../ExperimentalMethods/ class=md-nav__link> ExperimentalMethods </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13_3 type=checkbox id=__nav_2_13_3> <label class=md-nav__link for=__nav_2_13_3> ExternalCatalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ExternalCatalog data-md-level=3> <label class=md-nav__title for=__nav_2_13_3> <span class="md-nav__icon md-icon"></span> ExternalCatalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ExternalCatalog/ class=md-nav__link> ExternalCatalog </a> </li> <li class=md-nav__item> <a href=../InMemoryCatalog/ class=md-nav__link> InMemoryCatalog </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../FunctionRegistry/ class=md-nav__link> FunctionRegistry </a> </li> <li class=md-nav__item> <a href=../GlobalTempViewManager/ class=md-nav__link> GlobalTempViewManager </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13_6 type=checkbox id=__nav_2_13_6> <label class=md-nav__link for=__nav_2_13_6> SessionCatalog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SessionCatalog data-md-level=3> <label class=md-nav__title for=__nav_2_13_6> <span class="md-nav__icon md-icon"></span> SessionCatalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SessionCatalog/ class=md-nav__link> SessionCatalog </a> </li> <li class=md-nav__item> <a href=../CatalogTable/ class=md-nav__link> CatalogTable </a> </li> <li class=md-nav__item> <a href=../CatalogStorageFormat/ class=md-nav__link> CatalogStorageFormat </a> </li> <li class=md-nav__item> <a href=../CatalogTablePartition/ class=md-nav__link> CatalogTablePartition </a> </li> <li class=md-nav__item> <a href=../BucketSpec/ class=md-nav__link> BucketSpec </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../V2SessionCatalog/ class=md-nav__link> V2SessionCatalog </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13_8 type=checkbox id=__nav_2_13_8> <label class=md-nav__link for=__nav_2_13_8> SessionState <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SessionState data-md-level=3> <label class=md-nav__title for=__nav_2_13_8> <span class="md-nav__icon md-icon"></span> SessionState </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SessionState/ class=md-nav__link> SessionState </a> </li> <li class=md-nav__item> <a href=../BaseSessionStateBuilder/ class=md-nav__link> BaseSessionStateBuilder </a> </li> <li class=md-nav__item> <a href=../SessionStateBuilder/ class=md-nav__link> SessionStateBuilder </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_13_9 type=checkbox id=__nav_2_13_9> <label class=md-nav__link for=__nav_2_13_9> CacheManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CacheManager data-md-level=3> <label class=md-nav__title for=__nav_2_13_9> <span class="md-nav__icon md-icon"></span> CacheManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CacheManager/ class=md-nav__link> CacheManager </a> </li> <li class=md-nav__item> <a href=../CachedRDDBuilder/ class=md-nav__link> CachedRDDBuilder </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../RuntimeConfig/ class=md-nav__link> RuntimeConfig </a> </li> <li class=md-nav__item> <a href=../UDFRegistration/ class=md-nav__link> UDFRegistration </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14 type=checkbox id=__nav_2_14> <label class=md-nav__link for=__nav_2_14> Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Sources" data-md-level=2> <label class=md-nav__title for=__nav_2_14> <span class="md-nav__icon md-icon"></span> Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_1 type=checkbox id=__nav_2_14_1> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/noop/ >Noop</a> <label for=__nav_2_14_1> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Noop data-md-level=3> <label class=md-nav__title for=__nav_2_14_1> <span class="md-nav__icon md-icon"></span> Noop </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/noop/NoopDataSource/ class=md-nav__link> NoopDataSource </a> </li> <li class=md-nav__item> <a href=../datasources/noop/NoopTable/ class=md-nav__link> NoopTable </a> </li> <li class=md-nav__item> <a href=../datasources/noop/NoopWriteBuilder/ class=md-nav__link> NoopWriteBuilder </a> </li> <li class=md-nav__item> <a href=../datasources/noop/NoopBatchWrite/ class=md-nav__link> NoopBatchWrite </a> </li> <li class=md-nav__item> <a href=../datasources/noop/NoopStreamingWrite/ class=md-nav__link> NoopStreamingWrite </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_2 type=checkbox id=__nav_2_14_2> <label class=md-nav__link for=__nav_2_14_2> Files <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Files data-md-level=3> <label class=md-nav__title for=__nav_2_14_2> <span class="md-nav__icon md-icon"></span> Files </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../FileScan/ class=md-nav__link> FileScan </a> </li> <li class=md-nav__item> <a href=../FileScanBuilder/ class=md-nav__link> FileScanBuilder </a> </li> <li class=md-nav__item> <a href=../FileDataSourceV2/ class=md-nav__link> FileDataSourceV2 </a> </li> <li class=md-nav__item> <a href=../HadoopFsRelation/ class=md-nav__link> HadoopFsRelation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_2_5 type=checkbox id=__nav_2_14_2_5> <label class=md-nav__link for=__nav_2_14_2_5> FileIndex <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=FileIndex data-md-level=4> <label class=md-nav__title for=__nav_2_14_2_5> <span class="md-nav__icon md-icon"></span> FileIndex </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../FileIndex/ class=md-nav__link> FileIndex </a> </li> <li class=md-nav__item> <a href=../CatalogFileIndex/ class=md-nav__link> CatalogFileIndex </a> </li> <li class=md-nav__item> <a href=../InMemoryFileIndex/ class=md-nav__link> InMemoryFileIndex </a> </li> <li class=md-nav__item> <a href=../PartitioningAwareFileIndex/ class=md-nav__link> PartitioningAwareFileIndex </a> </li> <li class=md-nav__item> <a href=../PrunedInMemoryFileIndex/ class=md-nav__link> PrunedInMemoryFileIndex </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../FileFormatWriter/ class=md-nav__link> FileFormatWriter </a> </li> <li class=md-nav__item> <a href=../FileWriteBuilder/ class=md-nav__link> FileWriteBuilder </a> </li> <li class=md-nav__item> <a href=../SQLHadoopMapReduceCommitProtocol/ class=md-nav__link> SQLHadoopMapReduceCommitProtocol </a> </li> <li class=md-nav__item> <a href=../PartitionedFile/ class=md-nav__link> PartitionedFile </a> </li> <li class=md-nav__item> <a href=../RecordReaderIterator/ class=md-nav__link> RecordReaderIterator </a> </li> <li class=md-nav__item> <a href=../datasources/FileFormat/ class=md-nav__link> FileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/TextBasedFileFormat/ class=md-nav__link> TextBasedFileFormat </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_3 type=checkbox id=__nav_2_14_3> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/kafka/ >Kafka</a> <label for=__nav_2_14_3> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Kafka data-md-level=3> <label class=md-nav__title for=__nav_2_14_3> <span class="md-nav__icon md-icon"></span> Kafka </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceProvider/ class=md-nav__link> KafkaSourceProvider </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaTable/ class=md-nav__link> KafkaTable </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/options/ class=md-nav__link> Options </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaScan/ class=md-nav__link> KafkaScan </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaBatch/ class=md-nav__link> KafkaBatch </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaBatchWrite/ class=md-nav__link> KafkaBatchWrite </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaRelation/ class=md-nav__link> KafkaRelation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_3_10 type=checkbox id=__nav_2_14_3_10> <label class=md-nav__link for=__nav_2_14_3_10> KafkaSourceRDD <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaSourceRDD data-md-level=4> <label class=md-nav__title for=__nav_2_14_3_10> <span class="md-nav__icon md-icon"></span> KafkaSourceRDD </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceRDD/ class=md-nav__link> KafkaSourceRDD </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceRDDPartition/ class=md-nav__link> KafkaSourceRDDPartition </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../datasources/kafka/ConsumerStrategy/ class=md-nav__link> ConsumerStrategy </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetReader/ class=md-nav__link> KafkaOffsetReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetRangeLimit/ class=md-nav__link> KafkaOffsetRangeLimit </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaDataConsumer/ class=md-nav__link> KafkaDataConsumer </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/InternalKafkaConsumer/ class=md-nav__link> InternalKafkaConsumer </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaWriter/ class=md-nav__link> KafkaWriter </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaWriteTask/ class=md-nav__link> KafkaWriteTask </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/JsonUtils/ class=md-nav__link> JsonUtils </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/InternalKafkaProducerPool/ class=md-nav__link> InternalKafkaProducerPool </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_4 type=checkbox id=__nav_2_14_4> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/avro/ >Avro</a> <label for=__nav_2_14_4> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Avro data-md-level=3> <label class=md-nav__title for=__nav_2_14_4> <span class="md-nav__icon md-icon"></span> Avro </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/avro/AvroOptions/ class=md-nav__link> AvroOptions </a> </li> <li class=md-nav__item> <a href=../datasources/avro/AvroFileFormat/ class=md-nav__link> AvroFileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/avro/CatalystDataToAvro/ class=md-nav__link> CatalystDataToAvro </a> </li> <li class=md-nav__item> <a href=../datasources/avro/AvroDataToCatalyst/ class=md-nav__link> AvroDataToCatalyst </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_5 type=checkbox id=__nav_2_14_5> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/jdbc/ >JDBC</a> <label for=__nav_2_14_5> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=JDBC data-md-level=3> <label class=md-nav__title for=__nav_2_14_5> <span class="md-nav__icon md-icon"></span> JDBC </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/jdbc/JDBCOptions/ class=md-nav__link> JDBCOptions </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JdbcRelationProvider/ class=md-nav__link> JdbcRelationProvider </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JDBCRelation/ class=md-nav__link> JDBCRelation </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JDBCScan/ class=md-nav__link> JDBCScan </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JDBCRDD/ class=md-nav__link> JDBCRDD </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JdbcDialect/ class=md-nav__link> JdbcDialect </a> </li> <li class=md-nav__item> <a href=../datasources/jdbc/JdbcUtils/ class=md-nav__link> JdbcUtils </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_6 type=checkbox id=__nav_2_14_6> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/console/ >Console</a> <label for=__nav_2_14_6> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Console data-md-level=3> <label class=md-nav__title for=__nav_2_14_6> <span class="md-nav__icon md-icon"></span> Console </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/console/ConsoleSinkProvider/ class=md-nav__link> ConsoleSinkProvider </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_7 type=checkbox id=__nav_2_14_7> <label class=md-nav__link for=__nav_2_14_7> CSV <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CSV data-md-level=3> <label class=md-nav__title for=__nav_2_14_7> <span class="md-nav__icon md-icon"></span> CSV </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/csv/CSVFileFormat/ class=md-nav__link> CSVFileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/csv/CSVScanBuilder/ class=md-nav__link> CSVScanBuilder </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_8 type=checkbox id=__nav_2_14_8> <label class=md-nav__link for=__nav_2_14_8> JSON <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=JSON data-md-level=3> <label class=md-nav__title for=__nav_2_14_8> <span class="md-nav__icon md-icon"></span> JSON </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/json/JsonFileFormat/ class=md-nav__link> JsonFileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/json/JsonDataSource/ class=md-nav__link> JsonDataSource </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_9 type=checkbox id=__nav_2_14_9> <label class=md-nav__link for=__nav_2_14_9> ORC <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ORC data-md-level=3> <label class=md-nav__title for=__nav_2_14_9> <span class="md-nav__icon md-icon"></span> ORC </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/orc/OrcScanBuilder/ class=md-nav__link> OrcScanBuilder </a> </li> <li class=md-nav__item> <a href=../datasources/orc/OrcFileFormat/ class=md-nav__link> OrcFileFormat </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_10 type=checkbox id=__nav_2_14_10> <div class="md-nav__link md-nav__link--container "> <a href=../datasources/parquet/ >Parquet</a> <label for=__nav_2_14_10> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Parquet data-md-level=3> <label class=md-nav__title for=__nav_2_14_10> <span class="md-nav__icon md-icon"></span> Parquet </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/parquet/ParquetFileFormat/ class=md-nav__link> ParquetFileFormat </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/ParquetReadSupport/ class=md-nav__link> ParquetReadSupport </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/ParquetScanBuilder/ class=md-nav__link> ParquetScanBuilder </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/ParquetTable/ class=md-nav__link> ParquetTable </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/SpecificParquetRecordReaderBase/ class=md-nav__link> SpecificParquetRecordReaderBase </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/VectorizedColumnReader/ class=md-nav__link> VectorizedColumnReader </a> </li> <li class=md-nav__item> <a href=../datasources/parquet/VectorizedParquetRecordReader/ class=md-nav__link> VectorizedParquetRecordReader </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_14_11 type=checkbox id=__nav_2_14_11> <label class=md-nav__link for=__nav_2_14_11> Text <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Text data-md-level=3> <label class=md-nav__title for=__nav_2_14_11> <span class="md-nav__icon md-icon"></span> Text </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/text/TextFileFormat/ class=md-nav__link> TextFileFormat </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_15 type=checkbox id=__nav_2_15> <div class="md-nav__link md-nav__link--container "> <a href=../hive/ >Hive Integration</a> <label for=__nav_2_15> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Hive Integration" data-md-level=2> <label class=md-nav__title for=__nav_2_15> <span class="md-nav__icon md-icon"></span> Hive Integration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../hive/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../hive/spark-sql-hive-metastore/ class=md-nav__link> Hive Metastore </a> </li> <li class=md-nav__item> <a href=../hive/DataSinks/ class=md-nav__link> DataSinks </a> </li> <li class=md-nav__item> <a href=../hive/HiveFileFormat/ class=md-nav__link> HiveFileFormat </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_15_6 type=checkbox id=__nav_2_15_6> <label class=md-nav__link for=__nav_2_15_6> HiveClient <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=HiveClient data-md-level=3> <label class=md-nav__title for=__nav_2_15_6> <span class="md-nav__icon md-icon"></span> HiveClient </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../hive/HiveClient/ class=md-nav__link> HiveClient </a> </li> <li class=md-nav__item> <a href=../hive/HiveClientImpl/ class=md-nav__link> HiveClientImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../hive/HiveUtils/ class=md-nav__link> HiveUtils </a> </li> <li class=md-nav__item> <a href=../hive/IsolatedClientLoader/ class=md-nav__link> IsolatedClientLoader </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableRelation/ class=md-nav__link> HiveTableRelation </a> </li> <li class=md-nav__item> <a href=../hive/CreateHiveTableAsSelectCommand/ class=md-nav__link> CreateHiveTableAsSelectCommand </a> </li> <li class=md-nav__item> <a href=../hive/SaveAsHiveFile/ class=md-nav__link> SaveAsHiveFile </a> </li> <li class=md-nav__item> <a href=../hive/InsertIntoHiveDirCommand/ class=md-nav__link> InsertIntoHiveDirCommand </a> </li> <li class=md-nav__item> <a href=../hive/InsertIntoHiveTable/ class=md-nav__link> InsertIntoHiveTable </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableScans/ class=md-nav__link> HiveTableScans </a> </li> <li class=md-nav__item> <a href=../hive/HiveTableScanExec/ class=md-nav__link> HiveTableScanExec </a> </li> <li class=md-nav__item> <a href=../hive/TableReader/ class=md-nav__link> TableReader </a> </li> <li class=md-nav__item> <a href=../hive/HadoopTableReader/ class=md-nav__link> HadoopTableReader </a> </li> <li class=md-nav__item> <a href=../hive/HiveSessionStateBuilder/ class=md-nav__link> HiveSessionStateBuilder </a> </li> <li class=md-nav__item> <a href=../hive/HiveExternalCatalog/ class=md-nav__link> HiveExternalCatalog </a> </li> <li class=md-nav__item> <a href=../hive/HiveSessionCatalog/ class=md-nav__link> HiveSessionCatalog </a> </li> <li class=md-nav__item> <a href=../hive/HiveMetastoreCatalog/ class=md-nav__link> HiveMetastoreCatalog </a> </li> <li class=md-nav__item> <a href=../hive/RelationConversions/ class=md-nav__link> RelationConversions </a> </li> <li class=md-nav__item> <a href=../hive/ResolveHiveSerdeTable/ class=md-nav__link> ResolveHiveSerdeTable </a> </li> <li class=md-nav__item> <a href=../hive/DetermineTableStats/ class=md-nav__link> DetermineTableStats </a> </li> <li class=md-nav__item> <a href=../hive/HiveAnalysis/ class=md-nav__link> HiveAnalysis </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_16 type=checkbox id=__nav_2_16> <div class="md-nav__link md-nav__link--container "> <a href=../catalyst/ >Catalyst</a> <label for=__nav_2_16> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Catalyst data-md-level=2> <label class=md-nav__title for=__nav_2_16> <span class="md-nav__icon md-icon"></span> Catalyst </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../catalyst/GenericStrategy/ class=md-nav__link> GenericStrategy </a> </li> <li class=md-nav__item> <a href=../catalyst/Optimizer/ class=md-nav__link> Optimizer </a> </li> <li class=md-nav__item> <a href=../catalyst/PlanChangeLogger/ class=md-nav__link> PlanChangeLogger </a> </li> <li class=md-nav__item> <a href=../catalyst/QueryPlan/ class=md-nav__link> QueryPlan </a> </li> <li class=md-nav__item> <a href=../catalyst/QueryPlanner/ class=md-nav__link> QueryPlanner </a> </li> <li class=md-nav__item> <a href=../catalyst/Rule/ class=md-nav__link> Rule </a> </li> <li class=md-nav__item> <a href=../catalyst/RuleExecutor/ class=md-nav__link> RuleExecutor </a> </li> <li class=md-nav__item> <a href=../catalyst/TreeNode/ class=md-nav__link> TreeNode </a> </li> <li class=md-nav__item> <a href=../catalyst/TreePattern/ class=md-nav__link> TreePattern </a> </li> <li class=md-nav__item> <a href=../catalyst/TreePatternBits/ class=md-nav__link> TreePatternBits </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_17 type=checkbox id=__nav_2_17> <label class=md-nav__link for=__nav_2_17> Expressions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Expressions data-md-level=2> <label class=md-nav__title for=__nav_2_17> <span class="md-nav__icon md-icon"></span> Expressions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../expressions/AggregateExpression/ class=md-nav__link> AggregateExpression </a> </li> <li class=md-nav__item> <a href=../expressions/AggregateFunction/ class=md-nav__link> AggregateFunction </a> </li> <li class=md-nav__item> <a href=../expressions/AggregateWindowFunction/ class=md-nav__link> AggregateWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/Aggregator/ class=md-nav__link> Aggregator </a> </li> <li class=md-nav__item> <a href=../expressions/AttributeReference/ class=md-nav__link> AttributeReference </a> </li> <li class=md-nav__item> <a href=../expressions/Alias/ class=md-nav__link> Alias </a> </li> <li class=md-nav__item> <a href=../expressions/Attribute/ class=md-nav__link> Attribute </a> </li> <li class=md-nav__item> <a href=../expressions/BinaryComparison/ class=md-nav__link> BinaryComparison </a> </li> <li class=md-nav__item> <a href=../expressions/BinaryOperator/ class=md-nav__link> BinaryOperator </a> </li> <li class=md-nav__item> <a href=../expressions/BoundReference/ class=md-nav__link> BoundReference </a> </li> <li class=md-nav__item> <a href=../expressions/CallMethodViaReflection/ class=md-nav__link> CallMethodViaReflection </a> </li> <li class=md-nav__item> <a href=../expressions/Coalesce/ class=md-nav__link> Coalesce </a> </li> <li class=md-nav__item> <a href=../expressions/CodeGeneratorWithInterpretedFallback/ class=md-nav__link> CodeGeneratorWithInterpretedFallback </a> </li> <li class=md-nav__item> <a href=../expressions/CodegenFallback/ class=md-nav__link> CodegenFallback </a> </li> <li class=md-nav__item> <a href=../expressions/CollectionGenerator/ class=md-nav__link> CollectionGenerator </a> </li> <li class=md-nav__item> <a href=../expressions/CreateArray/ class=md-nav__link> CreateArray </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStruct/ class=md-nav__link> CreateNamedStruct </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStructLike/ class=md-nav__link> CreateNamedStructLike </a> </li> <li class=md-nav__item> <a href=../expressions/CreateNamedStructUnsafe/ class=md-nav__link> CreateNamedStructUnsafe </a> </li> <li class=md-nav__item> <a href=../expressions/CumeDist/ class=md-nav__link> CumeDist </a> </li> <li class=md-nav__item> <a href=../expressions/DeclarativeAggregate/ class=md-nav__link> DeclarativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/DecodeUsingSerializer/ class=md-nav__link> DecodeUsingSerializer </a> </li> <li class=md-nav__item> <a href=../expressions/DynamicPruningExpression/ class=md-nav__link> DynamicPruningExpression </a> </li> <li class=md-nav__item> <a href=../expressions/DynamicPruningSubquery/ class=md-nav__link> DynamicPruningSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/EncodeUsingSerializer/ class=md-nav__link> EncodeUsingSerializer </a> </li> <li class=md-nav__item> <a href=../expressions/EqualNullSafe/ class=md-nav__link> EqualNullSafe </a> </li> <li class=md-nav__item> <a href=../expressions/EqualTo/ class=md-nav__link> EqualTo </a> </li> <li class=md-nav__item> <a href=../expressions/ExecSubqueryExpression/ class=md-nav__link> ExecSubqueryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Exists/ class=md-nav__link> Exists </a> </li> <li class=md-nav__item> <a href=../expressions/ExpectsInputTypes/ class=md-nav__link> ExpectsInputTypes </a> </li> <li class=md-nav__item> <a href=../expressions/ExplodeBase/ class=md-nav__link> ExplodeBase </a> </li> <li class=md-nav__item> <a href=../expressions/Expression/ class=md-nav__link> Expression </a> </li> <li class=md-nav__item> <a href=../expressions/First/ class=md-nav__link> First </a> </li> <li class=md-nav__item> <a href=../expressions/Generator/ class=md-nav__link> Generator </a> </li> <li class=md-nav__item> <a href=../expressions/GetArrayStructFields/ class=md-nav__link> GetArrayStructFields </a> </li> <li class=md-nav__item> <a href=../expressions/GetArrayItem/ class=md-nav__link> GetArrayItem </a> </li> <li class=md-nav__item> <a href=../expressions/GetMapValue/ class=md-nav__link> GetMapValue </a> </li> <li class=md-nav__item> <a href=../expressions/GetStructField/ class=md-nav__link> GetStructField </a> </li> <li class=md-nav__item> <a href=../expressions/HashExpression/ class=md-nav__link> HashExpression </a> </li> <li class=md-nav__item> <a href=../expressions/HashPartitioning/ class=md-nav__link> HashPartitioning </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_17_41 type=checkbox id=__nav_2_17_41> <label class=md-nav__link for=__nav_2_17_41> Higher-Order Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Higher-Order Functions" data-md-level=3> <label class=md-nav__title for=__nav_2_17_41> <span class="md-nav__icon md-icon"></span> Higher-Order Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../expressions/HigherOrderFunction/ class=md-nav__link> HigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/ArrayBasedSimpleHigherOrderFunction/ class=md-nav__link> ArrayBasedSimpleHigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/MapBasedSimpleHigherOrderFunction/ class=md-nav__link> MapBasedSimpleHigherOrderFunction </a> </li> <li class=md-nav__item> <a href=../expressions/SimpleHigherOrderFunction/ class=md-nav__link> SimpleHigherOrderFunction </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../expressions/ImperativeAggregate/ class=md-nav__link> ImperativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/In/ class=md-nav__link> In </a> </li> <li class=md-nav__item> <a href=../expressions/Inline/ class=md-nav__link> Inline </a> </li> <li class=md-nav__item> <a href=../expressions/InSet/ class=md-nav__link> InSet </a> </li> <li class=md-nav__item> <a href=../expressions/InSubquery/ class=md-nav__link> InSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/InSubqueryExec/ class=md-nav__link> InSubqueryExec </a> </li> <li class=md-nav__item> <a href=../expressions/InterpretedProjection/ class=md-nav__link> InterpretedProjection </a> </li> <li class=md-nav__item> <a href=../expressions/JsonToStructs/ class=md-nav__link> JsonToStructs </a> </li> <li class=md-nav__item> <a href=../expressions/JsonTuple/ class=md-nav__link> JsonTuple </a> </li> <li class=md-nav__item> <a href=../expressions/ListQuery/ class=md-nav__link> ListQuery </a> </li> <li class=md-nav__item> <a href=../expressions/Literal/ class=md-nav__link> Literal </a> </li> <li class=md-nav__item> <a href=../expressions/MonotonicallyIncreasingID/ class=md-nav__link> MonotonicallyIncreasingID </a> </li> <li class=md-nav__item> <a href=../expressions/Murmur3Hash/ class=md-nav__link> Murmur3Hash </a> </li> <li class=md-nav__item> <a href=../expressions/MutableProjection/ class=md-nav__link> MutableProjection </a> </li> <li class=md-nav__item> <a href=../expressions/NamedExpression/ class=md-nav__link> NamedExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Nondeterministic/ class=md-nav__link> Nondeterministic </a> </li> <li class=md-nav__item> <a href=../expressions/NonSQLExpression/ class=md-nav__link> NonSQLExpression </a> </li> <li class=md-nav__item> <a href=../expressions/OffsetWindowFunction/ class=md-nav__link> OffsetWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/ParseToDate/ class=md-nav__link> ParseToDate </a> </li> <li class=md-nav__item> <a href=../expressions/ParseToTimestamp/ class=md-nav__link> ParseToTimestamp </a> </li> <li class=md-nav__item> <a href=../expressions/PlanExpression/ class=md-nav__link> PlanExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Predicate/ class=md-nav__link> Predicate </a> </li> <li class=md-nav__item> <a href=../expressions/PrettyAttribute/ class=md-nav__link> PrettyAttribute </a> </li> <li class=md-nav__item> <a href=../expressions/Projection/ class=md-nav__link> Projection </a> </li> <li class=md-nav__item> <a href=../expressions/RangePartitioning/ class=md-nav__link> RangePartitioning </a> </li> <li class=md-nav__item> <a href=../expressions/RankLike/ class=md-nav__link> RankLike </a> </li> <li class=md-nav__item> <a href=../expressions/ResolvedStar/ class=md-nav__link> ResolvedStar </a> </li> <li class=md-nav__item> <a href=../expressions/RowNumberLike/ class=md-nav__link> RowNumberLike </a> </li> <li class=md-nav__item> <a href=../expressions/RowOrdering/ class=md-nav__link> RowOrdering </a> </li> <li class=md-nav__item> <a href=../expressions/RuntimeReplaceable/ class=md-nav__link> RuntimeReplaceable </a> </li> <li class=md-nav__item> <a href=../expressions/SafeProjection/ class=md-nav__link> SafeProjection </a> </li> <li class=md-nav__item> <a href=../expressions/ScalarSubquery/ class=md-nav__link> ScalarSubquery </a> </li> <li class=md-nav__item> <a href=../expressions/spark-sql-Expression-ExecSubqueryExpression-ScalarSubquery/ class=md-nav__link> ScalarSubquery (ExecSubqueryExpression) Expression </a> </li> <li class=md-nav__item> <a href=../expressions/ScalaUDF/ class=md-nav__link> ScalaUDF </a> </li> <li class=md-nav__item> <a href=../expressions/ScalaUDAF/ class=md-nav__link> ScalaUDAF </a> </li> <li class=md-nav__item> <a href=../expressions/SizeBasedWindowFunction/ class=md-nav__link> SizeBasedWindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/SortOrder/ class=md-nav__link> SortOrder </a> </li> <li class=md-nav__item> <a href=../expressions/Stack/ class=md-nav__link> Stack </a> </li> <li class=md-nav__item> <a href=../expressions/Star/ class=md-nav__link> Star </a> </li> <li class=md-nav__item> <a href=../expressions/StaticInvoke/ class=md-nav__link> StaticInvoke </a> </li> <li class=md-nav__item> <a href=../expressions/SubqueryExpression/ class=md-nav__link> SubqueryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/TimeWindow/ class=md-nav__link> TimeWindow </a> </li> <li class=md-nav__item> <a href=../expressions/TypedImperativeAggregate/ class=md-nav__link> TypedImperativeAggregate </a> </li> <li class=md-nav__item> <a href=../expressions/UnaryExpression/ class=md-nav__link> UnaryExpression </a> </li> <li class=md-nav__item> <a href=../expressions/Unevaluable/ class=md-nav__link> Unevaluable </a> </li> <li class=md-nav__item> <a href=../expressions/UnixTimestamp/ class=md-nav__link> UnixTimestamp </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedAttribute/ class=md-nav__link> UnresolvedAttribute </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedFunction/ class=md-nav__link> UnresolvedFunction </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedGenerator/ class=md-nav__link> UnresolvedGenerator </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedOrdinal/ class=md-nav__link> UnresolvedOrdinal </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedRegex/ class=md-nav__link> UnresolvedRegex </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedStar/ class=md-nav__link> UnresolvedStar </a> </li> <li class=md-nav__item> <a href=../expressions/UnresolvedWindowExpression/ class=md-nav__link> UnresolvedWindowExpression </a> </li> <li class=md-nav__item> <a href=../expressions/UnsafeProjection/ class=md-nav__link> UnsafeProjection </a> </li> <li class=md-nav__item> <a href=../expressions/UserDefinedExpression/ class=md-nav__link> UserDefinedExpression </a> </li> <li class=md-nav__item> <a href=../expressions/WindowExpression/ class=md-nav__link> WindowExpression </a> </li> <li class=md-nav__item> <a href=../expressions/WindowFunction/ class=md-nav__link> WindowFunction </a> </li> <li class=md-nav__item> <a href=../expressions/WindowSpecDefinition/ class=md-nav__link> WindowSpecDefinition </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_18 type=checkbox id=__nav_2_18> <div class="md-nav__link md-nav__link--container "> <a href=../physical-operators/ >Physical Operators</a> <label for=__nav_2_18> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Physical Operators" data-md-level=2> <label class=md-nav__title for=__nav_2_18> <span class="md-nav__icon md-icon"></span> Physical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/AliasAwareOutputExpression/ class=md-nav__link> AliasAwareOutputExpression </a> </li> <li class=md-nav__item> <a href=../physical-operators/AliasAwareOutputOrdering/ class=md-nav__link> AliasAwareOutputOrdering </a> </li> <li class=md-nav__item> <a href=../physical-operators/AliasAwareOutputPartitioning/ class=md-nav__link> AliasAwareOutputPartitioning </a> </li> <li class=md-nav__item> <a href=../physical-operators/AlterTableExec/ class=md-nav__link> AlterTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AppendDataExec/ class=md-nav__link> AppendDataExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AppendDataExecV1/ class=md-nav__link> AppendDataExecV1 </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicTableWriteExec/ class=md-nav__link> AtomicTableWriteExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicCreateTableAsSelectExec/ class=md-nav__link> AtomicCreateTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicReplaceTableAsSelectExec/ class=md-nav__link> AtomicReplaceTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AtomicReplaceTableExec/ class=md-nav__link> AtomicReplaceTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseAggregateExec/ class=md-nav__link> BaseAggregateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseJoinExec/ class=md-nav__link> BaseJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseLimitExec/ class=md-nav__link> BaseLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BaseSubqueryExec/ class=md-nav__link> BaseSubqueryExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BatchWriteHelper/ class=md-nav__link> BatchWriteHelper </a> </li> <li class=md-nav__item> <a href=../physical-operators/BatchScanExec/ class=md-nav__link> BatchScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BinaryExecNode/ class=md-nav__link> BinaryExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/BlockingOperatorWithCodegen/ class=md-nav__link> BlockingOperatorWithCodegen </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastExchangeExec/ class=md-nav__link> BroadcastExchangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastExchangeLike/ class=md-nav__link> BroadcastExchangeLike </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastHashJoinExec/ class=md-nav__link> BroadcastHashJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastNestedLoopJoinExec/ class=md-nav__link> BroadcastNestedLoopJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CoalesceExec/ class=md-nav__link> CoalesceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CollectLimitExec/ class=md-nav__link> CollectLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CollectMetricsExec/ class=md-nav__link> CollectMetricsExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ColumnarToRowExec/ class=md-nav__link> ColumnarToRowExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CreateTableAsSelectExec/ class=md-nav__link> CreateTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CreateTableExec/ class=md-nav__link> CreateTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CustomShuffleReaderExec/ class=md-nav__link> CustomShuffleReaderExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/CodegenSupport/ class=md-nav__link> CodegenSupport </a> </li> <li class=md-nav__item> <a href=../physical-operators/ColumnarBatchScan/ class=md-nav__link> ColumnarBatchScan </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceScanExec/ class=md-nav__link> DataSourceScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceV2ScanExec/ class=md-nav__link> DataSourceV2ScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataSourceV2ScanExecBase/ class=md-nav__link> DataSourceV2ScanExecBase </a> </li> <li class=md-nav__item> <a href=../physical-operators/DataWritingCommandExec/ class=md-nav__link> DataWritingCommandExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DebugExec/ class=md-nav__link> DebugExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DeleteFromTableExec/ class=md-nav__link> DeleteFromTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DescribeTableExec/ class=md-nav__link> DescribeTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DeserializeToObjectExec/ class=md-nav__link> DeserializeToObjectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DropNamespaceExec/ class=md-nav__link> DropNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/DropTableExec/ class=md-nav__link> DropTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/EvalPythonExec/ class=md-nav__link> EvalPythonExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/Exchange/ class=md-nav__link> Exchange </a> </li> <li class=md-nav__item> <a href=../physical-operators/ExecutedCommandExec/ class=md-nav__link> ExecutedCommandExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ExternalRDDScanExec/ class=md-nav__link> ExternalRDDScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashedRelation/ class=md-nav__link> HashedRelation </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashJoin/ class=md-nav__link> HashJoin </a> </li> <li class=md-nav__item> <a href=../physical-operators/LeafExecNode/ class=md-nav__link> LeafExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/LimitExec/ class=md-nav__link> LimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/LongHashedRelation/ class=md-nav__link> LongHashedRelation </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectConsumerExec/ class=md-nav__link> ObjectConsumerExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReusedSubqueryExec/ class=md-nav__link> ReusedSubqueryExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffledJoin/ class=md-nav__link> ShuffledJoin </a> </li> <li class=md-nav__item> <a href=../physical-operators/SparkPlan/ class=md-nav__link> SparkPlan </a> </li> <li class=md-nav__item> <a href=../physical-operators/SQLMetric/ class=md-nav__link> SQLMetric </a> </li> <li class=md-nav__item> <a href=../physical-operators/SQLShuffleWriteMetricsReporter/ class=md-nav__link> SQLShuffleWriteMetricsReporter </a> </li> <li class=md-nav__item> <a href=../physical-operators/SubqueryAdaptiveBroadcastExec/ class=md-nav__link> SubqueryAdaptiveBroadcastExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SupportsV1Write/ class=md-nav__link> SupportsV1Write </a> </li> <li class=md-nav__item> <a href=../physical-operators/TableWriteExecHelper/ class=md-nav__link> TableWriteExecHelper </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnaryExecNode/ class=md-nav__link> UnaryExecNode </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnsafeHashedRelation/ class=md-nav__link> UnsafeHashedRelation </a> </li> <li class=md-nav__item> <a href=../physical-operators/V2CommandExec/ class=md-nav__link> V2CommandExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/V2TableWriteExec/ class=md-nav__link> V2TableWriteExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/V1FallbackWriters/ class=md-nav__link> V1FallbackWriters </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_18_66 type=checkbox id=__nav_2_18_66> <label class=md-nav__link for=__nav_2_18_66> Distribution and Partitioning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Distribution and Partitioning" data-md-level=3> <label class=md-nav__title for=__nav_2_18_66> <span class="md-nav__icon md-icon"></span> Distribution and Partitioning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/Distribution/ class=md-nav__link> Distribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/Partitioning/ class=md-nav__link> Partitioning </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_18_66_3 type=checkbox id=__nav_2_18_66_3> <label class=md-nav__link for=__nav_2_18_66_3> Distribution Specifications <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Distribution Specifications" data-md-level=4> <label class=md-nav__title for=__nav_2_18_66_3> <span class="md-nav__icon md-icon"></span> Distribution Specifications </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/AllTuples/ class=md-nav__link> AllTuples </a> </li> <li class=md-nav__item> <a href=../physical-operators/BroadcastDistribution/ class=md-nav__link> BroadcastDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/ClusteredDistribution/ class=md-nav__link> ClusteredDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashClusteredDistribution/ class=md-nav__link> HashClusteredDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/OrderedDistribution/ class=md-nav__link> OrderedDistribution </a> </li> <li class=md-nav__item> <a href=../physical-operators/UnspecifiedDistribution/ class=md-nav__link> UnspecifiedDistribution </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_18_66_4 type=checkbox id=__nav_2_18_66_4> <label class=md-nav__link for=__nav_2_18_66_4> Broadcast Modes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Broadcast Modes" data-md-level=4> <label class=md-nav__title for=__nav_2_18_66_4> <span class="md-nav__icon md-icon"></span> Broadcast Modes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/BroadcastMode/ class=md-nav__link> BroadcastMode </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashedRelationBroadcastMode/ class=md-nav__link> HashedRelationBroadcastMode </a> </li> <li class=md-nav__item> <a href=../physical-operators/IdentityBroadcastMode/ class=md-nav__link> IdentityBroadcastMode </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../physical-operators/FileSourceScanExec/ class=md-nav__link> FileSourceScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/FilterExec/ class=md-nav__link> FilterExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/GenerateExec/ class=md-nav__link> GenerateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/HashAggregateExec/ class=md-nav__link> HashAggregateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/InMemoryTableScanExec/ class=md-nav__link> InMemoryTableScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/InputAdapter/ class=md-nav__link> InputAdapter </a> </li> <li class=md-nav__item> <a href=../physical-operators/InputRDDCodegen/ class=md-nav__link> InputRDDCodegen </a> </li> <li class=md-nav__item> <a href=../physical-operators/LocalTableScanExec/ class=md-nav__link> LocalTableScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/MapElementsExec/ class=md-nav__link> MapElementsExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/MapPartitionsExec/ class=md-nav__link> MapPartitionsExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectHashAggregateExec/ class=md-nav__link> ObjectHashAggregateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ObjectProducerExec/ class=md-nav__link> ObjectProducerExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/OverwriteByExpressionExec/ class=md-nav__link> OverwriteByExpressionExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/OverwriteByExpressionExecV1/ class=md-nav__link> OverwriteByExpressionExecV1 </a> </li> <li class=md-nav__item> <a href=../physical-operators/OverwritePartitionsDynamicExec/ class=md-nav__link> OverwritePartitionsDynamicExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ProjectExec/ class=md-nav__link> ProjectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RangeExec/ class=md-nav__link> RangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RefreshTableExec/ class=md-nav__link> RefreshTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RenameTableExec/ class=md-nav__link> RenameTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReplaceTableAsSelectExec/ class=md-nav__link> ReplaceTableAsSelectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReplaceTableExec/ class=md-nav__link> ReplaceTableExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ReusedExchangeExec/ class=md-nav__link> ReusedExchangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RowDataSourceScanExec/ class=md-nav__link> RowDataSourceScanExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/RowToColumnarExec/ class=md-nav__link> RowToColumnarExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SerializeFromObjectExec/ class=md-nav__link> SerializeFromObjectExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SetCatalogAndNamespaceExec/ class=md-nav__link> SetCatalogAndNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShowCurrentNamespaceExec/ class=md-nav__link> ShowCurrentNamespaceExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShowTablesExec/ class=md-nav__link> ShowTablesExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShowTablePropertiesExec/ class=md-nav__link> ShowTablePropertiesExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffleExchangeExec/ class=md-nav__link> ShuffleExchangeExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffleExchangeLike/ class=md-nav__link> ShuffleExchangeLike </a> </li> <li class=md-nav__item> <a href=../physical-operators/ShuffledHashJoinExec/ class=md-nav__link> ShuffledHashJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortAggregateExec/ class=md-nav__link> SortAggregateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortMergeJoinExec/ class=md-nav__link> SortMergeJoinExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SortExec/ class=md-nav__link> SortExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SubqueryExec/ class=md-nav__link> SubqueryExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/SubqueryBroadcastExec/ class=md-nav__link> SubqueryBroadcastExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/TakeOrderedAndProjectExec/ class=md-nav__link> TakeOrderedAndProjectExec </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_18_105 type=checkbox id=__nav_2_18_105> <label class=md-nav__link for=__nav_2_18_105> WindowExec <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WindowExec data-md-level=3> <label class=md-nav__title for=__nav_2_18_105> <span class="md-nav__icon md-icon"></span> WindowExec </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/WindowExec/ class=md-nav__link> WindowExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/AggregateProcessor/ class=md-nav__link> AggregateProcessor </a> </li> <li class=md-nav__item> <a href=../physical-operators/WindowFunctionFrame/ class=md-nav__link> WindowFunctionFrame </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../physical-operators/WholeStageCodegenExec/ class=md-nav__link> WholeStageCodegenExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/WriteToDataSourceV2Exec/ class=md-nav__link> WriteToDataSourceV2Exec </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_19 type=checkbox id=__nav_2_19> <label class=md-nav__link for=__nav_2_19> Logical Analysis Rules <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Analysis Rules" data-md-level=2> <label class=md-nav__title for=__nav_2_19> <span class="md-nav__icon md-icon"></span> Logical Analysis Rules </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-analysis-rules/AddMetadataColumns/ class=md-nav__link> AddMetadataColumns </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/AliasViewChild/ class=md-nav__link> AliasViewChild </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/CleanupAliases/ class=md-nav__link> CleanupAliases </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/CTESubstitution/ class=md-nav__link> CTESubstitution </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/DataSourceAnalysis/ class=md-nav__link> DataSourceAnalysis </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ExtractWindowExpressions/ class=md-nav__link> ExtractWindowExpressions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/FindDataSourceTable/ class=md-nav__link> FindDataSourceTable </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/HandleNullInputsForUDF/ class=md-nav__link> HandleNullInputsForUDF </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/InConversion/ class=md-nav__link> InConversion </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/LookupFunctions/ class=md-nav__link> LookupFunctions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/PreprocessTableCreation/ class=md-nav__link> PreprocessTableCreation </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/PreWriteCheck/ class=md-nav__link> PreWriteCheck </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/RemoveAllHints/ class=md-nav__link> RemoveAllHints </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveAggregateFunctions/ class=md-nav__link> ResolveAggregateFunctions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveAliases/ class=md-nav__link> ResolveAliases </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCatalogs/ class=md-nav__link> ResolveCatalogs </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCoalesceHints/ class=md-nav__link> ResolveCoalesceHints </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveCreateNamedStruct/ class=md-nav__link> ResolveCreateNamedStruct </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveFunctions/ class=md-nav__link> ResolveFunctions </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveGroupingAnalytics/ class=md-nav__link> ResolveGroupingAnalytics </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveInlineTables/ class=md-nav__link> ResolveInlineTables </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveInsertInto/ class=md-nav__link> ResolveInsertInto </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveJoinStrategyHints/ class=md-nav__link> ResolveJoinStrategyHints </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveMissingReferences/ class=md-nav__link> ResolveMissingReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveOrdinalInOrderByAndGroupBy/ class=md-nav__link> ResolveOrdinalInOrderByAndGroupBy </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveOutputRelation/ class=md-nav__link> ResolveOutputRelation </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveReferences/ class=md-nav__link> ResolveReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveRelations/ class=md-nav__link> ResolveRelations </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSessionCatalog/ class=md-nav__link> ResolveSessionCatalog </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSQLOnFile/ class=md-nav__link> ResolveSQLOnFile </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveSubquery/ class=md-nav__link> ResolveSubquery </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveTables/ class=md-nav__link> ResolveTables </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveTempViews/ class=md-nav__link> ResolveTempViews </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveTimeZone/ class=md-nav__link> ResolveTimeZone </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveWindowFrame/ class=md-nav__link> ResolveWindowFrame </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveWindowOrder/ class=md-nav__link> ResolveWindowOrder </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/ResolveWithCTE/ class=md-nav__link> ResolveWithCTE </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/TableCapabilityCheck/ class=md-nav__link> TableCapabilityCheck </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/TimeWindowing/ class=md-nav__link> TimeWindowing </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/UpdateOuterReferences/ class=md-nav__link> UpdateOuterReferences </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WindowFrameCoercion/ class=md-nav__link> WindowFrameCoercion </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WidenSetOperationTypes/ class=md-nav__link> WidenSetOperationTypes </a> </li> <li class=md-nav__item> <a href=../logical-analysis-rules/WindowsSubstitution/ class=md-nav__link> WindowsSubstitution </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_20 type=checkbox id=__nav_2_20> <label class=md-nav__link for=__nav_2_20> Logical Optimizations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Optimizations" data-md-level=2> <label class=md-nav__title for=__nav_2_20> <span class="md-nav__icon md-icon"></span> Logical Optimizations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../logical-optimizations/CleanupDynamicPruningFilters/ class=md-nav__link> CleanupDynamicPruningFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CollapseWindow/ class=md-nav__link> CollapseWindow </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ColumnPruning/ class=md-nav__link> ColumnPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CombineTypedFilters/ class=md-nav__link> CombineTypedFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CombineUnions/ class=md-nav__link> CombineUnions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ComputeCurrentTime/ class=md-nav__link> ComputeCurrentTime </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ConstantFolding/ class=md-nav__link> ConstantFolding </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/CostBasedJoinReorder/ class=md-nav__link> CostBasedJoinReorder </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/DecimalAggregates/ class=md-nav__link> DecimalAggregates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateResolvedHint/ class=md-nav__link> EliminateResolvedHint </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateSerialization/ class=md-nav__link> EliminateSerialization </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateSubqueryAliases/ class=md-nav__link> EliminateSubqueryAliases </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/EliminateView/ class=md-nav__link> EliminateView </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ExtractPythonUDFFromAggregate/ class=md-nav__link> ExtractPythonUDFFromAggregate </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/GetCurrentDatabase/ class=md-nav__link> GetCurrentDatabase </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/InferFiltersFromConstraints/ class=md-nav__link> InferFiltersFromConstraints </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/InlineCTE/ class=md-nav__link> InlineCTE </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/LimitPushDown/ class=md-nav__link> LimitPushDown </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/NullPropagation/ class=md-nav__link> NullPropagation </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeIn/ class=md-nav__link> OptimizeIn </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeMetadataOnlyQuery/ class=md-nav__link> OptimizeMetadataOnlyQuery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/OptimizeSubqueries/ class=md-nav__link> OptimizeSubqueries </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PartitionPruning/ class=md-nav__link> PartitionPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PropagateEmptyRelation/ class=md-nav__link> PropagateEmptyRelation </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PruneFileSourcePartitions/ class=md-nav__link> PruneFileSourcePartitions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PruneFilters/ class=md-nav__link> PruneFilters </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PullupCorrelatedPredicates/ class=md-nav__link> PullupCorrelatedPredicates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownOperatorsToDataSource/ class=md-nav__link> PushDownOperatorsToDataSource </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownPredicate/ class=md-nav__link> PushDownPredicate </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushDownPredicates/ class=md-nav__link> PushDownPredicates </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/PushPredicateThroughJoin/ class=md-nav__link> PushPredicateThroughJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReorderJoin/ class=md-nav__link> ReorderJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExpressions/ class=md-nav__link> ReplaceExpressions </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExceptWithAntiJoin/ class=md-nav__link> ReplaceExceptWithAntiJoin </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/ReplaceExceptWithFilter/ class=md-nav__link> ReplaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewriteCorrelatedScalarSubquery/ class=md-nav__link> RewriteCorrelatedScalarSubquery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewriteExceptAll/ class=md-nav__link> RewriteExceptAll </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/RewritePredicateSubquery/ class=md-nav__link> RewritePredicateSubquery </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/SchemaPruning/ class=md-nav__link> SchemaPruning </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/SimplifyCasts/ class=md-nav__link> SimplifyCasts </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/UpdateCTERelationStats/ class=md-nav__link> UpdateCTERelationStats </a> </li> <li class=md-nav__item> <a href=../logical-optimizations/V2ScanRelationPushDown/ class=md-nav__link> V2ScanRelationPushDown </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_21 type=checkbox id=__nav_2_21> <label class=md-nav__link for=__nav_2_21> Execution Planning Strategies <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Execution Planning Strategies" data-md-level=2> <label class=md-nav__title for=__nav_2_21> <span class="md-nav__icon md-icon"></span> Execution Planning Strategies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../execution-planning-strategies/SparkStrategy/ class=md-nav__link> SparkStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/Aggregation/ class=md-nav__link> Aggregation </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/BasicOperators/ class=md-nav__link> BasicOperators </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/DataSourceStrategy/ class=md-nav__link> DataSourceStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/DataSourceV2Strategy/ class=md-nav__link> DataSourceV2Strategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/FileSourceStrategy/ class=md-nav__link> FileSourceStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/InMemoryScans/ class=md-nav__link> InMemoryScans </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/JoinSelection/ class=md-nav__link> JoinSelection </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/LogicalQueryStageStrategy/ class=md-nav__link> LogicalQueryStageStrategy </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/SpecialLimits/ class=md-nav__link> SpecialLimits </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/SparkStrategies/ class=md-nav__link> SparkStrategies </a> </li> <li class=md-nav__item> <a href=../execution-planning-strategies/WithCTEStrategy/ class=md-nav__link> WithCTEStrategy </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_22 type=checkbox id=__nav_2_22> <label class=md-nav__link for=__nav_2_22> Physical Optimizations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Optimizations" data-md-level=2> <label class=md-nav__title for=__nav_2_22> <span class="md-nav__icon md-icon"></span> Physical Optimizations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-optimizations/ApplyColumnarRulesAndInsertTransitions/ class=md-nav__link> ApplyColumnarRulesAndInsertTransitions </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/CoalesceBucketsInJoin/ class=md-nav__link> CoalesceBucketsInJoin </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/CollapseCodegenStages/ class=md-nav__link> CollapseCodegenStages </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/DisableUnnecessaryBucketedScan/ class=md-nav__link> DisableUnnecessaryBucketedScan </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/EnsureRequirements/ class=md-nav__link> EnsureRequirements </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ExtractPythonUDFs/ class=md-nav__link> ExtractPythonUDFs </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/PlanDynamicPruningFilters/ class=md-nav__link> PlanDynamicPruningFilters </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/PlanSubqueries/ class=md-nav__link> PlanSubqueries </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/RemoveRedundantProjects/ class=md-nav__link> RemoveRedundantProjects </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/RemoveRedundantSorts/ class=md-nav__link> RemoveRedundantSorts </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ReuseExchange/ class=md-nav__link> ReuseExchange </a> </li> <li class=md-nav__item> <a href=../physical-optimizations/ReuseSubquery/ class=md-nav__link> ReuseSubquery </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_23 type=checkbox id=__nav_2_23> <div class="md-nav__link md-nav__link--container "> <a href=../tungsten/ >Tungsten Execution Backend</a> <label for=__nav_2_23> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Tungsten Execution Backend" data-md-level=2> <label class=md-nav__title for=__nav_2_23> <span class="md-nav__icon md-icon"></span> Tungsten Execution Backend </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../InternalRow/ class=md-nav__link> InternalRow </a> </li> <li class=md-nav__item> <a href=../UnsafeRow/ class=md-nav__link> UnsafeRow </a> </li> <li class=md-nav__item> <a href=../tungsten/UnsafeRowSerializerInstance/ class=md-nav__link> UnsafeRowSerializerInstance </a> </li> <li class=md-nav__item> <a href=../CatalystSerde/ class=md-nav__link> CatalystSerde </a> </li> <li class=md-nav__item> <a href=../ExternalAppendOnlyUnsafeRowArray/ class=md-nav__link> ExternalAppendOnlyUnsafeRowArray </a> </li> <li class=md-nav__item> <a href=../UnsafeFixedWidthAggregationMap/ class=md-nav__link> UnsafeFixedWidthAggregationMap </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_24 type=checkbox id=__nav_2_24> <label class=md-nav__link for=__nav_2_24> Encoder <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Encoder data-md-level=2> <label class=md-nav__title for=__nav_2_24> <span class="md-nav__icon md-icon"></span> Encoder </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Encoder/ class=md-nav__link> Encoder </a> </li> <li class=md-nav__item> <a href=../ExpressionEncoder/ class=md-nav__link> ExpressionEncoder </a> </li> <li class=md-nav__item> <a href=../RowEncoder/ class=md-nav__link> RowEncoder </a> </li> <li class=md-nav__item> <a href=../ScalaReflection/ class=md-nav__link> ScalaReflection </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3> New &amp; Noteworthy <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="New & Noteworthy" data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> New &amp; Noteworthy </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../common-table-expressions/ class=md-nav__link> Common Table Expressions </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2 type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2> Join Queries <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Join Queries" data-md-level=2> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Join Queries </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../joins/ class=md-nav__link> Joins </a> </li> <li class=md-nav__item> <a href=../spark-sql-joins-broadcast/ class=md-nav__link> Broadcast Joins </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3 type=checkbox id=__nav_3_3> <div class="md-nav__link md-nav__link--container "> <a href=../adaptive-query-execution/ >Adaptive Query Execution</a> <label for=__nav_3_3> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Adaptive Query Execution" data-md-level=2> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Adaptive Query Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../adaptive-query-execution/AQEUtils/ class=md-nav__link> AQEUtils </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/LogicalQueryStage/ class=md-nav__link> LogicalQueryStage </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/InsertAdaptiveSparkPlan/ class=md-nav__link> InsertAdaptiveSparkPlan </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/AdaptiveSparkPlanExec/ class=md-nav__link> AdaptiveSparkPlanExec </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/AdaptiveExecutionContext/ class=md-nav__link> AdaptiveExecutionContext </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/PlanAdaptiveSubqueries/ class=md-nav__link> PlanAdaptiveSubqueries </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/ShuffleStage/ class=md-nav__link> ShuffleStage </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3_9 type=checkbox id=__nav_3_3_9> <label class=md-nav__link for=__nav_3_3_9> Adaptive Logical Optimizer <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Adaptive Logical Optimizer" data-md-level=3> <label class=md-nav__title for=__nav_3_3_9> <span class="md-nav__icon md-icon"></span> Adaptive Logical Optimizer </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../adaptive-query-execution/AQEOptimizer/ class=md-nav__link> AQEOptimizer </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/DemoteBroadcastHashJoin/ class=md-nav__link> DemoteBroadcastHashJoin </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/EliminateJoinToEmptyRelation/ class=md-nav__link> EliminateJoinToEmptyRelation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3_10 type=checkbox id=__nav_3_3_10> <label class=md-nav__link for=__nav_3_3_10> Physical Optimizations <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Optimizations" data-md-level=3> <label class=md-nav__title for=__nav_3_3_10> <span class="md-nav__icon md-icon"></span> Physical Optimizations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../adaptive-query-execution/AQEShuffleReadRule/ class=md-nav__link> AQEShuffleReadRule </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/CoalesceShufflePartitions/ class=md-nav__link> CoalesceShufflePartitions </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/OptimizeShuffleWithLocalRead/ class=md-nav__link> OptimizeShuffleWithLocalRead </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/OptimizeSkewInRebalancePartitions/ class=md-nav__link> OptimizeSkewInRebalancePartitions </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/OptimizeSkewedJoin/ class=md-nav__link> OptimizeSkewedJoin </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/PlanAdaptiveDynamicPruningFilters/ class=md-nav__link> PlanAdaptiveDynamicPruningFilters </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/ReuseAdaptiveSubquery/ class=md-nav__link> ReuseAdaptiveSubquery </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3_11 type=checkbox id=__nav_3_3_11> <label class=md-nav__link for=__nav_3_3_11> QueryStageExecs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=QueryStageExecs data-md-level=3> <label class=md-nav__title for=__nav_3_3_11> <span class="md-nav__icon md-icon"></span> QueryStageExecs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../adaptive-query-execution/QueryStageExec/ class=md-nav__link> QueryStageExec </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/BroadcastQueryStageExec/ class=md-nav__link> BroadcastQueryStageExec </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/ShuffleQueryStageExec/ class=md-nav__link> ShuffleQueryStageExec </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/CostEvaluator/ class=md-nav__link> CostEvaluator </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/SimpleCostEvaluator/ class=md-nav__link> SimpleCostEvaluator </a> </li> <li class=md-nav__item> <a href=../adaptive-query-execution/demo/ class=md-nav__link> Demo </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/statistics/ class=md-nav__link> Statistics </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/metadata-columns/ class=md-nav__link> Metadata Columns </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/catalog-plugin-api-and-multi-catalog-support/ class=md-nav__link> Catalog Plugin API and Multi-Catalog Support </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/dynamic-partition-pruning/ class=md-nav__link> Dynamic Partition Pruning </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/explain-command-improved/ class=md-nav__link> Explaining Query Plans Improved </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_9 type=checkbox id=__nav_3_9> <label class=md-nav__link for=__nav_3_9> Hints <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Hints data-md-level=2> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Hints </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../new-and-noteworthy/hint-framework/ class=md-nav__link> Hint Framework </a> </li> <li class=md-nav__item> <a href=../JoinHint/ class=md-nav__link> JoinHint </a> </li> <li class=md-nav__item> <a href=../HintInfo/ class=md-nav__link> HintInfo </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/join-strategy-hints/ class=md-nav__link> Join Strategy Hints </a> </li> <li class=md-nav__item> <a href=../HintErrorHandler/ class=md-nav__link> HintErrorHandler </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/observable-metrics/ class=md-nav__link> Observable Metrics </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/columnar-processing/ class=md-nav__link> Columnar Processing </a> </li> <li class=md-nav__item> <a href=../new-and-noteworthy/datasource-v2/ class=md-nav__link> DataSource V2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-hive-integration/ class=md-nav__link> Hive Integration </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_14 type=checkbox id=__nav_3_14> <label class=md-nav__link for=__nav_3_14> Vectorized Parquet Decoding <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Vectorized Parquet Decoding" data-md-level=2> <label class=md-nav__title for=__nav_3_14> <span class="md-nav__icon md-icon"></span> Vectorized Parquet Decoding </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../vectorized-parquet-reader/ class=md-nav__link> Vectorized Parquet Decoding </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_14_2 type=checkbox id=__nav_3_14_2> <label class=md-nav__link for=__nav_3_14_2> ColumnVectors <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ColumnVectors data-md-level=3> <label class=md-nav__title for=__nav_3_14_2> <span class="md-nav__icon md-icon"></span> ColumnVectors </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ColumnVector/ class=md-nav__link> ColumnVector </a> </li> <li class=md-nav__item> <a href=../WritableColumnVector/ class=md-nav__link> WritableColumnVector </a> </li> <li class=md-nav__item> <a href=../OnHeapColumnVector/ class=md-nav__link> OnHeapColumnVector </a> </li> <li class=md-nav__item> <a href=../OffHeapColumnVector/ class=md-nav__link> OffHeapColumnVector </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../dynamic-partition-inserts/ class=md-nav__link> Dynamic Partition Inserts </a> </li> <li class=md-nav__item> <a href=../bucketing/ class=md-nav__link> Bucketing </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_17 type=checkbox id=__nav_3_17> <div class="md-nav__link md-nav__link--container "> <a href=../whole-stage-code-generation/ >Whole-Stage CodeGen</a> <label for=__nav_3_17> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Whole-Stage CodeGen" data-md-level=2> <label class=md-nav__title for=__nav_3_17> <span class="md-nav__icon md-icon"></span> Whole-Stage CodeGen </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../whole-stage-code-generation/BufferedRowIterator/ class=md-nav__link> BufferedRowIterator </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/CodegenContext/ class=md-nav__link> CodegenContext </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/CodeGenerator/ class=md-nav__link> CodeGenerator </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/Block/ class=md-nav__link> Block </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GenerateUnsafeProjection/ class=md-nav__link> GenerateUnsafeProjection </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GenerateMutableProjection/ class=md-nav__link> GenerateMutableProjection </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GenerateColumnAccessor/ class=md-nav__link> GenerateColumnAccessor </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GenerateOrdering/ class=md-nav__link> GenerateOrdering </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GeneratePredicate/ class=md-nav__link> GeneratePredicate </a> </li> <li class=md-nav__item> <a href=../whole-stage-code-generation/GenerateSafeProjection/ class=md-nav__link> GenerateSafeProjection </a> </li> <li class=md-nav__item> <a href=../spark-sql-BytesToBytesMap/ class=md-nav__link> BytesToBytesMap </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_17_13 type=checkbox id=__nav_3_17_13> <label class=md-nav__link for=__nav_3_17_13> Subexpression Elimination <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Subexpression Elimination" data-md-level=3> <label class=md-nav__title for=__nav_3_17_13> <span class="md-nav__icon md-icon"></span> Subexpression Elimination </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-subexpression-elimination/ class=md-nav__link> Subexpression Elimination In Code-Generated Expression Evaluation (Common Expression Reuse) </a> </li> <li class=md-nav__item> <a href=../EquivalentExpressions/ class=md-nav__link> EquivalentExpressions </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_18 type=checkbox id=__nav_3_18> <label class=md-nav__link for=__nav_3_18> Vectorized Query Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Vectorized Query Execution" data-md-level=2> <label class=md-nav__title for=__nav_3_18> <span class="md-nav__icon md-icon"></span> Vectorized Query Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-vectorized-query-execution/ class=md-nav__link> Vectorized Query Execution </a> </li> <li class=md-nav__item> <a href=../ColumnarBatch/ class=md-nav__link> ColumnarBatch </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-subqueries/ class=md-nav__link> Subqueries </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_20 type=checkbox id=__nav_3_20> <label class=md-nav__link for=__nav_3_20> Cost-Based Optimization <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Cost-Based Optimization" data-md-level=2> <label class=md-nav__title for=__nav_3_20> <span class="md-nav__icon md-icon"></span> Cost-Based Optimization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-cost-based-optimization/ class=md-nav__link> Cost-Based Optimization (CBO) of Logical Query Plan </a> </li> <li class=md-nav__item> <a href=../spark-sql-CatalogStatistics/ class=md-nav__link> CatalogStatistics </a> </li> <li class=md-nav__item> <a href=../spark-sql-ColumnStat/ class=md-nav__link> ColumnStat </a> </li> <li class=md-nav__item> <a href=../CommandUtils/ class=md-nav__link> CommandUtils </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_21 type=checkbox id=__nav_3_21> <div class="md-nav__link md-nav__link--container "> <a href=../catalyst-dsl/ >Catalyst DSL</a> <label for=__nav_3_21> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Catalyst DSL" data-md-level=2> <label class=md-nav__title for=__nav_3_21> <span class="md-nav__icon md-icon"></span> Catalyst DSL </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../catalyst-dsl/DslLogicalPlan/ class=md-nav__link> DslLogicalPlan </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4 type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4> High-Level APIs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="High-Level APIs" data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> High-Level APIs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SparkSession/ class=md-nav__link> SparkSession </a> </li> <li class=md-nav__item> <a href=../SparkSession-Builder/ class=md-nav__link> SparkSession.Builder </a> </li> <li class=md-nav__item> <a href=../SparkSessionExtensions/ class=md-nav__link> SparkSessionExtensions </a> </li> <li class=md-nav__item> <a href=../ExecutionListenerManager/ class=md-nav__link> ExecutionListenerManager </a> </li> <li class=md-nav__item> <a href=../ColumnarRule/ class=md-nav__link> ColumnarRule </a> </li> <li class=md-nav__item> <a href=../Dataset/ class=md-nav__link> Dataset </a> </li> <li class=md-nav__item> <a href=../DataFrame/ class=md-nav__link> DataFrame </a> </li> <li class=md-nav__item> <a href=../DataFrameReader/ class=md-nav__link> DataFrameReader </a> </li> <li class=md-nav__item> <a href=../DataFrameWriter/ class=md-nav__link> DataFrameWriter </a> </li> <li class=md-nav__item> <a href=../DataFrameWriterV2/ class=md-nav__link> DataFrameWriterV2 </a> </li> <li class=md-nav__item> <a href=../Encoders/ class=md-nav__link> Encoders </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_5 type=checkbox id=__nav_5> <div class="md-nav__link md-nav__link--container "> <a href=../demo/ >Demos</a> <label for=__nav_5> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label=Demos data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../demo/developing-catalogplugin/ class=md-nav__link> Developing CatalogPlugin </a> </li> <li class=md-nav__item> <a href=../demo/connecting-spark-sql-to-hive-metastore/ class=md-nav__link> Connecting Spark SQL to Hive Metastore </a> </li> <li class=md-nav__item> <a href=../demo/hive-partitioned-parquet-table-partition-pruning/ class=md-nav__link> Hive Partitioned Parquet Table and Partition Pruning </a> </li> <li class=md-nav__item> <a href=../demo/using-jdbc-data-source-to-access-postgresql/ class=md-nav__link> Using JDBC Data Source to Access PostgreSQL </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6 type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6> Misc <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Misc data-md-level=1> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Misc </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../JoinSelectionHelper/ class=md-nav__link> JoinSelectionHelper </a> </li> <li class=md-nav__item> <a href=../ExplainUtils/ class=md-nav__link> ExplainUtils </a> </li> <li class=md-nav__item> <a href=../DataSourceRDD/ class=md-nav__link> DataSourceRDD </a> </li> <li class=md-nav__item> <a href=../DataSourceRDDPartition/ class=md-nav__link> DataSourceRDDPartition </a> </li> <li class=md-nav__item> <a href=../DataWritingSparkTask/ class=md-nav__link> DataWritingSparkTask </a> </li> <li class=md-nav__item> <a href=../spark-sql-dataset-rdd/ class=md-nav__link> Dataset, DataFrame and RDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-dataset-vs-sql/ class=md-nav__link> Dataset and SQL </a> </li> <li class=md-nav__item> <a href=../DataSourceV2Utils/ class=md-nav__link> DataSourceV2Utils </a> </li> <li class=md-nav__item> <a href=../implicits/ class=md-nav__link> Implicits </a> </li> <li class=md-nav__item> <a href=../Row/ class=md-nav__link> Row </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_11 type=checkbox id=__nav_6_11> <label class=md-nav__link for=__nav_6_11> Data Source API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Source API" data-md-level=2> <label class=md-nav__title for=__nav_6_11> <span class="md-nav__icon md-icon"></span> Data Source API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-datasource-api/ class=md-nav__link> DataSource API </a> </li> <li class=md-nav__item> <a href=../CreateTableWriter/ class=md-nav__link> CreateTableWriter </a> </li> <li class=md-nav__item> <a href=../WriteConfigMethods/ class=md-nav__link> WriteConfigMethods </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_12 type=checkbox id=__nav_6_12> <label class=md-nav__link for=__nav_6_12> Dataset API <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Dataset API" data-md-level=2> <label class=md-nav__title for=__nav_6_12> <span class="md-nav__icon md-icon"></span> Dataset API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-dataset-operators/ class=md-nav__link> Dataset API &mdash; Dataset Operators </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-typed-transformations/ class=md-nav__link> Dataset API &mdash; Typed Transformations </a> </li> <li class=md-nav__item> <a href=../Dataset-untyped-transformations/ class=md-nav__link> Untyped Transformations </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-basic-actions/ class=md-nav__link> Dataset API &mdash; Basic Actions </a> </li> <li class=md-nav__item> <a href=../spark-sql-Dataset-actions/ class=md-nav__link> Actions </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataFrameNaFunctions/ class=md-nav__link> DataFrameNaFunctions &mdash; Working With Missing Data </a> </li> <li class=md-nav__item> <a href=../spark-sql-DataFrameStatFunctions/ class=md-nav__link> DataFrameStatFunctions </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_13 type=checkbox id=__nav_6_13> <label class=md-nav__link for=__nav_6_13> Column <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Column data-md-level=2> <label class=md-nav__title for=__nav_6_13> <span class="md-nav__icon md-icon"></span> Column </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Column/ class=md-nav__link> Column </a> </li> <li class=md-nav__item> <a href=../spark-sql-column-operators/ class=md-nav__link> Column Operators </a> </li> <li class=md-nav__item> <a href=../TypedColumn/ class=md-nav__link> TypedColumn </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_14 type=checkbox id=__nav_6_14> <label class=md-nav__link for=__nav_6_14> Basic Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Basic Aggregation" data-md-level=2> <label class=md-nav__title for=__nav_6_14> <span class="md-nav__icon md-icon"></span> Basic Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-basic-aggregation/ class=md-nav__link> Basic Aggregation &mdash; Typed and Untyped Grouping Operators </a> </li> <li class=md-nav__item> <a href=../RelationalGroupedDataset/ class=md-nav__link> RelationalGroupedDataset </a> </li> <li class=md-nav__item> <a href=../KeyValueGroupedDataset/ class=md-nav__link> KeyValueGroupedDataset </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_15 type=checkbox id=__nav_6_15> <label class=md-nav__link for=__nav_6_15> Window Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Window Aggregation" data-md-level=2> <label class=md-nav__title for=__nav_6_15> <span class="md-nav__icon md-icon"></span> Window Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-window-aggregation/ class=md-nav__link> Window Aggregation </a> </li> <li class=md-nav__item> <a href=../WindowSpec/ class=md-nav__link> WindowSpec </a> </li> <li class=md-nav__item> <a href=../spark-sql-WindowSpec-Window/ class=md-nav__link> Window Utility </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_16 type=checkbox id=__nav_6_16> <label class=md-nav__link for=__nav_6_16> Standard Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Standard Functions" data-md-level=2> <label class=md-nav__title for=__nav_6_16> <span class="md-nav__icon md-icon"></span> Standard Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-functions/ class=md-nav__link> Standard Functions &mdash; functions Object </a> </li> <li class=md-nav__item> <a href=../spark-sql-aggregate-functions/ class=md-nav__link> Aggregate Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-collection/ class=md-nav__link> Standard Functions for Collections (Collection Functions) </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-datetime/ class=md-nav__link> Date and Time Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-regular-functions/ class=md-nav__link> Regular Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-functions-windows/ class=md-nav__link> Standard Functions for Window Aggregation (Window Functions) </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_17 type=checkbox id=__nav_6_17> <label class=md-nav__link for=__nav_6_17> User-Defined Functions <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="User-Defined Functions" data-md-level=2> <label class=md-nav__title for=__nav_6_17> <span class="md-nav__icon md-icon"></span> User-Defined Functions </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-udfs/ class=md-nav__link> User-Defined Functions </a> </li> <li class=md-nav__item> <a href=../spark-sql-udfs-blackbox/ class=md-nav__link> UDFs are Blackbox </a> </li> <li class=md-nav__item> <a href=../UserDefinedFunction/ class=md-nav__link> UserDefinedFunction </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_18 type=checkbox id=__nav_6_18> <div class="md-nav__link md-nav__link--container "> <a href=../types/ >Data Types</a> <label for=__nav_6_18> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav aria-label="Data Types" data-md-level=2> <label class=md-nav__title for=__nav_6_18> <span class="md-nav__icon md-icon"></span> Data Types </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../types/AbstractDataType/ class=md-nav__link> AbstractDataType </a> </li> <li class=md-nav__item> <a href=../types/ArrayType/ class=md-nav__link> ArrayType </a> </li> <li class=md-nav__item> <a href=../types/AtomicType/ class=md-nav__link> AtomicType </a> </li> <li class=md-nav__item> <a href=../types/DataType/ class=md-nav__link> DataType </a> </li> <li class=md-nav__item> <a href=../types/StructField/ class=md-nav__link> StructField </a> </li> <li class=md-nav__item> <a href=../types/StructType/ class=md-nav__link> StructType </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-multi-dimensional-aggregation/ class=md-nav__link> Multi-Dimensional Aggregation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_20 type=checkbox id=__nav_6_20> <label class=md-nav__link for=__nav_6_20> Caching and Persistence <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Caching and Persistence" data-md-level=2> <label class=md-nav__title for=__nav_6_20> <span class="md-nav__icon md-icon"></span> Caching and Persistence </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-caching-and-persistence/ class=md-nav__link> Caching and Persistence </a> </li> <li class=md-nav__item> <a href=../spark-sql-caching-webui-storage/ class=md-nav__link> User-Friendly Names Of Cached Queries in web UI's Storage Tab </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-checkpointing/ class=md-nav__link> Checkpointing </a> </li> <li class=md-nav__item> <a href=../UserDefinedAggregateFunction/ class=md-nav__link> UserDefinedAggregateFunction </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_23 type=checkbox id=__nav_6_23> <label class=md-nav__link for=__nav_6_23> AggregationIterators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=AggregationIterators data-md-level=2> <label class=md-nav__title for=__nav_6_23> <span class="md-nav__icon md-icon"></span> AggregationIterators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../AggregationIterator/ class=md-nav__link> AggregationIterator </a> </li> <li class=md-nav__item> <a href=../ObjectAggregationIterator/ class=md-nav__link> ObjectAggregationIterator </a> </li> <li class=md-nav__item> <a href=../SortBasedAggregationIterator/ class=md-nav__link> SortBasedAggregationIterator </a> </li> <li class=md-nav__item> <a href=../TungstenAggregationIterator/ class=md-nav__link> TungstenAggregationIterator </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../rdds/FileScanRDD/ class=md-nav__link> FileScanRDD </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_25 type=checkbox id=__nav_6_25> <label class=md-nav__link for=__nav_6_25> Monitoring <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Monitoring data-md-level=2> <label class=md-nav__title for=__nav_6_25> <span class="md-nav__icon md-icon"></span> Monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../SQLTab/ class=md-nav__link> SQLTab </a> </li> <li class=md-nav__item> <a href=../SQLListener/ class=md-nav__link> SQLListener </a> </li> <li class=md-nav__item> <a href=../QueryExecutionListener/ class=md-nav__link> QueryExecutionListener </a> </li> <li class=md-nav__item> <a href=../SQLAppStatusListener/ class=md-nav__link> SQLAppStatusListener </a> </li> <li class=md-nav__item> <a href=../SQLAppStatusPlugin/ class=md-nav__link> SQLAppStatusPlugin </a> </li> <li class=md-nav__item> <a href=../SQLAppStatusStore/ class=md-nav__link> SQLAppStatusStore </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_25_7 type=checkbox id=__nav_6_25_7> <label class=md-nav__link for=__nav_6_25_7> WriteTaskStats <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteTaskStats data-md-level=3> <label class=md-nav__title for=__nav_6_25_7> <span class="md-nav__icon md-icon"></span> WriteTaskStats </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-WriteTaskStats/ class=md-nav__link> WriteTaskStats </a> </li> <li class=md-nav__item> <a href=../spark-sql-BasicWriteTaskStats/ class=md-nav__link> BasicWriteTaskStats </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_25_8 type=checkbox id=__nav_6_25_8> <label class=md-nav__link for=__nav_6_25_8> WriteTaskStatsTracker <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteTaskStatsTracker data-md-level=3> <label class=md-nav__title for=__nav_6_25_8> <span class="md-nav__icon md-icon"></span> WriteTaskStatsTracker </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-WriteTaskStatsTracker/ class=md-nav__link> WriteTaskStatsTracker </a> </li> <li class=md-nav__item> <a href=../spark-sql-BasicWriteTaskStatsTracker/ class=md-nav__link> BasicWriteTaskStatsTracker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_25_9 type=checkbox id=__nav_6_25_9> <label class=md-nav__link for=__nav_6_25_9> WriteJobStatsTracker <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteJobStatsTracker data-md-level=3> <label class=md-nav__title for=__nav_6_25_9> <span class="md-nav__icon md-icon"></span> WriteJobStatsTracker </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../WriteJobStatsTracker/ class=md-nav__link> WriteJobStatsTracker </a> </li> <li class=md-nav__item> <a href=../BasicWriteJobStatsTracker/ class=md-nav__link> BasicWriteJobStatsTracker </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-logging/ class=md-nav__link> Logging </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_26 type=checkbox id=__nav_6_26> <label class=md-nav__link for=__nav_6_26> Performance Tuning and Debugging <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Performance Tuning and Debugging" data-md-level=2> <label class=md-nav__title for=__nav_6_26> <span class="md-nav__icon md-icon"></span> Performance Tuning and Debugging </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-performance-tuning/ class=md-nav__link> Performance Tuning </a> </li> <li class=md-nav__item> <a href=../spark-sql-performance-tuning-groupBy-aggregation/ class=md-nav__link> Case Study </a> </li> <li class=md-nav__item> <a href=../spark-sql-debugging-query-execution/ class=md-nav__link> Debugging Query Execution </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../tools/spark-sql-spark-sql/ class=md-nav__link> Tools </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_6_28 type=checkbox id=__nav_6_28> <label class=md-nav__link for=__nav_6_28> Spark Thrift Server <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Spark Thrift Server" data-md-level=2> <label class=md-nav__title for=__nav_6_28> <span class="md-nav__icon md-icon"></span> Spark Thrift Server </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../thrift-server/spark-sql-thrift-server/ class=md-nav__link> HiveThriftServer2 </a> </li> <li class=md-nav__item> <a href=../thrift-server/spark-sql-thriftserver-SparkSQLEnv/ class=md-nav__link> SparkSQLEnv </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../SQLExecution/ class=md-nav__link> SQLExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-RDDConversions/ class=md-nav__link> RDDConversions </a> </li> <li class=md-nav__item> <a href=../ShuffledRowRDD/ class=md-nav__link> ShuffledRowRDD </a> </li> <li class=md-nav__item> <a href=../UnsupportedOperationChecker/ class=md-nav__link> UnsupportedOperationChecker </a> </li> <li class=md-nav__item> <a href=../CheckAnalysis/ class=md-nav__link> CheckAnalysis </a> </li> <li class=md-nav__item> <a href=../CatalystTypeConverters/ class=md-nav__link> CatalystTypeConverters </a> </li> <li class=md-nav__item> <a href=../spark-sql-StatFunctions/ class=md-nav__link> StatFunctions </a> </li> <li class=md-nav__item> <a href=../spark-sql-SubExprUtils/ class=md-nav__link> SubExprUtils </a> </li> <li class=md-nav__item> <a href=../PredicateHelper/ class=md-nav__link> PredicateHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-DDLUtils/ class=md-nav__link> DDLUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-SchemaUtils/ class=md-nav__link> SchemaUtils </a> </li> <li class=md-nav__item> <a href=../AggUtils/ class=md-nav__link> AggUtils </a> </li> <li class=md-nav__item> <a href=../CreateStruct/ class=md-nav__link> CreateStruct </a> </li> <li class=md-nav__item> <a href=../spark-sql-TypeCoercion/ class=md-nav__link> TypeCoercion </a> </li> <li class=md-nav__item> <a href=../TypeCoercionRule/ class=md-nav__link> TypeCoercionRule </a> </li> <li class=md-nav__item> <a href=../ExtractEquiJoinKeys/ class=md-nav__link> ExtractEquiJoinKeys </a> </li> <li class=md-nav__item> <a href=../ExtractSingleColumnNullAwareAntiJoin/ class=md-nav__link> ExtractSingleColumnNullAwareAntiJoin </a> </li> <li class=md-nav__item> <a href=../ExtractJoinWithBuckets/ class=md-nav__link> ExtractJoinWithBuckets </a> </li> <li class=md-nav__item> <a href=../PhysicalAggregation/ class=md-nav__link> PhysicalAggregation </a> </li> <li class=md-nav__item> <a href=../PhysicalOperation/ class=md-nav__link> PhysicalOperation </a> </li> <li class=md-nav__item> <a href=../KnownSizeEstimation/ class=md-nav__link> KnownSizeEstimation </a> </li> <li class=md-nav__item> <a href=../spark-sql-SizeEstimator/ class=md-nav__link> SizeEstimator </a> </li> <li class=md-nav__item> <a href=../spark-sql-PartitioningUtils/ class=md-nav__link> PartitioningUtils </a> </li> <li class=md-nav__item> <a href=../spark-sql-spark-HadoopFileLinesReader/ class=md-nav__link> HadoopFileLinesReader </a> </li> <li class=md-nav__item> <a href=../CatalogUtils/ class=md-nav__link> CatalogUtils </a> </li> <li class=md-nav__item> <a href=../ExternalCatalogUtils/ class=md-nav__link> ExternalCatalogUtils </a> </li> <li class=md-nav__item> <a href=../CompressionCodecs/ class=md-nav__link> CompressionCodecs </a> </li> <li class=md-nav__item> <a href=../JoinStrategyHint/ class=md-nav__link> JoinStrategyHint </a> </li> <li class=md-nav__item> <a href=../PushDownUtils/ class=md-nav__link> PushDownUtils </a> </li> <li class=md-nav__item> <a href=../SQLContext/ class=md-nav__link> SQLContext </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#sparksqladaptiveautobroadcastjointhreshold class=md-nav__link> spark.sql.adaptive.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecustomcostevaluatorclass class=md-nav__link> spark.sql.adaptive.customCostEvaluatorClass </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdecorrelateinnerqueryenabled class=md-nav__link> spark.sql.optimizer.decorrelateInnerQuery.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveoptimizeskewsinrebalancepartitionsenabled class=md-nav__link> spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownonemptylocationinctas class=md-nav__link> spark.sql.legacy.allowNonEmptyLocationInCTAS </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowautogeneratedaliasforview class=md-nav__link> spark.sql.legacy.allowAutoGeneratedAliasForView </a> </li> <li class=md-nav__item> <a href=#sparksqlsessionwindowbufferspillthreshold class=md-nav__link> spark.sql.sessionWindow.buffer.spill.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkselfdestructenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.selfDestruct.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowstarwithsingletableidentifierincount class=md-nav__link> spark.sql.legacy.allowStarWithSingleTableIdentifierInCount </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerenablecsvexpressionoptimization class=md-nav__link> spark.sql.optimizer.enableCsvExpressionOptimization </a> </li> <li class=md-nav__item> <a href=#sparksqlsessionwindowbufferinmemorythreshold class=md-nav__link> spark.sql.sessionWindow.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlorcenablenestedcolumnvectorizedreader class=md-nav__link> spark.sql.orc.enableNestedColumnVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizeroptimizeonerowrelationsubquery class=md-nav__link> spark.sql.optimizer.optimizeOneRowRelationSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveforceapply class=md-nav__link> spark.sql.adaptive.forceApply </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsenabled class=md-nav__link> spark.sql.adaptive.coalescePartitions.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionsize class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionSize </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsparallelismfirst class=md-nav__link> spark.sql.adaptive.coalescePartitions.parallelismFirst </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveadvisorypartitionsizeinbytes class=md-nav__link> spark.sql.adaptive.advisoryPartitionSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsminpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.minPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivecoalescepartitionsinitialpartitionnum class=md-nav__link> spark.sql.adaptive.coalescePartitions.initialPartitionNum </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveenabled class=md-nav__link> spark.sql.adaptive.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivefetchshuffleblocksinbatch class=md-nav__link> spark.sql.adaptive.fetchShuffleBlocksInBatch </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivelocalshufflereaderenabled class=md-nav__link> spark.sql.adaptive.localShuffleReader.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveloglevel class=md-nav__link> spark.sql.adaptive.logLevel </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivemaxshuffledhashjoinlocalmapthreshold class=md-nav__link> spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveoptimizerexcludedrules class=md-nav__link> spark.sql.adaptive.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinenabled class=md-nav__link> spark.sql.adaptive.skewJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionfactor class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes class=md-nav__link> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin class=md-nav__link> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzermaxiterations class=md-nav__link> spark.sql.analyzer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqlanalyzerfailambiguousselfjoin class=md-nav__link> spark.sql.analyzer.failAmbiguousSelfJoin </a> </li> <li class=md-nav__item> <a href=#sparksqlansienabled class=md-nav__link> spark.sql.ansi.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcliprintheader class=md-nav__link> spark.sql.cli.print.header </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenwholestage class=md-nav__link> spark.sql.codegen.wholeStage </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmethodsplitthreshold class=md-nav__link> spark.sql.codegen.methodSplitThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqldebugmaxtostringfields class=md-nav__link> spark.sql.debug.maxToStringFields </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultcatalog class=md-nav__link> spark.sql.defaultCatalog </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionremoveredundantsorts class=md-nav__link> spark.sql.execution.removeRedundantSorts </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionreusesubquery class=md-nav__link> spark.sql.execution.reuseSubquery </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionsortbeforerepartition class=md-nav__link> spark.sql.execution.sortBeforeRepartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionrangeexchangesamplesizeperpartition class=md-nav__link> spark.sql.execution.rangeExchange.sampleSizePerPartition </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowpysparkfallbackenabled class=md-nav__link> spark.sql.execution.arrow.pyspark.fallback.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionarrowsparkrenabled class=md-nav__link> spark.sql.execution.arrow.sparkr.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasudfbuffersize class=md-nav__link> spark.sql.execution.pandas.udf.buffer.size </a> </li> <li class=md-nav__item> <a href=#sparksqlexecutionpandasconverttoarrowarraysafely class=md-nav__link> spark.sql.execution.pandas.convertToArrowArraySafely </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramenabled class=md-nav__link> spark.sql.statistics.histogram.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsessiontimezone class=md-nav__link> spark.sql.session.timeZone </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcescommitprotocolclass class=md-nav__link> spark.sql.sources.commitProtocolClass </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesignoredatalocality class=md-nav__link> spark.sql.sources.ignoreDataLocality </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesvalidatepartitioncolumns class=md-nav__link> spark.sql.sources.validatePartitionColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesusev1sourcelist class=md-nav__link> spark.sql.sources.useV1SourceList </a> </li> <li class=md-nav__item> <a href=#sparksqlstoreassignmentpolicy class=md-nav__link> spark.sql.storeAssignmentPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlthriftserverinterruptoncancel class=md-nav__link> spark.sql.thriftServer.interruptOnCancel </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetswitchthreshold class=md-nav__link> spark.sql.optimizer.inSetSwitchThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangeloglevel class=md-nav__link> spark.sql.optimizer.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogrules class=md-nav__link> spark.sql.optimizer.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerplanchangelogbatches class=md-nav__link> spark.sql.optimizer.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningenabled class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlhivetablepropertylengththreshold class=md-nav__link> spark.sql.hive.tablePropertyLengthThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizercanchangecachedplanoutputpartitioning class=md-nav__link> spark.sql.optimizer.canChangeCachedPlanOutputPartitioning </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningpruningsideextrafilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningusestats class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.useStats </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly class=md-nav__link> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources class=md-nav__link> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerserializernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexpressionnestedpruningenabled class=md-nav__link> spark.sql.optimizer.expression.nestedPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcmergeschema class=md-nav__link> spark.sql.orc.mergeSchema </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingautobucketedscanenabled class=md-nav__link> spark.sql.sources.bucketing.autoBucketedScan.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldatetimejava8apienabled class=md-nav__link> spark.sql.datetime.java8API.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyintervalenabled class=md-nav__link> spark.sql.legacy.interval.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbinaryfilemaxlength class=md-nav__link> spark.sql.sources.binaryFile.maxLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmapkeydeduppolicy class=md-nav__link> spark.sql.mapKeyDedupPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxconcurrentoutputfilewriters class=md-nav__link> spark.sql.maxConcurrentOutputFileWriters </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxmetadatastringlength class=md-nav__link> spark.sql.maxMetadataStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqlmavenadditionalremoterepositories class=md-nav__link> spark.sql.maven.additionalRemoteRepositories </a> </li> <li class=md-nav__item> <a href=#sparksqlmaxplanstringlength class=md-nav__link> spark.sql.maxPlanStringLength </a> </li> <li class=md-nav__item> <a href=#sparksqladdpartitioninbatchsize class=md-nav__link> spark.sql.addPartitionInBatch.size </a> </li> <li class=md-nav__item> <a href=#sparksqlscripttransformationexittimeoutinseconds class=md-nav__link> spark.sql.scriptTransformation.exitTimeoutInSeconds </a> </li> <li class=md-nav__item> <a href=#sparksqlautobroadcastjointhreshold class=md-nav__link> spark.sql.autoBroadcastJoinThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqlavrocompressioncodec class=md-nav__link> spark.sql.avro.compression.codec </a> </li> <li class=md-nav__item> <a href=#sparksqlbroadcasttimeout class=md-nav__link> spark.sql.broadcastTimeout </a> </li> <li class=md-nav__item> <a href=#sparksqlbucketingcoalescebucketsinjoinenabled class=md-nav__link> spark.sql.bucketing.coalesceBucketsInJoin.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcasesensitive class=md-nav__link> spark.sql.caseSensitive </a> </li> <li class=md-nav__item> <a href=#sparksqlcatalogspark_catalog class=md-nav__link> spark.sql.catalog.spark_catalog </a> </li> <li class=md-nav__item> <a href=#sparksqlcboenabled class=md-nav__link> spark.sql.cbo.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbojoinreorderenabled class=md-nav__link> spark.sql.cbo.joinReorder.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcboplanstatsenabled class=md-nav__link> spark.sql.cbo.planStats.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcbostarschemadetection class=md-nav__link> spark.sql.cbo.starSchemaDetection </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatemapvectorizedenable class=md-nav__link> spark.sql.codegen.aggregate.map.vectorized.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenaggregatesplitaggregatefuncenabled class=md-nav__link> spark.sql.codegen.aggregate.splitAggregateFunc.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegencomments class=md-nav__link> spark.sql.codegen.comments </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfactorymode class=md-nav__link> spark.sql.codegen.factoryMode </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenfallback class=md-nav__link> spark.sql.codegen.fallback </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenhugemethodlimit class=md-nav__link> spark.sql.codegen.hugeMethodLimit </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenuseidinclassname class=md-nav__link> spark.sql.codegen.useIdInClassName </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegenmaxfields class=md-nav__link> spark.sql.codegen.maxFields </a> </li> <li class=md-nav__item> <a href=#sparksqlcodegensplitconsumefuncbyoperator class=md-nav__link> spark.sql.codegen.splitConsumeFuncByOperator </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnvectoroffheapenabled class=md-nav__link> spark.sql.columnVector.offheap.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcolumnnameofcorruptrecord class=md-nav__link> spark.sql.columnNameOfCorruptRecord </a> </li> <li class=md-nav__item> <a href=#sparksqlconstraintpropagationenabled class=md-nav__link> spark.sql.constraintPropagation.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlcsvfilterpushdownenabled class=md-nav__link> spark.sql.csv.filterPushdown.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqldefaultsizeinbytes class=md-nav__link> spark.sql.defaultSizeInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqldialect class=md-nav__link> spark.sql.dialect </a> </li> <li class=md-nav__item> <a href=#sparksqlexchangereuse class=md-nav__link> spark.sql.exchange.reuse </a> </li> <li class=md-nav__item> <a href=#executionuseobjecthashaggregateexec class=md-nav__link> execution.useObjectHashAggregateExec </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignorecorruptfiles class=md-nav__link> spark.sql.files.ignoreCorruptFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesignoremissingfiles class=md-nav__link> spark.sql.files.ignoreMissingFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxrecordsperfile class=md-nav__link> spark.sql.files.maxRecordsPerFile </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesmaxpartitionbytes class=md-nav__link> spark.sql.files.maxPartitionBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlfilesopencostinbytes class=md-nav__link> spark.sql.files.openCostInBytes </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragecompressed class=md-nav__link> spark.sql.inMemoryColumnarStorage.compressed </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragebatchsize class=md-nav__link> spark.sql.inMemoryColumnarStorage.batchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorytablescanstatisticsenable class=md-nav__link> spark.sql.inMemoryTableScanStatistics.enable </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstorageenablevectorizedreader class=md-nav__link> spark.sql.inMemoryColumnarStorage.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlinmemorycolumnarstoragepartitionpruning class=md-nav__link> spark.sql.inMemoryColumnarStorage.partitionPruning </a> </li> <li class=md-nav__item> <a href=#sparksqljoinprefersortmergejoin class=md-nav__link> spark.sql.join.preferSortMergeJoin </a> </li> <li class=md-nav__item> <a href=#sparksqljsongeneratorignorenullfields class=md-nav__link> spark.sql.jsonGenerator.ignoreNullFields </a> </li> <li class=md-nav__item> <a href=#sparksqlleafnodedefaultparallelism class=md-nav__link> spark.sql.leafNodeDefaultParallelism </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydolooseupcast class=md-nav__link> spark.sql.legacy.doLooseUpcast </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycteprecedencepolicy class=md-nav__link> spark.sql.legacy.ctePrecedencePolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytimeparserpolicy class=md-nav__link> spark.sql.legacy.timeParserPolicy </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfollowthreevaluedlogicinarrayexists class=md-nav__link> spark.sql.legacy.followThreeValuedLogicInArrayExists </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyfromdaytimestringenabled class=md-nav__link> spark.sql.legacy.fromDayTimeString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacynotreserveproperties class=md-nav__link> spark.sql.legacy.notReserveProperties </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyaddsinglefileinaddfile class=md-nav__link> spark.sql.legacy.addSingleFileInAddFile </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyexponentliteralasdecimalenabled class=md-nav__link> spark.sql.legacy.exponentLiteralAsDecimal.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallownegativescaleofdecimal class=md-nav__link> spark.sql.legacy.allowNegativeScaleOfDecimal </a> </li> <li class=md-nav__item> <a href=#sparksqllegacybucketedtablescanoutputordering class=md-nav__link> spark.sql.legacy.bucketedTableScan.outputOrdering </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyjsonallowemptystringenabled class=md-nav__link> spark.sql.legacy.json.allowEmptyString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacycreateemptycollectionusingstringtype class=md-nav__link> spark.sql.legacy.createEmptyCollectionUsingStringType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowuntypedscalaudf class=md-nav__link> spark.sql.legacy.allowUntypedScalaUDF </a> </li> <li class=md-nav__item> <a href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue class=md-nav__link> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue </a> </li> <li class=md-nav__item> <a href=#sparksqllegacysetcommandrejectssparkcoreconfs class=md-nav__link> spark.sql.legacy.setCommandRejectsSparkCoreConfs </a> </li> <li class=md-nav__item> <a href=#sparksqllegacytypecoerciondatetimetostringenabled class=md-nav__link> spark.sql.legacy.typeCoercion.datetimeToString.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyallowhashonmaptype class=md-nav__link> spark.sql.legacy.allowHashOnMapType </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyparquetdatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.parquet.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinwrite class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInWrite </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyavrodatetimerebasemodeinread class=md-nav__link> spark.sql.legacy.avro.datetimeRebaseModeInRead </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyrddapplyconf class=md-nav__link> spark.sql.legacy.rdd.applyConf </a> </li> <li class=md-nav__item> <a href=#sparksqllegacyreplacedatabrickssparkavroenabled class=md-nav__link> spark.sql.legacy.replaceDatabricksSparkAvro.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqllimitscaleupfactor class=md-nav__link> spark.sql.limit.scaleUpFactor </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizenullawareantijoin class=md-nav__link> spark.sql.optimizeNullAwareAntiJoin </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerexcludedrules class=md-nav__link> spark.sql.optimizer.excludedRules </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerinsetconversionthreshold class=md-nav__link> spark.sql.optimizer.inSetConversionThreshold </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizermaxiterations class=md-nav__link> spark.sql.optimizer.maxIterations </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizerreplaceexceptwithfilter class=md-nav__link> spark.sql.optimizer.replaceExceptWithFilter </a> </li> <li class=md-nav__item> <a href=#sparksqloptimizernestedschemapruningenabled class=md-nav__link> spark.sql.optimizer.nestedSchemaPruning.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlorcimpl class=md-nav__link> spark.sql.orc.impl </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangeloglevel class=md-nav__link> spark.sql.planChangeLog.level </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangelogbatches class=md-nav__link> spark.sql.planChangeLog.batches </a> </li> <li class=md-nav__item> <a href=#sparksqlplanchangelogrules class=md-nav__link> spark.sql.planChangeLog.rules </a> </li> <li class=md-nav__item> <a href=#sparksqlpysparkjvmstacktraceenabled class=md-nav__link> spark.sql.pyspark.jvmStacktrace.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetbinaryasstring class=md-nav__link> spark.sql.parquet.binaryAsString </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetcolumnarreaderbatchsize class=md-nav__link> spark.sql.parquet.columnarReaderBatchSize </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96astimestamp class=md-nav__link> spark.sql.parquet.int96AsTimestamp </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetenablevectorizedreader class=md-nav__link> spark.sql.parquet.enableVectorizedReader </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdown class=md-nav__link> spark.sql.parquet.filterPushdown </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetfilterpushdowndate class=md-nav__link> spark.sql.parquet.filterPushdown.date </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetint96timestampconversion class=md-nav__link> spark.sql.parquet.int96TimestampConversion </a> </li> <li class=md-nav__item> <a href=#sparksqlparquetrecordlevelfilterenabled class=md-nav__link> spark.sql.parquet.recordLevelFilter.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlparserquotedregexcolumnnames class=md-nav__link> spark.sql.parser.quotedRegexColumnNames </a> </li> <li class=md-nav__item> <a href=#sparksqlpivotmaxvalues class=md-nav__link> spark.sql.pivotMaxValues </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionoptionsregex class=md-nav__link> spark.sql.redaction.options.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlredactionstringregex class=md-nav__link> spark.sql.redaction.string.regex </a> </li> <li class=md-nav__item> <a href=#sparksqlretaingroupcolumns class=md-nav__link> spark.sql.retainGroupColumns </a> </li> <li class=md-nav__item> <a href=#sparksqlrunsqlonfiles class=md-nav__link> spark.sql.runSQLOnFiles </a> </li> <li class=md-nav__item> <a href=#sparksqlselfjoinautoresolveambiguity class=md-nav__link> spark.sql.selfJoinAutoResolveAmbiguity </a> </li> <li class=md-nav__item> <a href=#sparksqlsortenableradixsort class=md-nav__link> spark.sql.sort.enableRadixSort </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesbucketingenabled class=md-nav__link> spark.sql.sources.bucketing.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesdefault class=md-nav__link> spark.sql.sources.default </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsfallbacktohdfs class=md-nav__link> spark.sql.statistics.fallBackToHdfs </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticshistogramnumbins class=md-nav__link> spark.sql.statistics.histogram.numBins </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled class=md-nav__link> spark.sql.statisticsparallelFileListingInStatsComputation.enabled* </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticsndvmaxerror class=md-nav__link> spark.sql.statistics.ndv.maxError </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticspercentileaccuracy class=md-nav__link> spark.sql.statistics.percentile.accuracy </a> </li> <li class=md-nav__item> <a href=#sparksqlstatisticssizeautoupdateenabled class=md-nav__link> spark.sql.statistics.size.autoUpdate.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlsubexpressioneliminationenabled class=md-nav__link> spark.sql.subexpressionElimination.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqlshufflepartitions class=md-nav__link> spark.sql.shuffle.partitions </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcesfilecompressionfactor class=md-nav__link> spark.sql.sources.fileCompressionFactor </a> </li> <li class=md-nav__item> <a href=#sparksqlsourcespartitionoverwritemode class=md-nav__link> spark.sql.sources.partitionOverwriteMode </a> </li> <li class=md-nav__item> <a href=#sparksqltruncatetableignorepermissionaclenabled class=md-nav__link> spark.sql.truncateTable.ignorePermissionAcl.enabled </a> </li> <li class=md-nav__item> <a href=#sparksqluiretainedexecutions class=md-nav__link> spark.sql.ui.retainedExecutions </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferinmemorythreshold class=md-nav__link> spark.sql.windowExec.buffer.in.memory.threshold </a> </li> <li class=md-nav__item> <a href=#sparksqlwindowexecbufferspillthreshold class=md-nav__link> spark.sql.windowExec.buffer.spill.threshold </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/jaceklaskowski/mastering-spark-sql-book/edit/main/docs/configuration-properties.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=configuration-properties>Configuration Properties<a class=headerlink href=#configuration-properties title="Permanent link">&para;</a></h1> <p><strong>Configuration properties</strong> (aka <strong>settings</strong>) allow you to fine-tune a Spark SQL application.</p> <p>Configuration properties are configured in a <a href=../SparkSession/ >SparkSession</a> while creating a new instance using <a href=../SparkSession-Builder/#config>config</a> method (e.g. <a href=../StaticSQLConf/#spark.sql.warehouse.dir>spark.sql.warehouse.dir</a>).</p> <div class=highlight><pre><span></span><code><span class=k>import</span> <span class=nn>org</span><span class=p>.</span><span class=nn>apache</span><span class=p>.</span><span class=nn>spark</span><span class=p>.</span><span class=nn>sql</span><span class=p>.</span><span class=nc>SparkSession</span>
<span class=kd>val</span> <span class=n>spark</span><span class=p>:</span> <span class=nc>SparkSession</span> <span class=o>=</span> <span class=nc>SparkSession</span><span class=p>.</span><span class=n>builder</span>
  <span class=p>.</span><span class=n>master</span><span class=p>(</span><span class=s>&quot;local[*]&quot;</span><span class=p>)</span>
  <span class=p>.</span><span class=n>appName</span><span class=p>(</span><span class=s>&quot;My Spark Application&quot;</span><span class=p>)</span>
  <span class=p>.</span><span class=n>config</span><span class=p>(</span><span class=s>&quot;spark.sql.warehouse.dir&quot;</span><span class=p>,</span> <span class=s>&quot;c:/Temp&quot;</span><span class=p>)</span> <span class=c1>// &lt;1&gt;</span>
  <span class=p>.</span><span class=n>getOrCreate</span>
</code></pre></div> <p>You can also set a property using SQL <code>SET</code> command.</p> <div class=highlight><pre><span></span><code><span class=n>assert</span><span class=p>(</span><span class=n>spark</span><span class=p>.</span><span class=n>conf</span><span class=p>.</span><span class=n>getOption</span><span class=p>(</span><span class=s>&quot;spark.sql.hive.metastore.version&quot;</span><span class=p>).</span><span class=n>isEmpty</span><span class=p>)</span>

<span class=n>scala</span><span class=o>&gt;</span> <span class=n>spark</span><span class=p>.</span><span class=n>sql</span><span class=p>(</span><span class=s>&quot;SET spark.sql.hive.metastore.version=2.3.2&quot;</span><span class=p>).</span><span class=n>show</span><span class=p>(</span><span class=n>truncate</span> <span class=o>=</span> <span class=kc>false</span><span class=p>)</span>
<span class=o>+--------------------------------+-----+</span>
<span class=o>|</span><span class=n>key</span>                             <span class=o>|</span><span class=n>value</span><span class=o>|</span>
<span class=o>+--------------------------------+-----+</span>
<span class=o>|</span><span class=n>spark</span><span class=p>.</span><span class=n>sql</span><span class=p>.</span><span class=n>hive</span><span class=p>.</span><span class=n>metastore</span><span class=p>.</span><span class=n>version</span><span class=o>|</span><span class=mf>2.3.2</span><span class=o>|</span>
<span class=o>+--------------------------------+-----+</span>

<span class=n>assert</span><span class=p>(</span><span class=n>spark</span><span class=p>.</span><span class=n>conf</span><span class=p>.</span><span class=n>get</span><span class=p>(</span><span class=s>&quot;spark.sql.hive.metastore.version&quot;</span><span class=p>)</span> <span class=o>==</span> <span class=s>&quot;2.3.2&quot;</span><span class=p>)</span>
</code></pre></div> <h2 id=sparksqladaptiveautobroadcastjointhreshold><span id=spark.sql.adaptive.autoBroadcastJoinThreshold> spark.sql.adaptive.autoBroadcastJoinThreshold<a class=headerlink href=#sparksqladaptiveautobroadcastjointhreshold title="Permanent link">&para;</a></h2> <p>The maximum size (in bytes) of a table to be broadcast when performing a join. <code>-1</code> turns broadcasting off. The default value is same as <a href=#spark.sql.autoBroadcastJoinThreshold>spark.sql.autoBroadcastJoinThreshold</a>.</p> <p>Used only in <a href=../adaptive-query-execution/ >Adaptive Query Execution</a></p> <p>Default: (undefined)</p> <p>Available as <a href=../SQLConf/#ADAPTIVE_AUTO_BROADCASTJOIN_THRESHOLD>SQLConf.ADAPTIVE_AUTO_BROADCASTJOIN_THRESHOLD</a> value.</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptivecustomcostevaluatorclass><span id=spark.sql.adaptive.customCostEvaluatorClass> spark.sql.adaptive.customCostEvaluatorClass<a class=headerlink href=#sparksqladaptivecustomcostevaluatorclass title="Permanent link">&para;</a></h2> <p>The fully-qualified class name of a custom <a href=../adaptive-query-execution/CostEvaluator/ >CostEvaluator</a> for <a href=../adaptive-query-execution/ >Adaptive Query Execution</a></p> <p>Default: <a href=../adaptive-query-execution/SimpleCostEvaluator/ >SimpleCostEvaluator</a></p> <p>Since: <code>3.2.0</code></p> <p>Use <a href=../SQLConf/#ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS>SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS</a> method to access the property (in a type-safe way).</p> <h2 id=sparksqloptimizerdecorrelateinnerqueryenabled><span id=spark.sql.optimizer.decorrelateInnerQuery.enabled> spark.sql.optimizer.decorrelateInnerQuery.enabled<a class=headerlink href=#sparksqloptimizerdecorrelateinnerqueryenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Decorrelates inner queries by eliminating correlated references and build domain joins</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#decorrelateInnerQueryEnabled>SQLConf.decorrelateInnerQueryEnabled</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptiveoptimizeskewsinrebalancepartitionsenabled><span id=spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled> spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled<a class=headerlink href=#sparksqladaptiveoptimizeskewsinrebalancepartitionsenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code> and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is <code>true</code>, Spark SQL will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a>), to avoid data skew</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#ADAPTIVE_OPTIMIZE_SKEWS_IN_REBALANCE_PARTITIONS_ENABLED>SQLConf.ADAPTIVE_OPTIMIZE_SKEWS_IN_REBALANCE_PARTITIONS_ENABLED</a> method to access the property (in a type-safe way)</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqllegacyallownonemptylocationinctas><span id=spark.sql.legacy.allowNonEmptyLocationInCTAS> spark.sql.legacy.allowNonEmptyLocationInCTAS<a class=headerlink href=#sparksqllegacyallownonemptylocationinctas title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>false</code>, CTAS with LOCATION throws an analysis exception if the location is not empty.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#allowNonEmptyLocationInCTAS>SQLConf.allowNonEmptyLocationInCTAS</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqllegacyallowautogeneratedaliasforview><span id=spark.sql.legacy.allowAutoGeneratedAliasForView> spark.sql.legacy.allowAutoGeneratedAliasForView<a class=headerlink href=#sparksqllegacyallowautogeneratedaliasforview title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, it's allowed to use an input query without explicit alias when creating a permanent view.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#allowAutoGeneratedAliasForView>SQLConf.allowAutoGeneratedAliasForView</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlsessionwindowbufferspillthreshold><span id=spark.sql.sessionWindow.buffer.spill.threshold> spark.sql.sessionWindow.buffer.spill.threshold<a class=headerlink href=#sparksqlsessionwindowbufferspillthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The threshold for number of rows to be spilled by window operator. Note that the buffer is used only for the query Spark SQL cannot apply aggregations on determining session window.</p> <p>Default: <a href=#spark.shuffle.spill.numElementsForceSpillThreshold>spark.shuffle.spill.numElementsForceSpillThreshold</a></p> <p>Use <a href=../SQLConf/#sessionWindowBufferSpillThreshold>SQLConf.sessionWindowBufferSpillThreshold</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlexecutionarrowpysparkselfdestructenabled><span id=spark.sql.execution.arrow.pyspark.selfDestruct.enabled> spark.sql.execution.arrow.pyspark.selfDestruct.enabled<a class=headerlink href=#sparksqlexecutionarrowpysparkselfdestructenabled title="Permanent link">&para;</a></h2> <p>(Experimental) When <code>true</code>, make use of Apache Arrow's self-destruct and split-blocks options for columnar data transfers in PySpark, when converting from Arrow to Pandas. This reduces memory usage at the cost of some CPU time. Applies to: <code>pyspark.sql.DataFrame.toPandas</code> when <a href=#spark.sql.execution.arrow.pyspark.enabled>spark.sql.execution.arrow.pyspark.enabled</a> is <code>true</code>.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#arrowPySparkSelfDestructEnabled>SQLConf.arrowPySparkSelfDestructEnabled</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqllegacyallowstarwithsingletableidentifierincount><span id=spark.sql.legacy.allowStarWithSingleTableIdentifierInCount> spark.sql.legacy.allowStarWithSingleTableIdentifierInCount<a class=headerlink href=#sparksqllegacyallowstarwithsingletableidentifierincount title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the SQL function <code>count</code> is allowed to take a single <code>tblName.*</code> as parameter</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#allowStarWithSingleTableIdentifierInCount>SQLConf.allowStarWithSingleTableIdentifierInCount</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizerenablecsvexpressionoptimization><span id=spark.sql.optimizer.enableCsvExpressionOptimization> spark.sql.optimizer.enableCsvExpressionOptimization<a class=headerlink href=#sparksqloptimizerenablecsvexpressionoptimization title="Permanent link">&para;</a></h2> <p>Whether to optimize CSV expressions in SQL optimizer. It includes pruning unnecessary columns from <code>from_csv</code>.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#csvExpressionOptimization>SQLConf.csvExpressionOptimization</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlsessionwindowbufferinmemorythreshold><span id=spark.sql.sessionWindow.buffer.in.memory.threshold> spark.sql.sessionWindow.buffer.in.memory.threshold<a class=headerlink href=#sparksqlsessionwindowbufferinmemorythreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Threshold for number of windows guaranteed to be held in memory by the session window operator. Note that the buffer is used only for the query Spark SQL cannot apply aggregations on determining session window.</p> <p>Default: <code>4096</code></p> <p>Use <a href=../SQLConf/#sessionWindowBufferInMemoryThreshold>SQLConf.sessionWindowBufferInMemoryThreshold</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlorcenablenestedcolumnvectorizedreader><span id=spark.sql.orc.enableNestedColumnVectorizedReader> spark.sql.orc.enableNestedColumnVectorizedReader<a class=headerlink href=#sparksqlorcenablenestedcolumnvectorizedreader title="Permanent link">&para;</a></h2> <p>Enables vectorized orc decoding for nested column</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#orcVectorizedReaderNestedColumnEnabled>SQLConf.orcVectorizedReaderNestedColumnEnabled</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizeroptimizeonerowrelationsubquery><span id=spark.sql.optimizer.optimizeOneRowRelationSubquery> spark.sql.optimizer.optimizeOneRowRelationSubquery<a class=headerlink href=#sparksqloptimizeroptimizeonerowrelationsubquery title="Permanent link">&para;</a></h2> <p><strong>(innternal)</strong> When <code>true</code>, the optimizer will inline subqueries with <a href=../logical-operators/OneRowRelation/ >OneRowRelation</a> as leaf nodes</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#OPTIMIZE_ONE_ROW_RELATION_SUBQUERY>SQLConf.OPTIMIZE_ONE_ROW_RELATION_SUBQUERY</a> method to access the property (in a type-safe way)</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptiveforceapply><span id=spark.sql.adaptive.forceApply> spark.sql.adaptive.forceApply<a class=headerlink href=#sparksqladaptiveforceapply title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code> (together with <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> enabled), Spark will <a href=../adaptive-query-execution/InsertAdaptiveSparkPlan/#shouldApplyAQE>force apply adaptive query execution for all supported queries</a>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#ADAPTIVE_EXECUTION_FORCE_APPLY>SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY</a> method to access the property (in a type-safe way).</p> <h2 id=sparksqladaptivecoalescepartitionsenabled><span id=spark.sql.adaptive.coalescePartitions.enabled> spark.sql.adaptive.coalescePartitions.enabled<a class=headerlink href=#sparksqladaptivecoalescepartitionsenabled title="Permanent link">&para;</a></h2> <p>Controls coalescing shuffle partitions</p> <p>When <code>true</code> and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark will coalesce contiguous shuffle partitions according to the target size (specified by <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a>), to avoid too many small tasks.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#coalesceShufflePartitionsEnabled>SQLConf.coalesceShufflePartitionsEnabled</a> method to access the current value.</p> <h2 id=sparksqladaptivecoalescepartitionsminpartitionsize><span id=spark.sql.adaptive.coalescePartitions.minPartitionSize> spark.sql.adaptive.coalescePartitions.minPartitionSize<a class=headerlink href=#sparksqladaptivecoalescepartitionsminpartitionsize title="Permanent link">&para;</a></h2> <p>The minimum size (in bytes unless specified) of shuffle partitions after coalescing. This is useful when the adaptively calculated target size is too small during partition coalescing</p> <p>Default: <code>1MB</code></p> <p>Use <a href=../SQLConf/#coalesceShufflePartitionsEnabled>SQLConf.coalesceShufflePartitionsEnabled</a> method to access the current value.</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptivecoalescepartitionsparallelismfirst><span id=spark.sql.adaptive.coalescePartitions.parallelismFirst> spark.sql.adaptive.coalescePartitions.parallelismFirst<a class=headerlink href=#sparksqladaptivecoalescepartitionsparallelismfirst title="Permanent link">&para;</a></h2> <p>When <code>true</code>, Spark does not respect the target size specified by <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a> when coalescing contiguous shuffle partitions, but adaptively calculate the target size according to the default parallelism of the Spark cluster. The calculated size is usually smaller than the configured target size. This is to maximize the parallelism and avoid performance regression when enabling adaptive query execution. It's recommended to set this config to <code>false</code> and respect the configured target size.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#coalesceShufflePartitionsEnabled>SQLConf.coalesceShufflePartitionsEnabled</a> method to access the current value.</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptiveadvisorypartitionsizeinbytes><span id=spark.sql.adaptive.advisoryPartitionSizeInBytes> spark.sql.adaptive.advisoryPartitionSizeInBytes<a class=headerlink href=#sparksqladaptiveadvisorypartitionsizeinbytes title="Permanent link">&para;</a></h2> <p>The advisory size in bytes of the shuffle partition during adaptive optimization (when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.</p> <p>Default: <code>64MB</code></p> <p>Since: <code>3.0.0</code></p> <p>Fallback Property: <code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code></p> <p>Use <a href=../SQLConf/#ADVISORY_PARTITION_SIZE_IN_BYTES>SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES</a> to reference the name.</p> <h2 id=sparksqladaptivecoalescepartitionsminpartitionnum><span id=spark.sql.adaptive.coalescePartitions.minPartitionNum> spark.sql.adaptive.coalescePartitions.minPartitionNum<a class=headerlink href=#sparksqladaptivecoalescepartitionsminpartitionnum title="Permanent link">&para;</a></h2> <p>The minimum number of shuffle partitions after coalescing. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptivecoalescepartitionsinitialpartitionnum><span id=spark.sql.adaptive.coalescePartitions.initialPartitionNum> spark.sql.adaptive.coalescePartitions.initialPartitionNum<a class=headerlink href=#sparksqladaptivecoalescepartitionsinitialpartitionnum title="Permanent link">&para;</a></h2> <p>The initial number of shuffle partitions before coalescing.</p> <p>By default it equals to <a href=#spark.sql.shuffle.partitions>spark.sql.shuffle.partitions</a>. If not set, the default value is the default parallelism of the Spark cluster. This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveenabled><span id=spark.sql.adaptive.enabled> spark.sql.adaptive.enabled<a class=headerlink href=#sparksqladaptiveenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../adaptive-query-execution/ >Adaptive Query Execution</a></p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#adaptiveExecutionEnabled>SQLConf.adaptiveExecutionEnabled</a> method to access the current value.</p> <h2 id=sparksqladaptivefetchshuffleblocksinbatch><span id=spark.sql.adaptive.fetchShuffleBlocksInBatch> spark.sql.adaptive.fetchShuffleBlocksInBatch<a class=headerlink href=#sparksqladaptivefetchshuffleblocksinbatch title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether to fetch the contiguous shuffle blocks in batch. Instead of fetching blocks one by one, fetching contiguous shuffle blocks for the same map task in batch can reduce IO and improve performance. Note, multiple contiguous blocks exist in single "fetch request only happen when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.adaptive.coalescePartitions.enabled>spark.sql.adaptive.coalescePartitions.enabled</a> are both enabled. This feature also depends on a relocatable serializer, the concatenation support codec in use and the new version shuffle fetch protocol.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#fetchShuffleBlocksInBatch>SQLConf.fetchShuffleBlocksInBatch</a> method to access the current value.</p> <h2 id=sparksqladaptivelocalshufflereaderenabled><span id=spark.sql.adaptive.localShuffleReader.enabled> spark.sql.adaptive.localShuffleReader.enabled<a class=headerlink href=#sparksqladaptivelocalshufflereaderenabled title="Permanent link">&para;</a></h2> <p>When true and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveloglevel><span id=spark.sql.adaptive.logLevel> spark.sql.adaptive.logLevel<a class=headerlink href=#sparksqladaptiveloglevel title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Log level for adaptive execution logging of plan changes. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>DEBUG</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#adaptiveExecutionLogLevel>SQLConf.adaptiveExecutionLogLevel</a> method to access the current value.</p> <h2 id=sparksqladaptivemaxshuffledhashjoinlocalmapthreshold><span id=spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold> spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold<a class=headerlink href=#sparksqladaptivemaxshuffledhashjoinlocalmapthreshold title="Permanent link">&para;</a></h2> <p>The maximum size (in bytes) per partition that can be allowed to build local hash map. If this value is not smaller than <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a> and all the partition size are not larger than this config, join selection prefer to use shuffled hash join instead of sort merge join regardless of the value of <a href=#spark.sql.join.preferSortMergeJoin>spark.sql.join.preferSortMergeJoin</a>.</p> <p>Default: <code>0</code></p> <p>Available as <a href=../SQLConf/#ADAPTIVE_MAX_SHUFFLE_HASH_JOIN_LOCAL_MAP_THRESHOLD>SQLConf.ADAPTIVE_MAX_SHUFFLE_HASH_JOIN_LOCAL_MAP_THRESHOLD</a></p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqladaptiveoptimizerexcludedrules><span id=spark.sql.adaptive.optimizer.excludedRules><span id=ADAPTIVE_OPTIMIZER_EXCLUDED_RULES> spark.sql.adaptive.optimizer.excludedRules<a class=headerlink href=#sparksqladaptiveoptimizerexcludedrules title="Permanent link">&para;</a></h2> <p>A comma-separated list of rules (names) to be disabled in the <a href=../adaptive-query-execution/AQEOptimizer/ >adaptive optimizer</a></p> <p>Default: undefined</p> <p>Use <a href=../SQLConf/#ADAPTIVE_OPTIMIZER_EXCLUDED_RULES>SQLConf.ADAPTIVE_OPTIMIZER_EXCLUDED_RULES</a> to reference the property.</p> <h2 id=sparksqladaptiveskewjoinenabled><span id=spark.sql.adaptive.skewJoin.enabled> spark.sql.adaptive.skewJoin.enabled<a class=headerlink href=#sparksqladaptiveskewjoinenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code> and <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is enabled, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#SKEW_JOIN_ENABLED>SQLConf.SKEW_JOIN_ENABLED</a> to reference the property.</p> <h2 id=sparksqladaptiveskewjoinskewedpartitionfactor><span id=spark.sql.adaptive.skewJoin.skewedPartitionFactor> spark.sql.adaptive.skewJoin.skewedPartitionFactor<a class=headerlink href=#sparksqladaptiveskewjoinskewedpartitionfactor title="Permanent link">&para;</a></h2> <p>A partition is considered skewed if its size is larger than this factor multiplying the median partition size and also larger than <a href=#spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</a>.</p> <p>Default: <code>5</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptiveskewjoinskewedpartitionthresholdinbytes><span id=spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes> spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes<a class=headerlink href=#sparksqladaptiveskewjoinskewedpartitionthresholdinbytes title="Permanent link">&para;</a></h2> <p>A partition is considered skewed if its size in bytes is larger than this threshold and also larger than <a href=#spark.sql.adaptive.skewJoin.skewedPartitionFactor>spark.sql.adaptive.skewJoin.skewedPartitionFactor</a> multiplying the median partition size. Ideally this config should be set larger than <a href=#spark.sql.adaptive.advisoryPartitionSizeInBytes>spark.sql.adaptive.advisoryPartitionSizeInBytes</a>.</p> <p>Default: <code>256MB</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladaptivenonemptypartitionratioforbroadcastjoin><span id=spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin> spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin<a class=headerlink href=#sparksqladaptivenonemptypartitionratioforbroadcastjoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A relation with a non-empty partition ratio (the number of non-empty partitions to all partitions) lower than this config will not be considered as the build side of a broadcast-hash join in <a href=../adaptive-query-execution/ >Adaptive Query Execution</a> regardless of the size.</p> <p>This configuration only has an effect when <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> is <code>true</code>.</p> <p>Default: <code>0.2</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#nonEmptyPartitionRatioForBroadcastJoin>SQLConf.nonEmptyPartitionRatioForBroadcastJoin</a> method to access the current value.</p> <h2 id=sparksqlanalyzermaxiterations><span id=spark.sql.analyzer.maxIterations> spark.sql.analyzer.maxIterations<a class=headerlink href=#sparksqlanalyzermaxiterations title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The max number of iterations the analyzer runs.</p> <p>Default: <code>100</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlanalyzerfailambiguousselfjoin><span id=spark.sql.analyzer.failAmbiguousSelfJoin> spark.sql.analyzer.failAmbiguousSelfJoin<a class=headerlink href=#sparksqlanalyzerfailambiguousselfjoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, fail the Dataset query if it contains ambiguous self-join.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlansienabled><span id=spark.sql.ansi.enabled> spark.sql.ansi.enabled<a class=headerlink href=#sparksqlansienabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, Spark tries to conform to the ANSI SQL specification:</p> <ol> <li>Spark will throw a runtime exception if an overflow occurs in any operation on integral/decimal field.</li> <li>Spark will forbid using the reserved keywords of ANSI SQL as identifiers in the SQL parser.</li> </ol> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcliprintheader><span id=spark.sql.cli.print.header> spark.sql.cli.print.header<a class=headerlink href=#sparksqlcliprintheader title="Permanent link">&para;</a></h2> <p>When <code>true</code>, spark-sql CLI prints the names of the columns in query output</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#cliPrintHeader>SQLConf.cliPrintHeader</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlcodegenwholestage><span id=spark.sql.codegen.wholeStage> spark.sql.codegen.wholeStage<a class=headerlink href=#sparksqlcodegenwholestage title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether the whole stage (of multiple physical operators) will be compiled into a single Java method (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageEnabled>SQLConf.wholeStageEnabled</a> method to access the current value.</p> <h2 id=sparksqlcodegenmethodsplitthreshold><span id=spark.sql.codegen.methodSplitThreshold> spark.sql.codegen.methodSplitThreshold<a class=headerlink href=#sparksqlcodegenmethodsplitthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The threshold of source-code splitting in the codegen. When the number of characters in a single Java function (without comment) exceeds the threshold, the function will be automatically split to multiple smaller ones. We cannot know how many bytecode will be generated, so use the code length as metric. When running on HotSpot, a function's bytecode should not go beyond 8KB, otherwise it will not be JITted; it also should not be too small, otherwise there will be many function calls.</p> <p>Default: <code>1024</code></p> <p>Use <a href=../SQLConf/#methodSplitThreshold>SQLConf.methodSplitThreshold</a> for the current value</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqldebugmaxtostringfields><span id=spark.sql.debug.maxToStringFields> spark.sql.debug.maxToStringFields<a class=headerlink href=#sparksqldebugmaxtostringfields title="Permanent link">&para;</a></h2> <p>Maximum number of fields of sequence-like entries can be converted to strings in debug output. Any elements beyond the limit will be dropped and replaced by a "... N more fields" placeholder.</p> <p>Default: <code>25</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#maxToStringFields>SQLConf.maxToStringFields</a> method to access the current value.</p> <h2 id=sparksqldefaultcatalog><span id=spark.sql.defaultCatalog> spark.sql.defaultCatalog<a class=headerlink href=#sparksqldefaultcatalog title="Permanent link">&para;</a></h2> <p>Name of the default catalog</p> <p>Default: <a href=../connector/catalog/CatalogManager/#SESSION_CATALOG_NAME>spark_catalog</a></p> <p>Use <a href=../SQLConf/#DEFAULT_CATALOG>SQLConf.DEFAULT_CATALOG</a> to access the current value.</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionarrowpysparkenabled><span id=spark.sql.execution.arrow.pyspark.enabled> spark.sql.execution.arrow.pyspark.enabled<a class=headerlink href=#sparksqlexecutionarrowpysparkenabled title="Permanent link">&para;</a></h2> <p>When true, make use of Apache Arrow for columnar data transfers in PySpark. This optimization applies to:</p> <ol> <li>pyspark.sql.DataFrame.toPandas</li> <li>pyspark.sql.SparkSession.createDataFrame when its input is a Pandas DataFrame</li> </ol> <p>The following data types are unsupported: BinaryType, MapType, <a href=../types/ArrayType/ >ArrayType</a> of TimestampType, and nested StructType.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionremoveredundantsorts><span id=spark.sql.execution.removeRedundantSorts> spark.sql.execution.removeRedundantSorts<a class=headerlink href=#sparksqlexecutionremoveredundantsorts title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether to remove redundant physical sort node</p> <p>Default: <code>true</code></p> <p>Used as <a href=../SQLConf/#REMOVE_REDUNDANT_SORTS_ENABLED>SQLConf.REMOVE_REDUNDANT_SORTS_ENABLED</a></p> <h2 id=sparksqlexecutionreusesubquery><span id=spark.sql.execution.reuseSubquery> spark.sql.execution.reuseSubquery<a class=headerlink href=#sparksqlexecutionreusesubquery title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the planner will try to find out duplicated subqueries and re-use them.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionsortbeforerepartition><span id=spark.sql.execution.sortBeforeRepartition> spark.sql.execution.sortBeforeRepartition<a class=headerlink href=#sparksqlexecutionsortbeforerepartition title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When perform a repartition following a shuffle, the output row ordering would be nondeterministic. If some downstream stages fail and some tasks of the repartition stage retry, these tasks may generate different data, and that can lead to correctness issues. Turn on this config to insert a local sort before actually doing repartition to generate consistent repartition results. The performance of <code>repartition()</code> may go down since we insert extra local sort before it.</p> <p>Default: <code>true</code></p> <p>Since: <code>2.1.4</code></p> <p>Use <a href=../SQLConf/#sortBeforeRepartition>SQLConf.sortBeforeRepartition</a> method to access the current value.</p> <h2 id=sparksqlexecutionrangeexchangesamplesizeperpartition><span id=spark.sql.execution.rangeExchange.sampleSizePerPartition> spark.sql.execution.rangeExchange.sampleSizePerPartition<a class=headerlink href=#sparksqlexecutionrangeexchangesamplesizeperpartition title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Number of points to sample per partition in order to determine the range boundaries for range partitioning, typically used in global sorting (without limit).</p> <p>Default: <code>100</code></p> <p>Since: <code>2.3.0</code></p> <p>Use <a href=../SQLConf/#rangeExchangeSampleSizePerPartition>SQLConf.rangeExchangeSampleSizePerPartition</a> method to access the current value.</p> <h2 id=sparksqlexecutionarrowpysparkfallbackenabled><span id=spark.sql.execution.arrow.pyspark.fallback.enabled> spark.sql.execution.arrow.pyspark.fallback.enabled<a class=headerlink href=#sparksqlexecutionarrowpysparkfallbackenabled title="Permanent link">&para;</a></h2> <p>When true, optimizations enabled by <a href=#spark.sql.execution.arrow.pyspark.enabled>spark.sql.execution.arrow.pyspark.enabled</a> will fallback automatically to non-optimized implementations if an error occurs.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionarrowsparkrenabled><span id=spark.sql.execution.arrow.sparkr.enabled> spark.sql.execution.arrow.sparkr.enabled<a class=headerlink href=#sparksqlexecutionarrowsparkrenabled title="Permanent link">&para;</a></h2> <p>When true, make use of Apache Arrow for columnar data transfers in SparkR. This optimization applies to:</p> <ol> <li>createDataFrame when its input is an R DataFrame</li> <li>collect</li> <li>dapply</li> <li>gapply</li> </ol> <p>The following data types are unsupported: FloatType, BinaryType, <a href=../types/ArrayType/ >ArrayType</a>, StructType and MapType.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionpandasudfbuffersize><span id=spark.sql.execution.pandas.udf.buffer.size> spark.sql.execution.pandas.udf.buffer.size<a class=headerlink href=#sparksqlexecutionpandasudfbuffersize title="Permanent link">&para;</a></h2> <p>Same as <code>${BUFFER_SIZE.key}</code> but only applies to Pandas UDF executions. If it is not set, the fallback is <code>${BUFFER_SIZE.key}</code>. Note that Pandas execution requires more than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and pipelined; however, it might degrade performance. See SPARK-27870.</p> <p>Default: <code>65536</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlexecutionpandasconverttoarrowarraysafely><span id=spark.sql.execution.pandas.convertToArrowArraySafely> spark.sql.execution.pandas.convertToArrowArraySafely<a class=headerlink href=#sparksqlexecutionpandasconverttoarrowarraysafely title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, Arrow will perform safe type conversion when converting Pandas. Series to Arrow array during serialization. Arrow will raise errors when detecting unsafe type conversion like overflow. When false, disabling Arrow's type check and do type conversions anyway. This config only works for Arrow 0.11.0+.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlstatisticshistogramenabled><span id=spark.sql.statistics.histogram.enabled> spark.sql.statistics.histogram.enabled<a class=headerlink href=#sparksqlstatisticshistogramenabled title="Permanent link">&para;</a></h2> <p>Enables generating histograms for <a href=../sql/AstBuilder/#visitAnalyze>ANALYZE TABLE</a> SQL statement</p> <p>Default: <code>false</code></p> <div class="admonition note"> <p class=admonition-title>Equi-Height Histogram</p> <p>Histograms can provide better estimation accuracy. Currently, Spark only supports equi-height histogram. Note that collecting histograms takes extra cost. For example, collecting column statistics usually takes only one table scan, but generating equi-height histogram will cause an extra table scan.</p> </div> <p>Use <a href=../SQLConf/#histogramEnabled>SQLConf.histogramEnabled</a> method to access the current value.</p> <h2 id=sparksqlsessiontimezone><span id=spark.sql.session.timeZone> spark.sql.session.timeZone<a class=headerlink href=#sparksqlsessiontimezone title="Permanent link">&para;</a></h2> <p>The ID of session-local timezone (e.g. "GMT", "America/Los_Angeles")</p> <p>Default: Java's <code>TimeZone.getDefault.getID</code></p> <p>Use <a href=../SQLConf/#sessionLocalTimeZone>SQLConf.sessionLocalTimeZone</a> method to access the current value.</p> <h2 id=sparksqlsourcescommitprotocolclass><span id=spark.sql.sources.commitProtocolClass> spark.sql.sources.commitProtocolClass<a class=headerlink href=#sparksqlsourcescommitprotocolclass title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Fully-qualified class name of the <code>FileCommitProtocol</code></p> <p>Default: <a href=../SQLHadoopMapReduceCommitProtocol/ >SQLHadoopMapReduceCommitProtocol</a></p> <p>Use <a href=../SQLConf/#fileCommitProtocolClass>SQLConf.fileCommitProtocolClass</a> method to access the current value.</p> <h2 id=sparksqlsourcesignoredatalocality><span id=spark.sql.sources.ignoreDataLocality> spark.sql.sources.ignoreDataLocality<a class=headerlink href=#sparksqlsourcesignoredatalocality title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, Spark will not fetch the block locations for each file on listing files. This speeds up file listing, but the scheduler cannot schedule tasks to take advantage of data locality. It can be particularly useful if data is read from a remote cluster so the scheduler could never take advantage of locality anyway.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesvalidatepartitioncolumns><span id=spark.sql.sources.validatePartitionColumns> spark.sql.sources.validatePartitionColumns<a class=headerlink href=#sparksqlsourcesvalidatepartitioncolumns title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When this option is set to true, partition column values will be validated with user-specified schema. If the validation fails, a runtime exception is thrown. When this option is set to false, the partition column value will be converted to null if it can not be casted to corresponding user-specified schema.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesusev1sourcelist><span id=spark.sql.sources.useV1SourceList><span id=USE_V1_SOURCE_LIST> spark.sql.sources.useV1SourceList<a class=headerlink href=#sparksqlsourcesusev1sourcelist title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A comma-separated list of data source short names (<a href=../DataSourceRegister/ >DataSourceRegister</a>s) or fully-qualified canonical class names of the data sources (<a href=../connector/TableProvider/ >TableProvider</a>s) for which DataSource V2 code path is disabled (and Data Source V1 code path used).</p> <p>Default: <code>avro,csv,json,kafka,orc,parquet,text</code></p> <p>Since: <code>3.0.0</code></p> <p>Used when:</p> <ul> <li><code>DataSource</code> utility is used to <a href=../DataSource/#lookupDataSourceV2>lookupDataSourceV2</a></li> </ul> <h2 id=sparksqlstoreassignmentpolicy><span id=spark.sql.storeAssignmentPolicy> spark.sql.storeAssignmentPolicy<a class=headerlink href=#sparksqlstoreassignmentpolicy title="Permanent link">&para;</a></h2> <p>When inserting a value into a column with different data type, Spark will perform type coercion. Currently, we support 3 policies for the type coercion rules: ANSI, legacy and strict. With ANSI policy, Spark performs the type coercion as per ANSI SQL. In practice, the behavior is mostly the same as PostgreSQL. It disallows certain unreasonable type conversions such as converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code>. With legacy policy, Spark allows the type coercion as long as it is a valid <code>Cast</code>, which is very loose. e.g. converting <code>string</code> to <code>int</code> or <code>double</code> to <code>boolean</code> is allowed. It is also the only behavior in Spark 2.x and it is compatible with Hive. With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion, e.g. converting <code>double</code> to <code>int</code> or <code>decimal</code> to <code>double</code> is not allowed.</p> <p>Possible values: <code>ANSI</code>, <code>LEGACY</code>, <code>STRICT</code></p> <p>Default: <code>ANSI</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlthriftserverinterruptoncancel><span id=spark.sql.thriftServer.interruptOnCancel> spark.sql.thriftServer.interruptOnCancel<a class=headerlink href=#sparksqlthriftserverinterruptoncancel title="Permanent link">&para;</a></h2> <p>When <code>true</code>, all running tasks will be interrupted if one cancels a query. When <code>false</code>, all running tasks will remain until finished.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#THRIFTSERVER_FORCE_CANCEL>SQLConf.THRIFTSERVER_FORCE_CANCEL</a> to access the property</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizerinsetswitchthreshold><span id=spark.sql.optimizer.inSetSwitchThreshold> spark.sql.optimizer.inSetSwitchThreshold<a class=headerlink href=#sparksqloptimizerinsetswitchthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures the max set size in InSet for which Spark will generate code with switch statements. This is applicable only to bytes, shorts, ints, dates.</p> <p>Must be non-negative and less than or equal to 600.</p> <p>Default: <code>400</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangeloglevel><span id=spark.sql.optimizer.planChangeLog.level> spark.sql.optimizer.planChangeLog.level<a class=headerlink href=#sparksqloptimizerplanchangeloglevel title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures the log level for logging the change from the original plan to the new plan after a rule or batch is applied. The value can be <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code> or <code>ERROR</code>.</p> <p>Default: <code>TRACE</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangelogrules><span id=spark.sql.optimizer.planChangeLog.rules> spark.sql.optimizer.planChangeLog.rules<a class=headerlink href=#sparksqloptimizerplanchangelogrules title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures a list of rules to be logged in the optimizer, in which the rules are specified by their rule names and separated by comma.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerplanchangelogbatches><span id=spark.sql.optimizer.planChangeLog.batches> spark.sql.optimizer.planChangeLog.batches<a class=headerlink href=#sparksqloptimizerplanchangelogbatches title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Configures a list of batches to be logged in the optimizer, in which the batches are specified by their batch names and separated by comma.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerdynamicpartitionpruningenabled><span id=spark.sql.optimizer.dynamicPartitionPruning.enabled> spark.sql.optimizer.dynamicPartitionPruning.enabled<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, Spark SQL will generate predicate for partition column used as a join key.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningEnabled>SQLConf.dynamicPartitionPruningEnabled</a> to access the current value.</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlhivetablepropertylengththreshold><span id=spark.sql.hive.tablePropertyLengthThreshold> spark.sql.hive.tablePropertyLengthThreshold<a class=headerlink href=#sparksqlhivetablepropertylengththreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The maximum length allowed in a single cell when storing Spark-specific information in Hive's metastore as table properties. Currently it covers 2 things: the schema's JSON string, the histogram of column statistics.</p> <p>Default: (undefined)</p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningEnabled>SQLConf.dynamicPartitionPruningEnabled</a> to access the current value.</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizercanchangecachedplanoutputpartitioning><span id=spark.sql.optimizer.canChangeCachedPlanOutputPartitioning> spark.sql.optimizer.canChangeCachedPlanOutputPartitioning<a class=headerlink href=#sparksqloptimizercanchangecachedplanoutputpartitioning title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether to forcibly enable some optimization rules that can change the output partitioning of a cached query when executing it for caching. If it is set to true, queries may need an extra shuffle to read the cached data. This configuration is disabled by default. Currently, the optimization rules enabled by this configuration are <a href=#spark.sql.adaptive.enabled>spark.sql.adaptive.enabled</a> and <a href=#spark.sql.sources.bucketing.autoBucketedScan.enabled>spark.sql.sources.bucketing.autoBucketedScan.enabled</a>.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#CAN_CHANGE_CACHED_PLAN_OUTPUT_PARTITIONING>SQLConf.CAN_CHANGE_CACHED_PLAN_OUTPUT_PARTITIONING</a> to access the property</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizerdynamicpartitionpruningpruningsideextrafilterratio><span id=spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio> spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningpruningsideextrafilterratio title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When filtering side doesn't support broadcast by join type, and doing DPP means running an extra query that may have significant overhead. This config will be used as the extra filter ratio for computing the data size of the pruning side after DPP, in order to evaluate if it is worth adding an extra subquery as the pruning filter.</p> <p>Must be a double between <code>0.0</code> and <code>1.0</code></p> <p>Default: <code>0.04</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningPruningSideExtraFilterRatio>SQLConf.dynamicPartitionPruningPruningSideExtraFilterRatio</a> to access the current value.</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqloptimizerdynamicpartitionpruningusestats><span id=spark.sql.optimizer.dynamicPartitionPruning.useStats> spark.sql.optimizer.dynamicPartitionPruning.useStats<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningusestats title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, distinct count statistics will be used for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningUseStats>SQLConf.dynamicPartitionPruningUseStats</a> method to access the current value.</p> <h2 id=sparksqloptimizerdynamicpartitionpruningfallbackfilterratio><span id=spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio> spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningfallbackfilterratio title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When statistics are not available or configured not to be used, this config will be used as the fallback filter ratio for computing the data size of the partitioned table after dynamic partition pruning, in order to evaluate if it is worth adding an extra subquery as the pruning filter if broadcast reuse is not applicable.</p> <p>Default: <code>0.5</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningFallbackFilterRatio>SQLConf.dynamicPartitionPruningFallbackFilterRatio</a> method to access the current value.</p> <h2 id=sparksqloptimizerdynamicpartitionpruningreusebroadcastonly><span id=spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly> spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly<a class=headerlink href=#sparksqloptimizerdynamicpartitionpruningreusebroadcastonly title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, dynamic partition pruning will only apply when the broadcast exchange of a broadcast hash join operation can be reused as the dynamic pruning filter.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <p>Use <a href=../SQLConf/#dynamicPartitionPruningReuseBroadcastOnly>SQLConf.dynamicPartitionPruningReuseBroadcastOnly</a> method to access the current value.</p> <h2 id=sparksqloptimizernestedpredicatepushdownsupportedfilesources><span id=spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources> spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources<a class=headerlink href=#sparksqloptimizernestedpredicatepushdownsupportedfilesources title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> A comma-separated list of data source short names or fully qualified data source implementation class names for which Spark tries to push down predicates for nested columns and/or names containing <code>dots</code> to data sources. This configuration is only effective with file-based data source in DSv1. Currently, Parquet implements both optimizations while ORC only supports predicates for names containing <code>dots</code>. The other data sources don't support this feature yet.</p> <p>Default: <code>parquet,orc</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerserializernestedschemapruningenabled><span id=spark.sql.optimizer.serializer.nestedSchemaPruning.enabled> spark.sql.optimizer.serializer.nestedSchemaPruning.enabled<a class=headerlink href=#sparksqloptimizerserializernestedschemapruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from object serialization operator which are unnecessary in satisfying a query. This optimization allows object serializers to avoid executing unnecessary nested expressions.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqloptimizerexpressionnestedpruningenabled><span id=spark.sql.optimizer.expression.nestedPruning.enabled> spark.sql.optimizer.expression.nestedPruning.enabled<a class=headerlink href=#sparksqloptimizerexpressionnestedpruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from expressions in an operator which are unnecessary in satisfying a query. Note that this optimization doesn't prune nested fields from physical data source scanning. For pruning nested fields from scanning, please use <a href=#spark.sql.optimizer.nestedSchemaPruning.enabled>spark.sql.optimizer.nestedSchemaPruning.enabled</a> config.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlorcmergeschema><span id=spark.sql.orc.mergeSchema> spark.sql.orc.mergeSchema<a class=headerlink href=#sparksqlorcmergeschema title="Permanent link">&para;</a></h2> <p>When true, the Orc data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlsourcesbucketingautobucketedscanenabled><span id=spark.sql.sources.bucketing.autoBucketedScan.enabled> spark.sql.sources.bucketing.autoBucketedScan.enabled<a class=headerlink href=#sparksqlsourcesbucketingautobucketedscanenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, decide whether to do bucketed scan on input tables based on query plan automatically. Do not use bucketed scan if 1. query does not have operators to utilize bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these operators and table scan.</p> <p>Note when <a href=#spark.sql.sources.bucketing.enabled>spark.sql.sources.bucketing.enabled</a> is set to <code>false</code>, this configuration does not take any effect.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#autoBucketedScanEnabled>SQLConf.autoBucketedScanEnabled</a> to access the property</p> <p>Since: <code>3.1.0</code></p> <h2 id=sparksqldatetimejava8apienabled><span id=spark.sql.datetime.java8API.enabled> spark.sql.datetime.java8API.enabled<a class=headerlink href=#sparksqldatetimejava8apienabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, java.time.Instant and java.time.LocalDate classes of Java 8 API are used as external types for Catalyst's TimestampType and DateType. When <code>false</code>, java.sql.Timestamp and java.sql.Date are used for the same purpose.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyintervalenabled><span id=spark.sql.legacy.interval.enabled> spark.sql.legacy.interval.enabled<a class=headerlink href=#sparksqllegacyintervalenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, Spark SQL uses the mixed legacy interval type <code>CalendarIntervalType</code> instead of the ANSI compliant interval types <code>YearMonthIntervalType</code> and <code>DayTimeIntervalType</code>. For instance, the date subtraction expression returns <code>CalendarIntervalType</code> when the SQL config is set to <code>true</code> otherwise an ANSI interval.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#legacyIntervalEnabled>SQLConf.legacyIntervalEnabled</a> to access the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlsourcesbinaryfilemaxlength><span id=spark.sql.sources.binaryFile.maxLength> spark.sql.sources.binaryFile.maxLength<a class=headerlink href=#sparksqlsourcesbinaryfilemaxlength title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The max length of a file that can be read by the binary file data source. Spark will fail fast and not attempt to read the file if its length exceeds this value. The theoretical max is Int.MaxValue, though VMs might implement a smaller max.</p> <p>Default: <code>Int.MaxValue</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmapkeydeduppolicy><span id=spark.sql.mapKeyDedupPolicy> spark.sql.mapKeyDedupPolicy<a class=headerlink href=#sparksqlmapkeydeduppolicy title="Permanent link">&para;</a></h2> <p>The policy to deduplicate map keys in builtin function: CreateMap, MapFromArrays, MapFromEntries, StringToMap, MapConcat and TransformKeys. When EXCEPTION, the query fails if duplicated map keys are detected. When LAST_WIN, the map key that is inserted at last takes precedence.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LAST_WIN</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmaxconcurrentoutputfilewriters><span id=spark.sql.maxConcurrentOutputFileWriters> spark.sql.maxConcurrentOutputFileWriters<a class=headerlink href=#sparksqlmaxconcurrentoutputfilewriters title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Maximum number of output file writers for <code>FileFormatWriter</code> to use concurrently (<a href=../FileFormatWriter/#write>writing out a query result</a>). If number of writers needed reaches this limit, a task will sort rest of output then writing them.</p> <p>Default: <code>0</code></p> <p>Use <a href=../SQLConf/#maxConcurrentOutputFileWriters>SQLConf.maxConcurrentOutputFileWriters</a> for the current value</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqlmaxmetadatastringlength><span id=spark.sql.maxMetadataStringLength> spark.sql.maxMetadataStringLength<a class=headerlink href=#sparksqlmaxmetadatastringlength title="Permanent link">&para;</a></h2> <p>Maximum number of characters to output for a metadata string (e.g. file location in <a href=../physical-operators/DataSourceScanExec/ >DataSourceScanExec</a>), every value will be abbreviated if exceed length.</p> <p>Must be bigger than 3</p> <p>Default: <code>100</code></p> <p>Use <a href=../SQLConf/#maxMetadataStringLength>SQLConf.maxMetadataStringLength</a> method to access the current value.</p> <h2 id=sparksqlmavenadditionalremoterepositories><span id=spark.sql.maven.additionalRemoteRepositories> spark.sql.maven.additionalRemoteRepositories<a class=headerlink href=#sparksqlmavenadditionalremoterepositories title="Permanent link">&para;</a></h2> <p>A comma-delimited string config of the optional additional remote Maven mirror repositories. This is only used for downloading Hive jars in IsolatedClientLoader if the default Maven Central repo is unreachable.</p> <p>Default: <code>https://maven-central.storage-download.googleapis.com/maven2/</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlmaxplanstringlength><span id=spark.sql.maxPlanStringLength> spark.sql.maxPlanStringLength<a class=headerlink href=#sparksqlmaxplanstringlength title="Permanent link">&para;</a></h2> <p>Maximum number of characters to output for a plan string. If the plan is longer, further output will be truncated. The default setting always generates a full plan. Set this to a lower value such as 8k if plan strings are taking up too much memory or are causing OutOfMemory errors in the driver or UI processes.</p> <p>Default: <code>Integer.MAX_VALUE - 15</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqladdpartitioninbatchsize><span id=spark.sql.addPartitionInBatch.size> spark.sql.addPartitionInBatch.size<a class=headerlink href=#sparksqladdpartitioninbatchsize title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The number of partitions to be handled in one turn when use <code>AlterTableAddPartitionCommand</code> to add partitions into table. The smaller batch size is, the less memory is required for the real handler, e.g. Hive Metastore.</p> <p>Default: <code>100</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlscripttransformationexittimeoutinseconds><span id=spark.sql.scriptTransformation.exitTimeoutInSeconds> spark.sql.scriptTransformation.exitTimeoutInSeconds<a class=headerlink href=#sparksqlscripttransformationexittimeoutinseconds title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Timeout for executor to wait for the termination of transformation script when EOF.</p> <p>Default: <code>10</code> seconds</p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlautobroadcastjointhreshold><span id=spark.sql.autoBroadcastJoinThreshold><span id=AUTO_BROADCASTJOIN_THRESHOLD> spark.sql.autoBroadcastJoinThreshold<a class=headerlink href=#sparksqlautobroadcastjointhreshold title="Permanent link">&para;</a></h2> <p>Maximum size (in bytes) for a table that can be broadcast (to all worker nodes) in a join</p> <p>Default: <code>10M</code></p> <p><code>-1</code> (or any negative value) disables broadcasting</p> <p>Use <a href=../SQLConf/#autoBroadcastJoinThreshold>SQLConf.autoBroadcastJoinThreshold</a> method to access the current value.</p> <h2 id=sparksqlavrocompressioncodec><span id=spark.sql.avro.compression.codec> spark.sql.avro.compression.codec<a class=headerlink href=#sparksqlavrocompressioncodec title="Permanent link">&para;</a></h2> <p>The compression codec to use when writing Avro data to disk</p> <p>Default: <code>snappy</code></p> <p>The supported codecs are:</p> <ul> <li><code>uncompressed</code></li> <li><code>deflate</code></li> <li><code>snappy</code></li> <li><code>bzip2</code></li> <li><code>xz</code></li> </ul> <p>Use <a href=../SQLConf/#avroCompressionCodec>SQLConf.avroCompressionCodec</a> method to access the current value.</p> <h2 id=sparksqlbroadcasttimeout><span id=spark.sql.broadcastTimeout> spark.sql.broadcastTimeout<a class=headerlink href=#sparksqlbroadcasttimeout title="Permanent link">&para;</a></h2> <p>Timeout in seconds for the broadcast wait time in broadcast joins.</p> <p>Default: <code>5 * 60</code></p> <p>When negative, it is assumed infinite (i.e. <code>Duration.Inf</code>)</p> <p>Use <a href=../SQLConf/#broadcastTimeout>SQLConf.broadcastTimeout</a> method to access the current value.</p> <h2 id=sparksqlbucketingcoalescebucketsinjoinenabled><span id=spark.sql.bucketing.coalesceBucketsInJoin.enabled> spark.sql.bucketing.coalesceBucketsInJoin.enabled<a class=headerlink href=#sparksqlbucketingcoalescebucketsinjoinenabled title="Permanent link">&para;</a></h2> <p>When enabled (<code>true</code>), if two bucketed tables with the different number of buckets are joined, the side with a bigger number of buckets will be coalesced to have the same number of buckets as the other side. Bigger number of buckets is divisible by the smaller number of buckets. Bucket coalescing is applied to sort-merge joins and shuffled hash join.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Coalescing bucketed table can avoid unnecessary shuffling in join, but it also reduces parallelism and could possibly cause OOM for shuffled hash join.</p> </div> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#coalesceBucketsInJoinEnabled>SQLConf.coalesceBucketsInJoinEnabled</a> method to access the current value.</p> <h2 id=sparksqlcasesensitive><span id=spark.sql.caseSensitive> spark.sql.caseSensitive<a class=headerlink href=#sparksqlcasesensitive title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether the query analyzer should be case sensitive (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p> <p>It is highly discouraged to turn on case sensitive mode.</p> <p>Use <a href=../SQLConf/#caseSensitiveAnalysis>SQLConf.caseSensitiveAnalysis</a> method to access the current value.</p> <h2 id=sparksqlcatalogspark_catalog><span id=spark.sql.catalog.spark_catalog> spark.sql.catalog.spark_catalog<a class=headerlink href=#sparksqlcatalogspark_catalog title="Permanent link">&para;</a></h2> <p>A catalog implementation that will be used as the v2 interface to Spark's built-in v1 catalog: <code>spark_catalog</code>. This catalog shares its identifier namespace with the <code>spark_catalog</code> and must be consistent with it; for example, if a table can be loaded by the <code>spark_catalog</code>, this catalog must also return the table metadata. To delegate operations to the <code>spark_catalog</code>, implementations can extend 'CatalogExtension'.</p> <p>Default: <code>(undefined)</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcboenabled><span id=spark.sql.cbo.enabled> spark.sql.cbo.enabled<a class=headerlink href=#sparksqlcboenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-cost-based-optimization/ >Cost-Based Optimization</a> (CBO) for estimation of plan statistics when <code>true</code>.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#cboEnabled>SQLConf.cboEnabled</a> method to access the current value.</p> <h2 id=sparksqlcbojoinreorderenabled><span id=spark.sql.cbo.joinReorder.enabled> spark.sql.cbo.joinReorder.enabled<a class=headerlink href=#sparksqlcbojoinreorderenabled title="Permanent link">&para;</a></h2> <p>Enables join reorder for cost-based optimization (CBO).</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#joinReorderEnabled>SQLConf.joinReorderEnabled</a> method to access the current value.</p> <h2 id=sparksqlcboplanstatsenabled><span id=spark.sql.cbo.planStats.enabled> spark.sql.cbo.planStats.enabled<a class=headerlink href=#sparksqlcboplanstatsenabled title="Permanent link">&para;</a></h2> <p>When <code>true</code>, the logical plan will fetch row counts and column statistics from catalog.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcbostarschemadetection><span id=spark.sql.cbo.starSchemaDetection> spark.sql.cbo.starSchemaDetection<a class=headerlink href=#sparksqlcbostarschemadetection title="Permanent link">&para;</a></h2> <p>Enables <em>join reordering</em> based on star schema detection for cost-based optimization (CBO) in <a href=../logical-optimizations/ReorderJoin/ >ReorderJoin</a> logical plan optimization.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#starSchemaDetection>SQLConf.starSchemaDetection</a> method to access the current value.</p> <h2 id=sparksqlcodegenaggregatemapvectorizedenable><span id=spark.sql.codegen.aggregate.map.vectorized.enable> spark.sql.codegen.aggregate.map.vectorized.enable<a class=headerlink href=#sparksqlcodegenaggregatemapvectorizedenable title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables vectorized aggregate hash map. This is for testing/benchmarking only.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcodegenaggregatesplitaggregatefuncenabled><span id=spark.sql.codegen.aggregate.splitAggregateFunc.enabled> spark.sql.codegen.aggregate.splitAggregateFunc.enabled<a class=headerlink href=#sparksqlcodegenaggregatesplitaggregatefuncenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the code generator would split aggregate code into individual methods instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlcodegencomments><span id=spark.sql.codegen.comments> spark.sql.codegen.comments<a class=headerlink href=#sparksqlcodegencomments title="Permanent link">&para;</a></h2> <p>Controls whether <code>CodegenContext</code> should <a href=../physical-operators/CodegenSupport/#registerComment>register comments</a> (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>false</code></p> <h2 id=sparksqlcodegenfactorymode><span id=spark.sql.codegen.factoryMode> spark.sql.codegen.factoryMode<a class=headerlink href=#sparksqlcodegenfactorymode title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Determines the codegen generator fallback behavior</p> <p>Default: <code>FALLBACK</code></p> <p>Acceptable values:</p> <ul> <li><code>CODEGEN_ONLY</code> - disable fallback mode</li> <li><code>FALLBACK</code> - try codegen first and, if any compile error happens, fallback to interpreted mode</li> <li><code>NO_CODEGEN</code> - skips codegen and always uses interpreted path</li> </ul> <p>Used when <code>CodeGeneratorWithInterpretedFallback</code> is requested to <a href=../expressions/CodeGeneratorWithInterpretedFallback/#createObject>createObject</a> (when <code>UnsafeProjection</code> is requested to <a href=../expressions/UnsafeProjection/#create>create an UnsafeProjection for Catalyst expressions</a>)</p> <h2 id=sparksqlcodegenfallback><span id=spark.sql.codegen.fallback> spark.sql.codegen.fallback<a class=headerlink href=#sparksqlcodegenfallback title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Whether the whole stage codegen could be temporary disabled for the part of a query that has failed to compile generated code (<code>true</code>) or not (<code>false</code>).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageFallback>SQLConf.wholeStageFallback</a> method to access the current value.</p> <h2 id=sparksqlcodegenhugemethodlimit><span id=spark.sql.codegen.hugeMethodLimit> spark.sql.codegen.hugeMethodLimit<a class=headerlink href=#sparksqlcodegenhugemethodlimit title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The maximum bytecode size of a single compiled Java function generated by whole-stage codegen.</p> <p>Default: <code>65535</code></p> <p>The default value <code>65535</code> is the largest bytecode size possible for a valid Java method. When running on HotSpot, it may be preferable to set the value to <code>8000</code> (which is the value of <code>HugeMethodLimit</code> in the OpenJDK JVM settings)</p> <p>Use <a href=../SQLConf/#hugeMethodLimit>SQLConf.hugeMethodLimit</a> method to access the current value.</p> <h2 id=sparksqlcodegenuseidinclassname><span id=spark.sql.codegen.useIdInClassName> spark.sql.codegen.useIdInClassName<a class=headerlink href=#sparksqlcodegenuseidinclassname title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether to embed the (whole-stage) codegen stage ID into the class name of the generated class as a suffix (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageUseIdInClassName>SQLConf.wholeStageUseIdInClassName</a> method to access the current value.</p> <h2 id=sparksqlcodegenmaxfields><span id=spark.sql.codegen.maxFields> spark.sql.codegen.maxFields<a class=headerlink href=#sparksqlcodegenmaxfields title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Maximum number of output fields (including nested fields) that whole-stage codegen supports. Going above the number deactivates whole-stage codegen.</p> <p>Default: <code>100</code></p> <p>Use <a href=../SQLConf/#wholeStageMaxNumFields>SQLConf.wholeStageMaxNumFields</a> method to access the current value.</p> <h2 id=sparksqlcodegensplitconsumefuncbyoperator><span id=spark.sql.codegen.splitConsumeFuncByOperator> spark.sql.codegen.splitConsumeFuncByOperator<a class=headerlink href=#sparksqlcodegensplitconsumefuncbyoperator title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether whole stage codegen puts the logic of consuming rows of each physical operator into individual methods, instead of a single big method. This can be used to avoid oversized function that can miss the opportunity of JIT optimization.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#wholeStageSplitConsumeFuncByOperator>SQLConf.wholeStageSplitConsumeFuncByOperator</a> method to access the current value.</p> <h2 id=sparksqlcolumnvectoroffheapenabled><span id=spark.sql.columnVector.offheap.enabled> spark.sql.columnVector.offheap.enabled<a class=headerlink href=#sparksqlcolumnvectoroffheapenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables <a href=../OffHeapColumnVector/ >OffHeapColumnVector</a> in <a href=../ColumnarBatch/ >ColumnarBatch</a> (<code>true</code>) or not (<code>false</code>). When <code>false</code>, <a href=../OnHeapColumnVector/ >OnHeapColumnVector</a> is used instead.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#offHeapColumnVectorEnabled>SQLConf.offHeapColumnVectorEnabled</a> method to access the current value.</p> <h2 id=sparksqlcolumnnameofcorruptrecord><span id=spark.sql.columnNameOfCorruptRecord> spark.sql.columnNameOfCorruptRecord<a class=headerlink href=#sparksqlcolumnnameofcorruptrecord title="Permanent link">&para;</a></h2> <h2 id=sparksqlconstraintpropagationenabled><span id=spark.sql.constraintPropagation.enabled> spark.sql.constraintPropagation.enabled<a class=headerlink href=#sparksqlconstraintpropagationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the query optimizer will infer and propagate data constraints in the query plan to optimize them. Constraint propagation can sometimes be computationally expensive for certain kinds of query plans (such as those with a large number of predicates and aliases) which might negatively impact overall runtime.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#constraintPropagationEnabled>SQLConf.constraintPropagationEnabled</a> method to access the current value.</p> <h2 id=sparksqlcsvfilterpushdownenabled><span id=spark.sql.csv.filterPushdown.enabled> spark.sql.csv.filterPushdown.enabled<a class=headerlink href=#sparksqlcsvfilterpushdownenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, enable filter pushdown to CSV datasource.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqldefaultsizeinbytes><span id=spark.sql.defaultSizeInBytes> spark.sql.defaultSizeInBytes<a class=headerlink href=#sparksqldefaultsizeinbytes title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Estimated size of a table or relation used in query planning</p> <p>Default: Java's <code>Long.MaxValue</code></p> <p>Set to Java's <code>Long.MaxValue</code> which is larger than <a href=#spark.sql.autoBroadcastJoinThreshold>spark.sql.autoBroadcastJoinThreshold</a> to be more conservative. That is to say by default the optimizer will not choose to broadcast a table unless it knows for sure that the table size is small enough.</p> <p>Used by the planner to decide when it is safe to broadcast a relation. By default, the system will assume that tables are too large to broadcast.</p> <p>Use <a href=../SQLConf/#defaultSizeInBytes>SQLConf.defaultSizeInBytes</a> method to access the current value.</p> <h2 id=sparksqldialect><span id=spark.sql.dialect> spark.sql.dialect<a class=headerlink href=#sparksqldialect title="Permanent link">&para;</a></h2> <h2 id=sparksqlexchangereuse><span id=spark.sql.exchange.reuse> spark.sql.exchange.reuse<a class=headerlink href=#sparksqlexchangereuse title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When enabled (<code>true</code>), the <a href=../SparkPlanner/ >Spark planner</a> will find duplicated exchanges and subqueries and re-use them.</p> <p>When disabled (<code>false</code>), <a href=../physical-optimizations/ReuseExchange/ >ReuseExchange</a> and <a href=../physical-optimizations/ReuseSubquery/ >ReuseSubquery</a> physical optimizations (that the Spark planner uses for physical query plan optimization) do nothing.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#exchangeReuseEnabled>SQLConf.exchangeReuseEnabled</a> method to access the current value.</p> <h2 id=executionuseobjecthashaggregateexec><span id=spark.sql.execution.useObjectHashAggregateExec> execution.useObjectHashAggregateExec<a class=headerlink href=#executionuseobjecthashaggregateexec title="Permanent link">&para;</a></h2> <p><strong>spark.sql.execution.useObjectHashAggregateExec</strong></p> <p><strong>(internal)</strong> <a href=../AggUtils/#createAggregate>Prefers ObjectHashAggregateExec (over SortAggregateExec) for aggregation</a></p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#useObjectHashAggregation>SQLConf.useObjectHashAggregation</a> method to access the current value.</p> <h2 id=sparksqlfilesignorecorruptfiles><span id=spark.sql.files.ignoreCorruptFiles> spark.sql.files.ignoreCorruptFiles<a class=headerlink href=#sparksqlfilesignorecorruptfiles title="Permanent link">&para;</a></h2> <p>Controls whether to ignore corrupt files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#ignoreCorruptFiles>SQLConf.ignoreCorruptFiles</a> method to access the current value.</p> <h2 id=sparksqlfilesignoremissingfiles><span id=spark.sql.files.ignoreMissingFiles> spark.sql.files.ignoreMissingFiles<a class=headerlink href=#sparksqlfilesignoremissingfiles title="Permanent link">&para;</a></h2> <p>Controls whether to ignore missing files (<code>true</code>) or not (<code>false</code>). If <code>true</code>, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#ignoreMissingFiles>SQLConf.ignoreMissingFiles</a> method to access the current value.</p> <h2 id=sparksqlfilesmaxrecordsperfile><span id=spark.sql.files.maxRecordsPerFile> spark.sql.files.maxRecordsPerFile<a class=headerlink href=#sparksqlfilesmaxrecordsperfile title="Permanent link">&para;</a></h2> <p>Maximum number of records to write out to a single file. If this value is <code>0</code> or negative, there is no limit.</p> <p>Default: <code>0</code></p> <p>Use <a href=../SQLConf/#maxRecordsPerFile>SQLConf.maxRecordsPerFile</a> method to access the current value.</p> <h2 id=sparksqlfilesmaxpartitionbytes><span id=spark.sql.files.maxPartitionBytes> spark.sql.files.maxPartitionBytes<a class=headerlink href=#sparksqlfilesmaxpartitionbytes title="Permanent link">&para;</a></h2> <p>The maximum number of bytes to pack into a single partition when reading files.</p> <p>Default: <code>128 * 1024 * 1024</code> (which corresponds to <code>parquet.block.size</code>)</p> <p>Use <a href=../SQLConf/#filesMaxPartitionBytes>SQLConf.filesMaxPartitionBytes</a> method to access the current value.</p> <h2 id=sparksqlfilesopencostinbytes><span id=spark.sql.files.openCostInBytes> spark.sql.files.openCostInBytes<a class=headerlink href=#sparksqlfilesopencostinbytes title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The estimated cost to open a file, measured by the number of bytes could be scanned at the same time (to include multiple files into a partition).</p> <p>Default: <code>4 * 1024 * 1024</code></p> <p>It's better to over estimate it, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</p> <p>Use <a href=../SQLConf/#filesOpenCostInBytes>SQLConf.filesOpenCostInBytes</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragecompressed><span id=spark.sql.inMemoryColumnarStorage.compressed> spark.sql.inMemoryColumnarStorage.compressed<a class=headerlink href=#sparksqlinmemorycolumnarstoragecompressed title="Permanent link">&para;</a></h2> <p>When enabled, Spark SQL will automatically select a compression codec for each column based on statistics of the data.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#useCompression>SQLConf.useCompression</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragebatchsize><span id=spark.sql.inMemoryColumnarStorage.batchSize> spark.sql.inMemoryColumnarStorage.batchSize<a class=headerlink href=#sparksqlinmemorycolumnarstoragebatchsize title="Permanent link">&para;</a></h2> <p>Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data.</p> <p>Default: <code>10000</code></p> <p>Use <a href=../SQLConf/#columnBatchSize>SQLConf.columnBatchSize</a> method to access the current value.</p> <h2 id=sparksqlinmemorytablescanstatisticsenable><span id=spark.sql.inMemoryTableScanStatistics.enable> spark.sql.inMemoryTableScanStatistics.enable<a class=headerlink href=#sparksqlinmemorytablescanstatisticsenable title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, enable in-memory table scan accumulators.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlinmemorycolumnarstorageenablevectorizedreader><span id=spark.sql.inMemoryColumnarStorage.enableVectorizedReader> spark.sql.inMemoryColumnarStorage.enableVectorizedReader<a class=headerlink href=#sparksqlinmemorycolumnarstorageenablevectorizedreader title="Permanent link">&para;</a></h2> <p>Enables <a href=../spark-sql-vectorized-query-execution/ >vectorized reader</a> for columnar caching.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#cacheVectorizedReaderEnabled>SQLConf.cacheVectorizedReaderEnabled</a> method to access the current value.</p> <h2 id=sparksqlinmemorycolumnarstoragepartitionpruning><span id=spark.sql.inMemoryColumnarStorage.partitionPruning> spark.sql.inMemoryColumnarStorage.partitionPruning<a class=headerlink href=#sparksqlinmemorycolumnarstoragepartitionpruning title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables partition pruning for in-memory columnar tables</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#inMemoryPartitionPruning>SQLConf.inMemoryPartitionPruning</a> method to access the current value.</p> <h2 id=sparksqljoinprefersortmergejoin><span id=spark.sql.join.preferSortMergeJoin> spark.sql.join.preferSortMergeJoin<a class=headerlink href=#sparksqljoinprefersortmergejoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether <a href=../execution-planning-strategies/JoinSelection/ >JoinSelection</a> execution planning strategy prefers <a href=../physical-operators/SortMergeJoinExec/ >sort merge join</a> over <a href=../physical-operators/ShuffledHashJoinExec/ >shuffled hash join</a>.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#preferSortMergeJoin>SQLConf.preferSortMergeJoin</a> method to access the current value.</p> <h2 id=sparksqljsongeneratorignorenullfields><span id=spark.sql.jsonGenerator.ignoreNullFields> spark.sql.jsonGenerator.ignoreNullFields<a class=headerlink href=#sparksqljsongeneratorignorenullfields title="Permanent link">&para;</a></h2> <p>Whether to ignore null fields when generating JSON objects in JSON data source and JSON functions such as to_json. If false, it generates null for null fields in JSON objects.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlleafnodedefaultparallelism><span id=spark.sql.leafNodeDefaultParallelism> spark.sql.leafNodeDefaultParallelism<a class=headerlink href=#sparksqlleafnodedefaultparallelism title="Permanent link">&para;</a></h2> <p>The default parallelism of leaf operators that produce data (e.g. the file scan node, the local data scan node, the range node).</p> <p>Must be positive</p> <p>Default: <code>SparkContext.defaultParallelism</code> (<a href=https://books.japila.pl/apache-spark-internals/SparkContext#defaultParallelism>Spark Core</a>)</p> <p>Since: <code>3.2.0</code></p> <h2 id=sparksqllegacydolooseupcast><span id=spark.sql.legacy.doLooseUpcast> spark.sql.legacy.doLooseUpcast<a class=headerlink href=#sparksqllegacydolooseupcast title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the upcast will be loose and allows string to atomic types.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacycteprecedencepolicy><span id=spark.sql.legacy.ctePrecedencePolicy> spark.sql.legacy.ctePrecedencePolicy<a class=headerlink href=#sparksqllegacycteprecedencepolicy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> This config will be removed in future versions and <code>CORRECTED</code> will be the only behavior.</p> <p>Possible values:</p> <ol> <li><code>CORRECTED</code> - inner CTE definitions take precedence</li> <li><code>EXCEPTION</code> - <code>AnalysisException</code> is thrown while name conflict is detected in nested CTE</li> <li><code>LEGACY</code> - outer CTE definitions takes precedence over inner definitions</li> </ol> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacytimeparserpolicy><span id=spark.sql.legacy.timeParserPolicy> spark.sql.legacy.timeParserPolicy<a class=headerlink href=#sparksqllegacytimeparserpolicy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, java.text.SimpleDateFormat is used for formatting and parsing dates/timestamps in a locale-sensitive manner, which is the approach before Spark 3.0. When set to CORRECTED, classes from <code>java.time.*</code> packages are used for the same purpose. The default value is EXCEPTION, RuntimeException is thrown when we will get different results.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyfollowthreevaluedlogicinarrayexists><span id=spark.sql.legacy.followThreeValuedLogicInArrayExists> spark.sql.legacy.followThreeValuedLogicInArrayExists<a class=headerlink href=#sparksqllegacyfollowthreevaluedlogicinarrayexists title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When true, the ArrayExists will follow the three-valued boolean logic.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyfromdaytimestringenabled><span id=spark.sql.legacy.fromDayTimeString.enabled> spark.sql.legacy.fromDayTimeString.enabled<a class=headerlink href=#sparksqllegacyfromdaytimestringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the <code>from</code> bound is not taken into account in conversion of a day-time string to an interval, and the <code>to</code> bound is used to skip all interval units out of the specified range. When <code>false</code>, <code>ParseException</code> is thrown if the input does not match to the pattern defined by <code>from</code> and <code>to</code>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacynotreserveproperties><span id=spark.sql.legacy.notReserveProperties> spark.sql.legacy.notReserveProperties<a class=headerlink href=#sparksqllegacynotreserveproperties title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, all database and table properties are not reserved and available for create/alter syntaxes. But please be aware that the reserved properties will be silently removed.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyaddsinglefileinaddfile><span id=spark.sql.legacy.addSingleFileInAddFile> spark.sql.legacy.addSingleFileInAddFile<a class=headerlink href=#sparksqllegacyaddsinglefileinaddfile title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, only a single file can be added using ADD FILE. If false, then users can add directory by passing directory path to ADD FILE.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyexponentliteralasdecimalenabled><span id=spark.sql.legacy.exponentLiteralAsDecimal.enabled> spark.sql.legacy.exponentLiteralAsDecimal.enabled<a class=headerlink href=#sparksqllegacyexponentliteralasdecimalenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, a literal with an exponent (e.g. 1E-30) would be parsed as Decimal rather than Double.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallownegativescaleofdecimal><span id=spark.sql.legacy.allowNegativeScaleOfDecimal> spark.sql.legacy.allowNegativeScaleOfDecimal<a class=headerlink href=#sparksqllegacyallownegativescaleofdecimal title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, negative scale of Decimal type is allowed. For example, the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is Decimal(11, 0) in non legacy mode.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacybucketedtablescanoutputordering><span id=spark.sql.legacy.bucketedTableScan.outputOrdering> spark.sql.legacy.bucketedTableScan.outputOrdering<a class=headerlink href=#sparksqllegacybucketedtablescanoutputordering title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the bucketed table scan will list files during planning to figure out the output ordering, which is expensive and may make the planning quite slow.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyjsonallowemptystringenabled><span id=spark.sql.legacy.json.allowEmptyString.enabled> spark.sql.legacy.json.allowEmptyString.enabled<a class=headerlink href=#sparksqllegacyjsonallowemptystringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the parser of JSON data source treats empty strings as null for some data types such as <code>IntegerType</code>.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacycreateemptycollectionusingstringtype><span id=spark.sql.legacy.createEmptyCollectionUsingStringType> spark.sql.legacy.createEmptyCollectionUsingStringType<a class=headerlink href=#sparksqllegacycreateemptycollectionusingstringtype title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, Spark returns an empty collection with <code>StringType</code> as element type if the <code>array</code>/<code>map</code> function is called without any parameters. Otherwise, Spark returns an empty collection with <code>NullType</code> as element type.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallowuntypedscalaudf><span id=spark.sql.legacy.allowUntypedScalaUDF> spark.sql.legacy.allowUntypedScalaUDF<a class=headerlink href=#sparksqllegacyallowuntypedscalaudf title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, user is allowed to use <code>org.apache.spark.sql.functions.udf(f: AnyRef, dataType: DataType)</code>. Otherwise, an exception will be thrown at runtime.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacydatasetnamenonstructgroupingkeyasvalue><span id=spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue> spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue<a class=headerlink href=#sparksqllegacydatasetnamenonstructgroupingkeyasvalue title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the key attribute resulted from running <code>Dataset.groupByKey</code> for non-struct key type, will be named as <code>value</code>, following the behavior of Spark version 2.4 and earlier.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacysetcommandrejectssparkcoreconfs><span id=spark.sql.legacy.setCommandRejectsSparkCoreConfs> spark.sql.legacy.setCommandRejectsSparkCoreConfs<a class=headerlink href=#sparksqllegacysetcommandrejectssparkcoreconfs title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> If it is set to true, SET command will fail when the key is registered as a SparkConf entry.</p> <p>Default: <code>true</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacytypecoerciondatetimetostringenabled><span id=spark.sql.legacy.typeCoercion.datetimeToString.enabled> spark.sql.legacy.typeCoercion.datetimeToString.enabled<a class=headerlink href=#sparksqllegacytypecoerciondatetimetostringenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, date/timestamp will cast to string in binary comparisons with String</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyallowhashonmaptype><span id=spark.sql.legacy.allowHashOnMapType> spark.sql.legacy.allowHashOnMapType<a class=headerlink href=#sparksqllegacyallowhashonmaptype title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, hash expressions can be applied on elements of MapType. Otherwise, an analysis exception will be thrown.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyparquetdatetimerebasemodeinwrite><span id=spark.sql.legacy.parquet.datetimeRebaseModeInWrite> spark.sql.legacy.parquet.datetimeRebaseModeInWrite<a class=headerlink href=#sparksqllegacyparquetdatetimerebasemodeinwrite title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Parquet files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyparquetdatetimerebasemodeinread><span id=spark.sql.legacy.parquet.datetimeRebaseModeInRead> spark.sql.legacy.parquet.datetimeRebaseModeInRead<a class=headerlink href=#sparksqllegacyparquetdatetimerebasemodeinread title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Parquet files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Parquet files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyavrodatetimerebasemodeinwrite><span id=spark.sql.legacy.avro.datetimeRebaseModeInWrite> spark.sql.legacy.avro.datetimeRebaseModeInWrite<a class=headerlink href=#sparksqllegacyavrodatetimerebasemodeinwrite title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from Proleptic Gregorian calendar to the legacy hybrid (Julian + Gregorian) calendar when writing Avro files. When CORRECTED, Spark will not do rebase and write the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the writing if it sees ancient dates/timestamps that are ambiguous between the two calendars.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyavrodatetimerebasemodeinread><span id=spark.sql.legacy.avro.datetimeRebaseModeInRead> spark.sql.legacy.avro.datetimeRebaseModeInRead<a class=headerlink href=#sparksqllegacyavrodatetimerebasemodeinread title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When LEGACY, Spark will rebase dates/timestamps from the legacy hybrid (Julian + Gregorian) calendar to Proleptic Gregorian calendar when reading Avro files. When CORRECTED, Spark will not do rebase and read the dates/timestamps as it is. When EXCEPTION, which is the default, Spark will fail the reading if it sees ancient dates/timestamps that are ambiguous between the two calendars. This config is only effective if the writer info (like Spark, Hive) of the Avro files is unknown.</p> <p>Possible values: <code>EXCEPTION</code>, <code>LEGACY</code>, <code>CORRECTED</code></p> <p>Default: <code>EXCEPTION</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqllegacyrddapplyconf><span id=spark.sql.legacy.rdd.applyConf> spark.sql.legacy.rdd.applyConf<a class=headerlink href=#sparksqllegacyrddapplyconf title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables propagation of <a href=../SQLConf/#getAllConfs>SQL configurations</a> when executing operations on the <a href=../QueryExecution/#toRdd>RDD that represents a structured query</a>. This is the (buggy) behavior up to 2.4.4.</p> <p>Default: <code>true</code></p> <p>This is for cases not tracked by <a href=../SQLExecution/ >SQL execution</a>, when a <code>Dataset</code> is converted to an RDD either using Dataset.md#rdd[rdd] operation or <a href=../QueryExecution/#toRdd>QueryExecution</a>, and then the returned RDD is used to invoke actions on it.</p> <p>This config is deprecated and will be removed in 3.0.0.</p> <h2 id=sparksqllegacyreplacedatabrickssparkavroenabled><span id=spark.sql.legacy.replaceDatabricksSparkAvro.enabled> spark.sql.legacy.replaceDatabricksSparkAvro.enabled<a class=headerlink href=#sparksqllegacyreplacedatabrickssparkavroenabled title="Permanent link">&para;</a></h2> <p>Enables resolving (<em>mapping</em>) the data source provider <code>com.databricks.spark.avro</code> to the built-in (but external) Avro data source module for backward compatibility.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#replaceDatabricksSparkAvroEnabled>SQLConf.replaceDatabricksSparkAvroEnabled</a> method to access the current value.</p> <h2 id=sparksqllimitscaleupfactor><span id=spark.sql.limit.scaleUpFactor> spark.sql.limit.scaleUpFactor<a class=headerlink href=#sparksqllimitscaleupfactor title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Minimal increase rate in the number of partitions between attempts when executing <code>take</code> operator on a structured query. Higher values lead to more partitions read. Lower values might lead to longer execution times as more jobs will be run.</p> <p>Default: <code>4</code></p> <p>Use <a href=../SQLConf/#limitScaleUpFactor>SQLConf.limitScaleUpFactor</a> method to access the current value.</p> <h2 id=sparksqloptimizenullawareantijoin><span id=spark.sql.optimizeNullAwareAntiJoin> spark.sql.optimizeNullAwareAntiJoin<a class=headerlink href=#sparksqloptimizenullawareantijoin title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables <a href=../ExtractSingleColumnNullAwareAntiJoin/#unapply>single-column NULL-aware anti join execution planning</a> into <a href=../physical-operators/BroadcastHashJoinExec/ >BroadcastHashJoinExec</a> (with flag <a href=../physical-operators/BroadcastHashJoinExec/#isNullAwareAntiJoin>isNullAwareAntiJoin</a> enabled), optimized from O(M*N) calculation into O(M) calculation using hash lookup instead of looping lookup.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#optimizeNullAwareAntiJoin>SQLConf.optimizeNullAwareAntiJoin</a> method to access the current value.</p> <h2 id=sparksqloptimizerexcludedrules><span id=spark.sql.optimizer.excludedRules> spark.sql.optimizer.excludedRules<a class=headerlink href=#sparksqloptimizerexcludedrules title="Permanent link">&para;</a></h2> <p>Comma-separated list of fully-qualified class names of the optimization rules that should be disabled (excluded) from <a href=../catalyst/Optimizer/#spark.sql.optimizer.excludedRules>logical query optimization</a>.</p> <p>Default: <code>(empty)</code></p> <p>Use <a href=../SQLConf/#optimizerExcludedRules>SQLConf.optimizerExcludedRules</a> method to access the current value.</p> <div class="admonition important"> <p class=admonition-title>Important</p> <p>It is not guaranteed that all the rules to be excluded will eventually be excluded, as some rules are <a href=../catalyst/Optimizer/#nonExcludableRules>non-excludable</a>.</p> </div> <h2 id=sparksqloptimizerinsetconversionthreshold><span id=spark.sql.optimizer.inSetConversionThreshold> spark.sql.optimizer.inSetConversionThreshold<a class=headerlink href=#sparksqloptimizerinsetconversionthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The threshold of set size for <code>InSet</code> conversion.</p> <p>Default: <code>10</code></p> <p>Use <a href=../SQLConf/#optimizerInSetConversionThreshold>SQLConf.optimizerInSetConversionThreshold</a> method to access the current value.</p> <h2 id=sparksqloptimizermaxiterations><span id=spark.sql.optimizer.maxIterations> spark.sql.optimizer.maxIterations<a class=headerlink href=#sparksqloptimizermaxiterations title="Permanent link">&para;</a></h2> <p>Maximum number of iterations for <a href=../Analyzer/#fixedPoint>Analyzer</a> and <a href=../catalyst/Optimizer/#fixedPoint>Logical Optimizer</a>.</p> <p>Default: <code>100</code></p> <h2 id=sparksqloptimizerreplaceexceptwithfilter><span id=spark.sql.optimizer.replaceExceptWithFilter> spark.sql.optimizer.replaceExceptWithFilter<a class=headerlink href=#sparksqloptimizerreplaceexceptwithfilter title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>true</code>, the apply function of the rule verifies whether the right node of the except operation is of type Filter or Project followed by Filter. If yes, the rule further verifies 1) Excluding the filter operations from the right (as well as the left node, if any) on the top, whether both the nodes evaluates to a same result. 2) The left and right nodes don't contain any SubqueryExpressions. 3) The output column names of the left node are distinct. If all the conditions are met, the rule will replace the except operation with a Filter by flipping the filter condition(s) of the right node.</p> <p>Default: <code>true</code></p> <h2 id=sparksqloptimizernestedschemapruningenabled><span id=spark.sql.optimizer.nestedSchemaPruning.enabled> spark.sql.optimizer.nestedSchemaPruning.enabled<a class=headerlink href=#sparksqloptimizernestedschemapruningenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Prune nested fields from the output of a logical relation that are not necessary in satisfying a query. This optimization allows columnar file format readers to avoid reading unnecessary nested column data.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#nestedSchemaPruningEnabled>SQLConf.nestedSchemaPruningEnabled</a> method to access the current value.</p> <h2 id=sparksqlorcimpl><span id=spark.sql.orc.impl> spark.sql.orc.impl<a class=headerlink href=#sparksqlorcimpl title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When <code>native</code>, use the native version of ORC support instead of the ORC library in Hive 1.2.1.</p> <p>Default: <code>native</code></p> <p>Acceptable values:</p> <ul> <li><code>hive</code></li> <li><code>native</code></li> </ul> <h2 id=sparksqlplanchangeloglevel><span id=spark.sql.planChangeLog.level> spark.sql.planChangeLog.level<a class=headerlink href=#sparksqlplanchangeloglevel title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Log level for logging the change from the original plan to the new plan after a rule or batch is applied.</p> <p>Default: <code>trace</code></p> <p>Supported Values (case-insensitive):</p> <ul> <li>trace</li> <li>debug</li> <li>info</li> <li>warn</li> <li>error</li> </ul> <p>Use <a href=../SQLConf/#planChangeLogLevel>SQLConf.planChangeLogLevel</a> method to access the current value.</p> <h2 id=sparksqlplanchangelogbatches><span id=spark.sql.planChangeLog.batches> spark.sql.planChangeLog.batches<a class=headerlink href=#sparksqlplanchangelogbatches title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Comma-separated list of batch names for plan changes logging</p> <p>Default: (undefined)</p> <p>Use <a href=../SQLConf/#planChangeBatches>SQLConf.planChangeBatches</a> method to access the current value.</p> <h2 id=sparksqlplanchangelogrules><span id=spark.sql.planChangeLog.rules> spark.sql.planChangeLog.rules<a class=headerlink href=#sparksqlplanchangelogrules title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Comma-separated list of rule names for plan changes logging</p> <p>Default: (undefined)</p> <p>Use <a href=../SQLConf/#planChangeRules>SQLConf.planChangeRules</a> method to access the current value.</p> <h2 id=sparksqlpysparkjvmstacktraceenabled><span id=spark.sql.pyspark.jvmStacktrace.enabled> spark.sql.pyspark.jvmStacktrace.enabled<a class=headerlink href=#sparksqlpysparkjvmstacktraceenabled title="Permanent link">&para;</a></h2> <p>When true, it shows the JVM stacktrace in the user-facing PySpark exception together with Python stacktrace. By default, it is disabled and hides JVM stacktrace and shows a Python-friendly exception only.</p> <p>Default: <code>false</code></p> <p>Since: <code>3.0.0</code></p> <h2 id=sparksqlparquetbinaryasstring><span id=spark.sql.parquet.binaryAsString> spark.sql.parquet.binaryAsString<a class=headerlink href=#sparksqlparquetbinaryasstring title="Permanent link">&para;</a></h2> <p>Some other Parquet-producing systems, in particular Impala and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#isParquetBinaryAsString>SQLConf.isParquetBinaryAsString</a> method to access the current value.</p> <h2 id=sparksqlparquetcolumnarreaderbatchsize><span id=spark.sql.parquet.columnarReaderBatchSize> spark.sql.parquet.columnarReaderBatchSize<a class=headerlink href=#sparksqlparquetcolumnarreaderbatchsize title="Permanent link">&para;</a></h2> <p>The number of rows to include in a parquet vectorized reader batch (the capacity of <a href=../datasources/parquet/VectorizedParquetRecordReader/ >VectorizedParquetRecordReader</a>).</p> <p>Default: <code>4096</code> (4k)</p> <p>The number should be carefully chosen to minimize overhead and avoid OOMs while reading data.</p> <p>Use <a href=../SQLConf/#parquetVectorizedReaderBatchSize>SQLConf.parquetVectorizedReaderBatchSize</a> method to access the current value.</p> <h2 id=sparksqlparquetint96astimestamp><span id=spark.sql.parquet.int96AsTimestamp> spark.sql.parquet.int96AsTimestamp<a class=headerlink href=#sparksqlparquetint96astimestamp title="Permanent link">&para;</a></h2> <p>Some Parquet-producing systems, in particular Impala, store Timestamp into INT96. Spark would also store Timestamp as INT96 because we need to avoid precision lost of the nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#isParquetINT96AsTimestamp>SQLConf.isParquetINT96AsTimestamp</a> method to access the current value.</p> <h2 id=sparksqlparquetenablevectorizedreader><span id=spark.sql.parquet.enableVectorizedReader> spark.sql.parquet.enableVectorizedReader<a class=headerlink href=#sparksqlparquetenablevectorizedreader title="Permanent link">&para;</a></h2> <p>Enables <a href=../vectorized-parquet-reader/ >vectorized parquet decoding</a>.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetVectorizedReaderEnabled>SQLConf.parquetVectorizedReaderEnabled</a> method to access the current value.</p> <h2 id=sparksqlparquetfilterpushdown><span id=spark.sql.parquet.filterPushdown> spark.sql.parquet.filterPushdown<a class=headerlink href=#sparksqlparquetfilterpushdown title="Permanent link">&para;</a></h2> <p>Controls the <a href=../logical-optimizations/PushDownPredicate/ >filter predicate push-down optimization</a> for data sources using <a href=../datasources/parquet/ParquetFileFormat/ >parquet</a> file format</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetFilterPushDown>SQLConf.parquetFilterPushDown</a> method to access the current value.</p> <h2 id=sparksqlparquetfilterpushdowndate><span id=spark.sql.parquet.filterPushdown.date> spark.sql.parquet.filterPushdown.date<a class=headerlink href=#sparksqlparquetfilterpushdowndate title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables parquet filter push-down optimization for Date (when <a href=#spark.sql.parquet.filterPushdown>spark.sql.parquet.filterPushdown</a> is enabled)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parquetFilterPushDownDate>SQLConf.parquetFilterPushDownDate</a> method to access the current value.</p> <h2 id=sparksqlparquetint96timestampconversion><span id=spark.sql.parquet.int96TimestampConversion> spark.sql.parquet.int96TimestampConversion<a class=headerlink href=#sparksqlparquetint96timestampconversion title="Permanent link">&para;</a></h2> <p>Controls whether timestamp adjustments should be applied to INT96 data when converting to timestamps, for data written by Impala.</p> <p>Default: <code>false</code></p> <p>This is necessary because Impala stores INT96 data with a different timezone offset than Hive and Spark.</p> <p>Use <a href=../SQLConf/#isParquetINT96TimestampConversion>SQLConf.isParquetINT96TimestampConversion</a> method to access the current value.</p> <h2 id=sparksqlparquetrecordlevelfilterenabled><span id=spark.sql.parquet.recordLevelFilter.enabled> spark.sql.parquet.recordLevelFilter.enabled<a class=headerlink href=#sparksqlparquetrecordlevelfilterenabled title="Permanent link">&para;</a></h2> <p>Enables Parquet's native record-level filtering using the pushed down filters (when <a href=#spark.sql.parquet.filterPushdown>spark.sql.parquet.filterPushdown</a> is enabled).</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#parquetRecordFilterEnabled>SQLConf.parquetRecordFilterEnabled</a> method to access the current value.</p> <h2 id=sparksqlparserquotedregexcolumnnames><span id=spark.sql.parser.quotedRegexColumnNames> spark.sql.parser.quotedRegexColumnNames<a class=headerlink href=#sparksqlparserquotedregexcolumnnames title="Permanent link">&para;</a></h2> <p>Controls whether quoted identifiers (using backticks) in SELECT statements should be interpreted as regular expressions.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#supportQuotedRegexColumnName>SQLConf.supportQuotedRegexColumnName</a> method to access the current value.</p> <h2 id=sparksqlpivotmaxvalues><span id=spark.sql.pivotMaxValues> spark.sql.pivotMaxValues<a class=headerlink href=#sparksqlpivotmaxvalues title="Permanent link">&para;</a></h2> <p>Maximum number of (distinct) values that will be collected without error (when doing a <a href=../RelationalGroupedDataset/#pivot>pivot</a> without specifying the values for the pivot column)</p> <p>Default: <code>10000</code></p> <p>Use <a href=../SQLConf/#dataFramePivotMaxValues>SQLConf.dataFramePivotMaxValues</a> method to access the current value.</p> <h2 id=sparksqlredactionoptionsregex><span id=spark.sql.redaction.options.regex> spark.sql.redaction.options.regex<a class=headerlink href=#sparksqlredactionoptionsregex title="Permanent link">&para;</a></h2> <p>Regular expression to find options of a Spark SQL command with sensitive information</p> <p>Default: <code>(?i)secret!password</code></p> <p>The values of the options matched will be redacted in the explain output.</p> <p>This redaction is applied on top of the global redaction configuration defined by <code>spark.redaction.regex</code> configuration.</p> <p>Used exclusively when <code>SQLConf</code> is requested to <a href=../SQLConf/#redactOptions>redactOptions</a>.</p> <h2 id=sparksqlredactionstringregex><span id=spark.sql.redaction.string.regex> spark.sql.redaction.string.regex<a class=headerlink href=#sparksqlredactionstringregex title="Permanent link">&para;</a></h2> <p>Regular expression to point at sensitive information in text output</p> <p>Default: <code>(undefined)</code></p> <p>When this regex matches a string part, it is replaced by a dummy value (i.e. <code>*********(redacted)</code>). This is currently used to redact the output of SQL explain commands.</p> <p>NOTE: When this conf is not set, the value of <code>spark.redaction.string.regex</code> is used instead.</p> <p>Use <a href=../SQLConf/#stringRedactionPattern>SQLConf.stringRedactionPattern</a> method to access the current value.</p> <h2 id=sparksqlretaingroupcolumns><span id=spark.sql.retainGroupColumns> spark.sql.retainGroupColumns<a class=headerlink href=#sparksqlretaingroupcolumns title="Permanent link">&para;</a></h2> <p>Controls whether to retain columns used for aggregation or not (in <a href=../RelationalGroupedDataset/ >RelationalGroupedDataset</a> operators).</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#dataFrameRetainGroupColumns>SQLConf.dataFrameRetainGroupColumns</a> method to access the current value.</p> <h2 id=sparksqlrunsqlonfiles><span id=spark.sql.runSQLOnFiles> spark.sql.runSQLOnFiles<a class=headerlink href=#sparksqlrunsqlonfiles title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether Spark SQL could use <code>datasource</code>.<code>path</code> as a table in a SQL query.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#runSQLonFile>SQLConf.runSQLonFile</a> method to access the current value.</p> <h2 id=sparksqlselfjoinautoresolveambiguity><span id=spark.sql.selfJoinAutoResolveAmbiguity> spark.sql.selfJoinAutoResolveAmbiguity<a class=headerlink href=#sparksqlselfjoinautoresolveambiguity title="Permanent link">&para;</a></h2> <p>Controls whether to resolve ambiguity in join conditions for <a href=../joins/#join>self-joins</a> automatically (<code>true</code>) or not (<code>false</code>)</p> <p>Default: <code>true</code></p> <h2 id=sparksqlsortenableradixsort><span id=spark.sql.sort.enableRadixSort> spark.sql.sort.enableRadixSort<a class=headerlink href=#sparksqlsortenableradixsort title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Controls whether to use radix sort (<code>true</code>) or not (<code>false</code>) in <a href=../physical-operators/ShuffleExchangeExec/ >ShuffleExchangeExec</a> and <a href=../physical-operators/SortExec/ >SortExec</a> physical operators</p> <p>Default: <code>true</code></p> <p>Radix sort is much faster but requires additional memory to be reserved up-front. The memory overhead may be significant when sorting very small rows (up to 50% more).</p> <p>Use <a href=../SQLConf/#enableRadixSort>SQLConf.enableRadixSort</a> method to access the current value.</p> <h2 id=sparksqlsourcesbucketingenabled><span id=spark.sql.sources.bucketing.enabled> spark.sql.sources.bucketing.enabled<a class=headerlink href=#sparksqlsourcesbucketingenabled title="Permanent link">&para;</a></h2> <p>Enables <a href=../bucketing/ >bucketing</a> support. When disabled (i.e. <code>false</code>), bucketed tables are considered regular (non-bucketed) tables.</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#bucketingEnabled>SQLConf.bucketingEnabled</a> method to access the current value.</p> <h2 id=sparksqlsourcesdefault><span id=spark.sql.sources.default><span id=DEFAULT_DATA_SOURCE_NAME> spark.sql.sources.default<a class=headerlink href=#sparksqlsourcesdefault title="Permanent link">&para;</a></h2> <p>Default data source to use for loading or saving data</p> <p>Default: <a href=../datasources/parquet/ >parquet</a></p> <p>Use <a href=../SQLConf/#defaultDataSourceName>SQLConf.defaultDataSourceName</a> method to access the current value.</p> <h2 id=sparksqlstatisticsfallbacktohdfs><span id=spark.sql.statistics.fallBackToHdfs> spark.sql.statistics.fallBackToHdfs<a class=headerlink href=#sparksqlstatisticsfallbacktohdfs title="Permanent link">&para;</a></h2> <p>Enables automatic calculation of table size statistic by falling back to HDFS if the table statistics are not available from table metadata.</p> <p>Default: <code>false</code></p> <p>This can be useful in determining if a table is small enough for auto broadcast joins in query planning.</p> <p>Use <a href=../SQLConf/#fallBackToHdfsForStatsEnabled>SQLConf.fallBackToHdfsForStatsEnabled</a> method to access the current value.</p> <h2 id=sparksqlstatisticshistogramnumbins><span id=spark.sql.statistics.histogram.numBins> spark.sql.statistics.histogram.numBins<a class=headerlink href=#sparksqlstatisticshistogramnumbins title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The number of bins when generating histograms.</p> <p>Default: <code>254</code></p> <p>NOTE: The number of bins must be greater than 1.</p> <p>Use <a href=../SQLConf/#histogramNumBins>SQLConf.histogramNumBins</a> method to access the current value.</p> <h2 id=sparksqlstatisticsparallelfilelistinginstatscomputationenabled><span id=spark.sql.statistics.parallelFileListingInStatsComputation.enabled> spark.sql.statisticsparallelFileListingInStatsComputation.enabled*<a class=headerlink href=#sparksqlstatisticsparallelfilelistinginstatscomputationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables parallel file listing in SQL commands, e.g. <code>ANALYZE TABLE</code> (as opposed to single thread listing that can be particularly slow with tables with hundreds of partitions)</p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#parallelFileListingInStatsComputation>SQLConf.parallelFileListingInStatsComputation</a> method to access the current value.</p> <h2 id=sparksqlstatisticsndvmaxerror><span id=spark.sql.statistics.ndv.maxError> spark.sql.statistics.ndv.maxError<a class=headerlink href=#sparksqlstatisticsndvmaxerror title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> The maximum estimation error allowed in HyperLogLog++ algorithm when generating column level statistics.</p> <p>Default: <code>0.05</code></p> <h2 id=sparksqlstatisticspercentileaccuracy><span id=spark.sql.statistics.percentile.accuracy> spark.sql.statistics.percentile.accuracy<a class=headerlink href=#sparksqlstatisticspercentileaccuracy title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Accuracy of percentile approximation when generating equi-height histograms. Larger value means better accuracy. The relative error can be deduced by 1.0 / PERCENTILE_ACCURACY.</p> <p>Default: <code>10000</code></p> <h2 id=sparksqlstatisticssizeautoupdateenabled><span id=spark.sql.statistics.size.autoUpdate.enabled> spark.sql.statistics.size.autoUpdate.enabled<a class=headerlink href=#sparksqlstatisticssizeautoupdateenabled title="Permanent link">&para;</a></h2> <p>Enables automatic update of the table size statistic of a table after the table has changed.</p> <p>Default: <code>false</code></p> <p>IMPORTANT: If the total number of files of the table is very large this can be expensive and slow down data change commands.</p> <p>Use <a href=../SQLConf/#autoSizeUpdateEnabled>SQLConf.autoSizeUpdateEnabled</a> method to access the current value.</p> <h2 id=sparksqlsubexpressioneliminationenabled><span id=spark.sql.subexpressionElimination.enabled> spark.sql.subexpressionElimination.enabled<a class=headerlink href=#sparksqlsubexpressioneliminationenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Enables <a href=../spark-sql-subexpression-elimination/ >subexpression elimination</a></p> <p>Default: <code>true</code></p> <p>Use <a href=../SQLConf/#subexpressionEliminationEnabled>SQLConf.subexpressionEliminationEnabled</a> method to access the current value.</p> <h2 id=sparksqlshufflepartitions><span id=spark.sql.shuffle.partitions> spark.sql.shuffle.partitions<a class=headerlink href=#sparksqlshufflepartitions title="Permanent link">&para;</a></h2> <p>The default number of partitions to use when shuffling data for joins or aggregations.</p> <p>Default: <code>200</code></p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Corresponds to Apache Hive's <a href=https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-mapred.reduce.tasks>mapred.reduce.tasks</a> property that Spark SQL considers deprecated.</p> </div> <div class="admonition note"> <p class=admonition-title>Spark Structured Streaming</p> <p><code>spark.sql.shuffle.partitions</code> cannot be changed in Spark Structured Streaming between query restarts from the same checkpoint location.</p> </div> <p>Use <a href=../SQLConf/#numShufflePartitions>SQLConf.numShufflePartitions</a> method to access the current value.</p> <h2 id=sparksqlsourcesfilecompressionfactor><span id=spark.sql.sources.fileCompressionFactor> spark.sql.sources.fileCompressionFactor<a class=headerlink href=#sparksqlsourcesfilecompressionfactor title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> When estimating the output data size of a table scan, multiply the file size with this factor as the estimated data size, in case the data is compressed in the file and lead to a heavily underestimated result.</p> <p>Default: <code>1.0</code></p> <p>Use <a href=../SQLConf/#fileCompressionFactor>SQLConf.fileCompressionFactor</a> method to access the current value.</p> <h2 id=sparksqlsourcespartitionoverwritemode><span id=spark.sql.sources.partitionOverwriteMode> spark.sql.sources.partitionOverwriteMode<a class=headerlink href=#sparksqlsourcespartitionoverwritemode title="Permanent link">&para;</a></h2> <p>Enables <a href=../dynamic-partition-inserts/ >dynamic partition inserts</a> when <code>dynamic</code></p> <p>Default: <code>static</code></p> <p>When <code>INSERT OVERWRITE</code> a partitioned data source table with dynamic partition columns, Spark SQL supports two modes (case-insensitive):</p> <ul> <li> <p><strong>static</strong> - Spark deletes all the partitions that match the partition specification (e.g. <code>PARTITION(a=1,b)</code>) in the INSERT statement, before overwriting</p> </li> <li> <p><strong>dynamic</strong> - Spark doesn't delete partitions ahead, and only overwrites those partitions that have data written into it</p> </li> </ul> <p>The default <code>STATIC</code> overwrite mode is to keep the same behavior of Spark prior to 2.3. Note that this config doesn't affect Hive serde tables, as they are always overwritten with dynamic mode.</p> <p>Use <a href=../SQLConf/#partitionOverwriteMode>SQLConf.partitionOverwriteMode</a> method to access the current value.</p> <h2 id=sparksqltruncatetableignorepermissionaclenabled><span id=spark.sql.truncateTable.ignorePermissionAcl.enabled> spark.sql.truncateTable.ignorePermissionAcl.enabled<a class=headerlink href=#sparksqltruncatetableignorepermissionaclenabled title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Disables setting back original permission and ACLs when re-creating the table/partition paths for <a href=../logical-operators/TruncateTableCommand/ >TRUNCATE TABLE</a> command.</p> <p>Default: <code>false</code></p> <p>Use <a href=../SQLConf/#truncateTableIgnorePermissionAcl>SQLConf.truncateTableIgnorePermissionAcl</a> method to access the current value.</p> <h2 id=sparksqluiretainedexecutions><span id=spark.sql.ui.retainedExecutions> spark.sql.ui.retainedExecutions<a class=headerlink href=#sparksqluiretainedexecutions title="Permanent link">&para;</a></h2> <p>The number of <a href=../SQLListener/#SQLExecutionUIData>SQLExecutionUIData</a> entries to keep in <code>failedExecutions</code> and <code>completedExecutions</code> internal registries.</p> <p>Default: <code>1000</code></p> <p>When a query execution finishes, the execution is removed from the internal <code>activeExecutions</code> registry and stored in <code>failedExecutions</code> or <code>completedExecutions</code> given the end execution status. It is when <code>SQLListener</code> makes sure that the number of <code>SQLExecutionUIData</code> entires does not exceed <code>spark.sql.ui.retainedExecutions</code> Spark property and removes the excess of entries.</p> <h2 id=sparksqlwindowexecbufferinmemorythreshold><span id=spark.sql.windowExec.buffer.in.memory.threshold> spark.sql.windowExec.buffer.in.memory.threshold<a class=headerlink href=#sparksqlwindowexecbufferinmemorythreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Threshold for number of rows guaranteed to be held in memory by <a href=../physical-operators/WindowExec/ >WindowExec</a> physical operator.</p> <p>Default: <code>4096</code></p> <p>Use <a href=../SQLConf/#windowExecBufferInMemoryThreshold>SQLConf.windowExecBufferInMemoryThreshold</a> method to access the current value.</p> <h2 id=sparksqlwindowexecbufferspillthreshold><span id=spark.sql.windowExec.buffer.spill.threshold> spark.sql.windowExec.buffer.spill.threshold<a class=headerlink href=#sparksqlwindowexecbufferspillthreshold title="Permanent link">&para;</a></h2> <p><strong>(internal)</strong> Threshold for number of rows buffered in a <a href=../physical-operators/WindowExec/ >WindowExec</a> physical operator.</p> <p>Default: <code>4096</code></p> <p>Use <a href=../SQLConf/#windowExecBufferSpillThreshold>SQLConf.windowExecBufferSpillThreshold</a> method to access the current value.</p> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../tags/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Tags" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Tags </div> </div> </a> <a href=../SQLConf/ class="md-footer__link md-footer__link--next" aria-label="Next: SQLConf" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> SQLConf </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2021 Jacek Laskowski </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs Insiders </a> </div> <div class=md-footer-social> <a href=https://github.com/jaceklaskowski target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://linkedin.com/in/jaceklaskowski target=_blank rel=noopener title=linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> <a href=https://jaceklaskowski.medium.com target=_blank rel=noopener title=jaceklaskowski.medium.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M0 32v448h448V32H0zm372.2 106.1-24 23c-2.1 1.6-3.1 4.2-2.7 6.7v169.3c-.4 2.6.6 5.2 2.7 6.7l23.5 23v5.1h-118V367l24.3-23.6c2.4-2.4 2.4-3.1 2.4-6.7V199.8l-67.6 171.6h-9.1L125 199.8v115c-.7 4.8 1 9.7 4.4 13.2l31.6 38.3v5.1H71.2v-5.1l31.6-38.3c3.4-3.5 4.9-8.4 4.1-13.2v-133c.4-3.7-1-7.3-3.8-9.8L75 138.1V133h87.3l67.4 148L289 133.1h83.2v5z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.annotate", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.d1f5481f.min.js", "version": null}</script> <script src=../assets/javascripts/bundle.d9c629a2.min.js></script> </body> </html>